\section{The Broyden-Fletcher-Goldfarb-Shanno Formula}
\label{Section2.3}

In this section we explain an approach to how the matrix $H_{k+1}$ (or $B_{k+1}$) can arise from the matrix $H_k$ (or $B_k$) and other information. This process is also known as update. Various formulae have been developed, some of which are interrelated. We deal with the Broyden-Fletcher-Goldfarb-Shanno formula, or short BFGS formula. The method based on the BFGS formula has proven to be the most efficient quasi-Newton method in practice \cite[p.~69]{UlbrichUlbrich:2012}. However, all quasi-Newton updates follow the following three important guidelines, which must be fulfilled when generating $H_{k+1}$:
\begin{enumerate}
    \item $H_{k+1}$ is a positive definite, symmetric matrix in $\mathbb{R}^{n \times n}$. 
    \item $H_{k+1}$ satisfies the quasi-Newton equation \cref{quasi-NewtonEquation}.
    \item $H_{k+1}$ is “near” $H_k$.
\end{enumerate}
Of course these three characteristics should also hold for the approximation of the inverse $B_{k+1}$. In \cref{Section2.2} was shown that $H_{k+1}$ must satisfy the quasi-Newton equation \cref{quasi-NewtonEquation}. The strongest motivation comes from the fact that we approximate our objective function local by a quadratic model and the Hessian of a quadratic function always satisfies the quasi-Newton equation. The fact that the distance between $H_{k+1}$ and $H_k$ should not be too large will be related to the rate of convergence of the resulting method and the uniqueness of the formula. It's obvious that the matrix $H_{k+1}$ should be symmetric, since we want to create a quadratic model at the iterate $x_{k+1}$ and at the same time we also want to approximate the Hessian $\nabla^2 f(x_{k+1})$, which is always symmetric in the case of a twice continuously differentiable function $f \in C^2$. We need positive-definiteness for efficiency, numerical stability and global convergence. If the Hessian $\nabla^2 f(x^*)$ is positive definite, the stationary point $x^*$ is a strict minimizer. Hence, we require that the Hessian approximations $\{H_k\}_k$, or inverse Hessian approximations $\{B_k\}_k$, are positive definite. In addition, if $H_k$ (or $B_k$) is positive definite, the local quadratic model of $f$ has a unique local minimizer, and the direction $d_k$ from \cref{DescentDirection} is a descent direction \cite[p.~212]{SunYuan:2006}. \\
Before we get to the BFGS formula, also called BFGS update, let us first look at the so-called DFP update, proposed by Davidon in \cite{Davidon:1959} and developed later by Fletcher and Powell in \cite{FletcherPowell:1963}. We assume that the matrix $B_k$ approximates $\nabla^2 f(x_k)^{-1}$ sufficiently well. Let us consider a symmetric rank-two update of this matrix, that means we add two symmetric rank-one matrices to the current matrix
\begin{equation*}
    B_{k+1} = B_k + a \; uu^{\mathrm{T}} + b \; vv^{\mathrm{T}}
\end{equation*}
where $u,v \in \mathbb{R}^n$, $a,b \in \mathbb{R}$ are to be determined. From the quasi-Newton equation follows
\begin{equation}\label{UpdateQuasi-NewtonEquation}
    B_{k+1} y_k = B_k y_k + a \; uu^{\mathrm{T}} y_k + b \; vv^{\mathrm{T}} y_k = s_k.
\end{equation}
Clearly, $u$ and $v$ can not uniquely be determined. One possible choice is
\begin{equation*}
    u = s_k, \quad v = B_k y_k.
\end{equation*}
Then we obtain from \cref{UpdateQuasi-NewtonEquation}
\begin{equation*}
    a = \frac{1}{u^{\mathrm{T}} y_k} = \frac{1}{s^{\mathrm{T}}_k y_k}, \quad b = - \frac{1}{v^{\mathrm{T}} y_k} = \frac{1}{y^{\mathrm{T}}_k B_k y_k}.
\end{equation*}
Therefore
\begin{equation}\label{DFPFormula}
    B^{DFP}_{k+1} = B^{DFP}_k + \frac{s_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k} - \frac{B^{DFP}_k y_k y^{\mathrm{T}}_k B^{DFP}_k}{y^{\mathrm{T}}_k B^{DFP}_k y_k}.
\end{equation}
\cref{DFPFormula} is the first quasi-Newton formula, proposed originally in \cite{Davidon:1959}. It is widely known as DFP formula or DFP update and the resulting matrices $B^{DFP}_k$ approximate the inverse of the Hessian $\nabla^2 f(x_k)^{-1}$ in every iteration \cite[p.~210]{SunYuan:2006}. \\
The last two terms in the right-hand-side of \cref{DFPFormula} are symmetric rank-one matrices. This is the fundamental idea of quasi-Newton updating: Instead of recomputing the approximate Hessian (or inverse Hessian) from scratch at every iteration, we apply a simple modification that combines the most recently observed information about the objective function with the existing knowledge embedded in our current Hessian approximation \cite[p.~139]{NocedalWright:2006}. \\
The BFGS formula can be obtained by a simple trick: we replace the triple $(B^{DFP}_k,s_k,y_k)$ in \cref{DFPFormula} by $(H^{BFGS}_k,y_k,s_k)$ \cite{SunYuan:2006}. The result is the following rank-two update of $H^{BFGS}_k$
\begin{equation}\label{directBFGSformula}
    H^{BFGS}_{k+1} = H^{BFGS}_k + \frac{y_k y^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k} - \frac{H^{BFGS}_k s_k s^{\mathrm{T}}_k H^{BFGS}_k}{s^{\mathrm{T}}_k H^{BFGS}_k s_k}.
\end{equation}
This formula was discovered independently by Broyden in \cite{Broyden:1967}, by Fletcher in \cite{Fletcher:1970}, by Goldfarb in \cite{Goldfarb:1970} and by Shanno in \cite{Shanno:1970}, which is the reason for the naming. 
All four authors derive the BFGS formula in a slightly different way and the fact that so many different approaches lead to the BFGS formula may already be seen as a reason why the BFGS formula is in practice superior to other updating formulae \cite[p.~136]{GeigerKanzow:1999}. The DFP formula is quite effective, but it was soon superseded by the BFGS formula, which is presently considered to be the most effective of all quasi-Newton formulae \cite[p.~139]{NocedalWright:2006}. The BFGS formula is considered to be the best one of all quasi-Newton formulae, which has all good properties of the DFP formula \cite[p.~219]{SunYuan:2006}. \\
Of course, one is also interested in a formula for the update of the inverse of $H^{BFGS}_k$, since $B^{BFGS}_k$ would make solving a system of equations in \cref{DescentDirection} and thus the (complex) inverting of $H^{BFGS}_k$ unnecessary. By applying the Sherman-Morrison-Woodbury formula, \cref{Sherman-Morrison-WoodburyFormula}, twice to \cref{directBFGSformula} and by rearranging, we obtain
\begin{equation}\label{inverseBFGSformula}
    \begin{split}
        B^{BFGS}_{k+1} &= B^{BFGS}_k + \frac{(s_k - B^{BFGS}_k y_k) s^{\mathrm{T}}_k + s_k (s_k - B^{BFGS}_k y_k)^{\mathrm{T}}}{s^{\mathrm{T}}_k y_k} - \frac{(s_k - B^{BFGS}_k y_k)^{\mathrm{T}} y_k s_k s^{\mathrm{T}}_k}{(s^{\mathrm{T}}_k y_k)^2} = \\
        &= B^{BFGS}_k + \Big{(} I_{n \times n} + \frac{y^{\mathrm{T}}_k B^{BFGS}_k y_k}{s^{\mathrm{T}}_k y_k} \Big{)} \frac{s_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k} - \frac{s_k y^{\mathrm{T}}_k B^{BFGS}_k + B^{BFGS}_k y_k s^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k} = \\
        &= \Big{(} I_{n \times n} - \frac{s_k y^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} B^{BFGS}_k \Big{(} I_{n \times n} - \frac{y_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} + \frac{s_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k}.
    \end{split}
\end{equation}
\cref{inverseBFGSformula} are different formulae for approximating the Hessian inverse $\nabla^2 f(x_{k+1})$. All formulae in \cref{inverseBFGSformula} are also called BFGS formula. Of course
\begin{equation*}
    H^{BFGS}_{k+1} B^{BFGS}_{k+1} = B^{BFGS}_{k+1} H^{BFGS}_{k+1} = I_{n \times n}
\end{equation*}
holds or all $k \geq 0$. Replacing the triple $(B^{BFGS}_k, s_k, y_k)$ in \cref{inverseBFGSformula} by $(H^{DFP}_k, y_k, s_k)$, one would get a formula for $H^{DFP}_{k+1}$, the direct DFP update. This describes a method for finding the so-called “dual” update from a given update \cite[p.218]{SunYuan:2006}. For this reason, the DFP and BFGS formulae are sometimes referred to as “dual” update formulae. \\
We show that the characteristics mentioned in the beginning of this section can be proven for $H^{BFGS}_{k+1}$. For the first two there is the following statement:

\begin{theorem}[{Generalization of \cite[Theorem~13.4]{UlbrichUlbrich:2012}}]\label{thmUlbrichUlbrich13.4} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item If $y_k^{\mathrm{T}} s_k \neq 0$ and $s_k^{\mathrm{T}} H^{BFGS}_k s_k \neq 0$ holds, then the matrices $H^{BFGS}_{k+1} \in \mathbb{R}^{n \times n}$ are well defined, symmetric and satisfy the quasi-Newton equation (\cref{quasi-NewtonEquation}).
        \item If $H^{BFGS}_k$ is positive definite and $y_k^{\mathrm{T}} s_k > 0$, then $H^{BFGS}_{k+1}$ is positive definite.
    \end{enumerate}
\end{theorem} 

This is why \cref{directBFGSformula} is also called a positive definite update, as it inherits the positive-definiteness of $H^{BFGS}_k$. The same statements of course also apply to the inverse $B^{BFGS}_{k+1}$. \\
In \cref{Section2.2} was shown that the curvature condition, \cref{CurvatureCondition}, must hold. This was achieved by imposing restrictions on the line search method (see \cref{CurvatureConditionWolfe}). This is why the positive-definiteness is guaranteed by a stepsize that meets the (strong) Wolfe conditions. \\
The statements in \cref{thmUlbrichUlbrich13.4} were actually made for Broyden class matrices (the update of these matrices is a convex combination of the direct DFP formula and the direct BFGS formula, see e.g. \cite[p.~68]{UlbrichUlbrich:2012}), which means that \cref{thmUlbrichUlbrich13.4} can be transferred one-to-one to the DFP matrices $H^{DFP}_{k+1}$. \\
The last characteristic, that $H_{k+1}$ should be “near” $H_k$, has a more powerful meaning than the first two. Many authors use only this to define the BFGS formula which of course is perfectly legitimate. As already mentioned, this property leads to the fact that the formula can be considered as unique and it has something to do with the rate of convergence. \\
As already mentioned in \cref{Section2.2}, one is interested that the method resulting from the BFGS formula converges superlinearly. If we look at local quasi-Newton methods, where we assume that the stepsize is always $\alpha_k = 1$, then the Dennis-Mor\'{e} condition follows, among other things, from the characteristic of closeness:

\begin{lemma}[{\cite[Lemma~13.2]{UlbrichUlbrich:2012}}]
    Let $x^* \in \mathbb{R}^n$ meet the second-order sufficient conditions (see \cref{Minimizers}). Let $x_0 \in \mathbb{R}^n$ and $\{x_k\}_k$ be a sequence convergent to $x^*$, generated by $x_{k+1} = x_k + d_k$ with $H_k d_k = -\nabla f(x_k)$, where the matrices $H_{k+1}$ in each iteration are determined to satisfy the quasi-Newton equation, \cref{quasi-NewtonEquation}, and are symmetric, nonsingular. Let $H_0$ be symmetric and invertible. If
    \begin{equation*}
        \lim\limits_{k \rightarrow \infty} \lVert H_{k+1} - H_k \rVert_{\mathrm{F}} = 0,
    \end{equation*}
    holds, then $H_k$ satisfies the Dennis-Mor\'{e} condition, \cref{corGeigerKanzow7.9}, and $\{x_k\}_k$ converges superlinear to $x^*$.
\end{lemma}

Therefore one looks for quasi-Newton updates for which $H_{k+1}$ is close to $H_k$ in each iteration, so that the distance between them converges towards zero. Of course the same applies to $B_{k+1}$ and $B_k$. \\
With this goal in mind and since we want to determine $B^{BFGS}_{k+1}$ uniquely, we impose the condition that among all symmetric matrices satisfying the quasi-Newton equation, \cref{quasi-NewtonEquation}, $B^{BFGS}_{k+1}$ is closest to the current matrix $B^{BFGS}_k$. In other words, we solve the problem:
\begin{equation}\label{BFGSFormulaOptimizationProblem}
    \begin{aligned}
        \min_{B} \quad \lVert B - B^{BFGS}_k \rVert^{2}_{W^2} \\
        \textrm{s.t.} \quad B = B^{\mathrm{T}}, \quad B y_k = s_k \\
    \end{aligned}
\end{equation}
We use the weighted Frobenius norm, which allows an easy solution of \cref{BFGSFormulaOptimizationProblem}:
\begin{equation}\label{WeightedFrobenius}
    \lVert A \rVert_{W^2} = \lVert W A W \rVert_{\mathrm{F}}
\end{equation}
where $\lVert M \rVert^2_{\mathrm{F}} = \sum^n_{i,j=1} (M)^2_{ij}$ \cite[p.~138]{NocedalWright:2006}. By a suitable choice of the so-called weighting matrix $W$ the uniqueness of $B^{BFGS}_{k+1}$ can be shown: 

\begin{theorem}[{\cite[Theorem~11.8]{GeigerKanzow:1999}}]\label{thrGeigerKanzow11.8}
    Let $B^{BFGS}_k \in \mathbb{R}^{n \times n}$ be $\spd$ and $s_k, y_k \in \mathbb{R}^n$ satisfying $s^{\mathrm{T}}_k y_k > 0$. Let $W^2 \in \mathbb{R}^{n \times n}$ be a $\spd$ matrix satisfying $W^2 s_k = y_k$. Then the unique solution of the weighted problem \cref{BFGSFormulaOptimizationProblem} with the weighting matrix $W$ is given by
    \begin{equation*}
        B^{BFGS}_{k+1} = B^{BFGS}_k + \frac{(s_k - B^{BFGS}_k y_k) s^{\mathrm{T}}_k + s_k (s_k - B^{BFGS}_k y_k)^{\mathrm{T}}}{s^{\mathrm{T}}_k y_k} - \frac{(s_k - B^{BFGS}_k y_k)^{\mathrm{T}} y_k s_k s^{\mathrm{T}}_k}{(s^{\mathrm{T}}_k y_k)^2}.
    \end{equation*}    
\end{theorem}

In fact, $W$ can be any matrix that satisfies $W^2 s_k = y_k$. For concreteness, we can assume that $W^2 = \tilde{G}_k$, where $\tilde{G}_k$ is the average Hessian (see \cref{AverageHessian}). With this choice of weighting matrix $W$, is \cref{WeightedFrobenius} non-dimensional, which is a desirable property, since we do not wish the solution of \cref{BFGSFormulaOptimizationProblem} to depend on the units of the problem \cite[p.~139-140]{NocedalWright:2006}. The existence of a $\spd$ matrix $W^2$ is shown in \cite[Lemma~11.5.]{GeigerKanzow:1999} and since it is $\spd$, it implies the existence of the $\spd$ matrix $W$, see \cite[Satz~B.6.]{GeigerKanzow:1999}. The specified minimum characteristic ensures the invariance of the BFGS method under affine-linear variable transformations. This important characteristic is also present in the Newton method \cite[p.~69]{UlbrichUlbrich:2012}. \\
The initial approximation $B^{BFGS}_0$ must still be discussed. There is a lot of flexibility in the choice, as the only thing that needs to be ensured is that $B^{BFGS}_0$ is symmetric and positive definite. But unfortunately, there is no perfect strategy for this yet. One possibility is to use information about the problem and approximate the Hessian inverse by finite differences at $x_0$. One could also use a multiple of the identity matrix $\kappa \; I$, where $\kappa > 0$ is a scaling factor for the variables. But to determine this factor is problematic. If $\kappa$ is too large, so that the first step $d_0 = -\kappa \; g_0$ is too long, many function evaluations may be required to find a suitable value for the stepsize $\alpha_0$. A quite effective heuristic is to scale the starting matrix after the first step, i.e. after computing $x_{1} = x_0 + \alpha_0 d_0$, but before the first BFGS update is performed. The provisional value $B^{BFGS}_0 = I$ is changed by setting
\begin{equation*}
    B^{BFGS}_0 = \frac{y^{\mathrm{T}}_0 s_0}{y^{\mathrm{T}}_0 y_0} \; I
\end{equation*}
before applying the update to obtain $B^{BFGS}_1$. This formula attempts to make the size of $B^{BFGS}_0$ similar to that of $\nabla^2 f (x_0)^{-1}$ \cite[p.~142-143]{NocedalWright:2006}.
