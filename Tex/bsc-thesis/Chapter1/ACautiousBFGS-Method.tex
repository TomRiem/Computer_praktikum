\section{Cautious BFGS Method}
\label{Section2.5}

In \cite{Dai:2012} a four-dimensional example where the objective function is smooth (polynomial) and nonconvex was presented such that the globalized BFGS method does not converge. It can therefore be concluded that \cref{InverseGlobalBFGS-Method} unfortunately does not converge in general. We present the method from \cite{LiFukushima:2001}, in which a so-called cautious update is introduced in \cref{InverseGlobalBFGS-Method}. It can be shown that this method with a stepsize strategy, which satisfies the Wolfe conditions, converges globally if the objective function has Lipschitz continuous gradients. Moreover, under appropriate conditions, it can be shown that the cautious update eventually reduces to the “ordinary” update, i.e. \cref{inverseBFGSformula} is used to update the matrix. \\
To motivate the introduction of this new update in \cref{InverseGlobalBFGS-Method}, the following statement due to \cite{Powell:1975} is mentioned:

\begin{lemma}[{\cite[Lemma~2.1]{LiFukushima:2001}}]
    If the BFGS method with a line search that satisfies the Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2} is applied to a continuously differentiable function $f$ that is bounded below, and if there exists a constant $M > 0$ such that the inequality
    \begin{equation}\label{CautiousMotivation}
        \frac{\lVert y_k \rVert^2}{s^{\mathrm{T}}_k y_k} \leq M
    \end{equation}
    holds for all $k$, then
    \begin{equation}\label{CautiousLimInf}
        \liminf_{k \to \infty} \lVert \nabla f(x_k) \rVert = 0.
    \end{equation}
\end{lemma}

If $f$ is twice continuously differentiable and convex, then the average Hessian $\tilde{G}_k$ (see \cref{AverageHessian}) is $\spd$ and its square root ${\tilde{G}_k}^{1/2}$ is well-defined. Since $\tilde{G}_k s_k = y_k$ holds and by defining $z_k = {\tilde{G}_k}^{1/2} s_k$, we obtain
\begin{equation*}
    \frac{\lVert y_k \rVert^2}{s^{\mathrm{T}}_k y_k} = \frac{y^{\mathrm{T}}_k y_k}{s^{\mathrm{T}}_k y_k} = \frac{(\tilde{G}_k s_k)^{\mathrm{T}} \tilde{G}_k s_k}{s^{\mathrm{T}}_k \tilde{G}_k s_k} = \frac{s_k^{\mathrm{T}} {\tilde{G}_k}^2 s_k}{s_k^{\mathrm{T}} \tilde{G}_k s_k} = \frac{z_k^{\mathrm{T}} \tilde{G}_k z_k}{z_k^{\mathrm{T}} z_k}\leq M
\end{equation*}
and therefore holds \cref{CautiousMotivation} always whenever the sequence of iterates $\{ x_k \}_k$ is bounded.  If $f$ is nonconvex, it seems difficult to guarantee \cref{CautiousMotivation}. This is probably the main reason why global convergence of the BFGS method, \cref{InverseGlobalBFGS-Method}, has not yet to be proven \cite[p.~1056]{LiFukushima:2001}. \\
In order to still achieve \cref{CautiousLimInf} for nonconvex functions, a decision rule was established, according to which the update of the approximation is based. To be precise, the following update is now used in each iteration:
\begin{equation}\label{CautiousUpdate}
    B^{CBFGS}_{k+1} = \begin{cases} \text{using \cref{inverseBFGSformula}}, & \; \frac{s^{\mathrm{T}}_k y_k}{\lVert s_k \rVert^{2}} \geq \mu \lVert \nabla f(x_k) \rVert^{\lambda}, \\ B^{CBFGS}_k, & \; \text{otherwise}, \end{cases}
\end{equation}
where $\mu$ and $\lambda$ are positive constants. \\
The only requirement for the matrix $B^{CBFGS}_{k+1}$ to be updated is that the inner product $s^{\mathrm{T}}_k y_k$ must be greater than or equal to a positive value depending on the gradient. This means that the vectors $s_k, y_k$, obtained by the iteration, must fulfill a “little bit” more than just the curvature condition, \cref{CurvatureCondition}. The introduction of this update is therefore also motivated by the fact that it can be ensured that even stepsize strategies which do not fulfill \cref{WolfeConditions2} (or \cref{StrongWolfeCondition}) generate positive definite updates, since it is ensured that $B^{CBFGS}_{k+1}$ is updated if \cref{CurvatureCondition} holds, which in turn implies that the positive-definiteness is inherited (see \cref{thmUlbrichUlbrich13.4}) \cite[p.~1061]{LiFukushima:2001}. \\
However, we assume that a stepsize strategy is used, which determines a stepsize $\alpha_k$ that only fulfills the Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2} and therefore \cref{CurvatureCondition} is always satisfied. The BFGS method with cautious update \cref{CautiousUpdate}, in short cautious BFGS method or CBFGS method, is described in \cref{CautiousBFGSMethod}. \\

\begin{algorithm}[H]
	\caption{Cautious BFGS Algorithm}\label{CautiousBFGSMethod}
	\begin{algorithmic}[1]
        \State Continuously differentiable real-valued function $f$ on $\mathbb{R}^n$, bounded below; initial iterate $x_0 \in \mathbb{R}^n$; initial $\spd$ matrix $B^{CBFGS}_0 \in \mathbb{R}^{n \times n}$; convergence tolerance $\varepsilon > 0$; linesearch parameters $0 < c_1 < \frac{1}{2} < c_2 < 1$; constants $\lambda, \mu > 0$ for \cref{CautiousUpdate}. Set $k = 0$.
		\While{$\lVert \operatorname{grad} f(x_k) \rVert > \varepsilon$}
            \State Compute the search direction $d_k = - B^{CBFGS}_k \nabla f(x_k)$.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies the Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2}. 
            \State Set $x_{k+1} = x_k + \alpha_k d_k$.
            \State Set $s_k = x_{k+1} - x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$.
            \State Compute $B^{CBFGS}_{k+1} \in \mathbb{R}^{n \times n}$ by means of \cref{CautiousUpdate}. 
            \State Set $k = k+1$.
		\EndWhile
		\State \textbf{return} $x_k$
    \end{algorithmic}
\end{algorithm}

We now come to the more convenient convergence results achieved by \cref{CautiousBFGSMethod}. These now concern not only convex functions and therefore have weaker assumptions:

\begin{assumption}[{\cite[Assumption~A]{LiFukushima:2001}}]\label{CautiousAssumption}
    The level set
    \begin{equation*}
        \mathcal{L} = \{ x \in \mathbb{R}^n \colon \; f(x) \leq f(x_0) \}
    \end{equation*}    
    is bounded, the function $f$ is continuously differentiable on $\mathcal{L}$, and there exists a constant $L > 0$ such that
    \begin{equation}\label{CautiousAssumptionLipschitz}
        \lVert \nabla f(x) - \nabla f(y) \rVert \leq L \lVert x - y \rVert, \quad \text{for all } \; x,y \in \mathcal{L}.
    \end{equation}
\end{assumption}

We see that the assumption that $f$ must be convex on the sublevel set $\mathcal{L}$ no longer applies, in contrast to \cref{AssumptionsGlobalConvergence}. Instead the assumption, that the objective function $f$ is Lipschitz continuously differentiable on $\mathcal{L}$, which means that the gradients of $f$ are Lipschitz continuous on $\mathcal{L}$, was added. Since $\{ f(x_k) \}_k$ is a decreasing sequence, it follows that the sequence $\{ x_k \}_k$ generated by \cref{CautiousBFGSMethod} is contained in $\mathcal{L}$. We have the following statement on global convergence:

\begin{theorem}[{\cite[Theorem~3.3.]{LiFukushima:2001}}]\label{CautiousGlobalConvergence1}
    Let \cref{CautiousAssumption} hold and $\{ x_k \}_k$ be generated by \cref{CautiousBFGSMethod}. Then \cref{CautiousLimInf} holds.
\end{theorem}

\cref{CautiousGlobalConvergence1} shows the existence of a subsequence of $\{ x_k \}_k$ which converges to a stationary point $x^*$ of \cref{OptimizationProblem}. \cref{CautiousGlobalConvergence1} does not rely on the convexity assumption on the objective function. \\
If $f$ is nevertheless convex, then $x^*$ is a global minimum of $f$. Since the sequence $\{ f(x_k) \}_k$ converges, it is clear that every accumulation point of $\{ x_k \}_k$ is a global optimal solution of \cref{OptimizationProblem}: 

\begin{corollary}[{\cite[Corollary~3.4.]{LiFukushima:2001}}]\label{CautiousGlobalConvergence2}
    Let \cref{CautiousAssumption} hold and $\{ x_k \}_k$ be generated by \cref{CautiousBFGSMethod}. If $f$ is convex, then the whole sequence $\{\nabla f(x_k)\}_k$ converges to zero. Consequently, every accumulation point of $\{ x_k \}_k$ is a global optimal solution.
\end{corollary}

If $f$ is nonconvex, \cref{CautiousGlobalConvergence2} is not guaranteed. The next theorem shows that if some additional assumptions are made, then the whole sequence $\{ x_k \}_k$ converges to a local optimal solution of \cref{OptimizationProblem}: 

\begin{theorem}[{\cite[Theorem~3.5.]{LiFukushima:2001}}]\label{CautiousLocalConvergence}
    Let $f$ be twice continuously differentiable. Suppose that $\lim\limits_{k \to \infty} s_k = 0$. If there exists an accumulation point $x^*$ of $\{ x_k \}_k$ at which $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*)$ is positive definite, then the whole sequence $\{ x_k \}_k$ converges to $x^*$. If in addition, $\nabla^2 f(\cdot)$ is Hölder continuous and $0 < c_1 < \frac{1}{2}$ (in \cref{WolfeConditions1}) holds, then the convergence rate is superlinear.
\end{theorem}

The assumptions imply that $x^*$ is a strict local optimal solution (see \cref{Minimizers}). In the proof is shown that \cref{CautiousBFGSMethod} is reduces to \cref{InverseGlobalBFGS-Method}, that means when $k$ is sufficiently large, the decision rule in \cref{CautiousUpdate} is always fulfilled, which implies that the algorithm reduces to the “ordinary” BFGS method. The superlinear convergence then follows from the convergence results of \cref{InverseGlobalBFGS-Method}. \\
The parameters $\mu, \lambda$ need not be constant while the \cref{CautiousBFGSMethod} is running. By choosing $\lambda = 0.01$ if $\lVert \nabla f(x_k) \rVert \geq 1$ and $\lambda = 3$ if $\lVert \nabla f(x_k) \rVert < 1$ one tries to make the cautious update \cref{CautiousUpdate} closer to the original BFGS update \cref{inverseBFGSformula}. Another option is that $\lambda$ can also be within an interval $[a,b]$ with $a>0$. More generally, the value $\mu \lVert \nabla f(x_k) \rVert^{\lambda}$ can be replaced by a general forcing function $\theta(\lVert \nabla f(x_k) \rVert)$, which is strictly monotone with $\theta(0) = 0$. For all these variants of parameter selection or adjustments of the decision rule, the convergence results hold \cite[p.~1059-1060]{LiFukushima:2001}. \\
