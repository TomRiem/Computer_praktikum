\section{Limited-Memory BFGS Method}
\label{Section2.6}

One of the disadvantages of quasi-Newton methods is that a $n \times n$ matrix (namely $B^{BFGS}_{k+1}$) must be stored in each iteration. Even when using the symmetry of this matrix, a memory requirement of $n(n+1)/2$ matrix entries remains. For large-scale optimization problems is this not feasible \cite[p.~197]{GeigerKanzow:1999}. \\
Limited-memory quasi-Newton methods, also called variable-storage quasi-Newton methods, are useful for solving large optimization problems, where the Hessian matrices cannot be computed at a reasonable cost or are not sparse. These methods save only a few $n$-dimensional vectors, instead of storing and computing fully dense $n \times n$ approximations of the Hessian. The main idea is to use the curvature information (the information about the curvature of the objective function) from only the most recent iterations to construct the Hessian approximation. Curvature information from earlier iterations, which is less likely to be relevant to the actual behavior of the Hessian at the current iterate, is discarded in the interest of saving storage. Their rate of convergence is acceptable, despite these modest storage requirements \cite[p.~176]{NocedalWright:2006}. \\
Due to the outstanding importance of the BFGS method in the class of quasi-Newton methods \cite[p.~197]{GeigerKanzow:1999}, it is also predominantly used as a limited-memory quasi-Newton method, called LBFGS. But there are also limited-memory versions of other quasi-Newton methods such as the Symmetric Rank-One (SR1) method \cite[p.~177]{NocedalWright:2006}. \\
In \cref{Section2.3} three different formulae for the approximation of the Hessian inverse were introduced, see \cref{inverseBFGSformula}. We consider now the last one 
 \begin{equation*}
    B^{BFGS}_{k+1} = \Big{(} I_{n \times n} - \frac{s_k y^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} B^{BFGS}_k \Big{(} I_{n \times n} - \frac{y_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} + \frac{s_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k}.
\end{equation*}    
For given vectors $s_k, y_k \in \mathbb{R}^n$ with $s_k^{\mathrm{T}} y_k > 0$ one sets 
\begin{equation}\label{LimitedMemoryVariables}
    \rho_k = \frac{1}{s^{\mathrm{T}}_{k} y_k} , \quad V_k = I_{n \times n} - \rho_k \; y_k s^{\mathrm{T}}_{k},
\end{equation}
obtaining
\begin{equation}\label{LimitedMemoryFormula}
    B^{BFGS}_{k+1} = V^{\mathrm{T}}_k B^{BFGS}_k V_k + \rho_k \; s_k s_k^{\mathrm{T}}.
\end{equation} 

The matrix $B^{BFGS}_{k+1}$ is obtained by updating $B^{BFGS}_k$ using the pair $\{ s_k, y_k\}$ \cite[p.~293]{SunYuan:2006}. \\
We now consider what happens in the $k$-th iteration. Since the inverse Hessian approximation $B^{BFGS}_k$ will generally be dense, the cost of storing and modifying it, is prohibitive when the number of variables is large. To circumvent this problem, one stores a modified version of $B^{BFGS}_k$ implicitly, by storing a certain number (say, $m$) of the vector pairs $\{ s_i, y_i\}$ \cite[p.~177]{NocedalWright:2006}. After the new iterate $x_{k+1}$ is computed, the oldest vector pair in the set $\{ s_i, y_i\}_{i=k-m}^{k-1}$ (namely $\{ s_{k-m}, y_{k-m}\}$) is discarded and the new pair $\{ s_k, y_k\}$, obtained from the current iteration, is added. This works according to the strategy “first in, first out”, which means that the vectors that were imported first are also discarded first. In this way, the set of vector pairs $\{ s_i, y_i\}_{i=k-m}^{k-1}$ includes curvature information from the $m$ most recent iterations. Practical experience has shown that modest values of $m$ (between $3$ and $20$) often produce satisfactory results. The strategy of keeping the $m$ most recent pairs $\{ s_i, y_i\}_{i=k-m}^{k-1}$ works well in practice. Indeed no other strategy has yet proved to be consistently better \cite[p.~179]{NocedalWright:2006}. Normally, for large-scale problems, one takes $m \ll n$. In practice, the choice of $m$ depends on the dimension of the problem and the storage of the employed computer \cite[p.~295]{SunYuan:2006}. \\
At iteration $k$, the current iterate is $x_k$ and the set of vector pairs is given by $\{ s_i, y_i\}_{i=k-m}^{k-1}$. At first an initial approximation $B^{(0)}_k$ is chosen for the $k$-th iteration (in contrast to the “ordinary” BFGS iteration, this initial approximation is allowed to vary from iteration to iteration). The formula \cref{LimitedMemoryFormula} is applied $m$ times repeatedly, i.e.
\begin{equation*}
    B^{(j+1)}_k = V^{\mathrm{T}}_{k-m+j} B^{(j)}_k V_{k-m+j} + \rho_{k-m+j} \; s_{k-m+j} s_{k-m+j}^{\mathrm{T}}, \; j = 0,1, \cdots, m-1.
\end{equation*}
The LBFGS approximation, called $B^{LBFGS}_k$, reads the following:
\begin{equation*}
    \begin{split}
        B^{LBFGS}_k = B^{(m)}_k = & V^{\mathrm{T}}_{k-1} B^{(m-1)}_k V_{k-1} + \rho_{k-1} \; s_{k-1} s_{k-1}^{\mathrm{T}}  = \\
        = & \cdots = \\
        = & \; (V^{\mathrm{T}}_{k-1} \cdots V^{\mathrm{T}}_{k-m}) B^{(0)}_k (V_{k-m} V_{k-m+1} \cdots V_{k-1}) + \\
        & + \rho_{k-m} \; (V^{\mathrm{T}}_{k-1} \cdots V^{\mathrm{T}}_{k-m+1}) s_{k-m} s^{\mathrm{T}}_{k-m} (V_{k-m+1} \cdots V_{k-1}) + \\
        & + \rho_{k-m+1} \; (V^{\mathrm{T}}_{k-1} \cdots V^{\mathrm{T}}_{k-m+2}) s_{k-m+1} s^{\mathrm{T}}_{k-m+1} (V_{k-m+2} \cdots V_{k-1}) +\\
        & + \cdots +\\
        & + \rho_{k-1} \; s_{k-1} s^{\mathrm{T}}_{k-1}.
    \end{split}
\end{equation*}
$B^{LBFGS}_k$ is calculated completely from $B^{(0)}_k$ and the vector pairs $\{ s_i, y_i\}_{i=k-m}^{k-1}$. We note that $B^{LBFGS}_k$ must be considered as an approximation of $B^{BFGS}_k$ because only the last $m$ vectors are used for its representation and thus the information from the previous vectors (if $k > m$) is no longer included in its representation. Nevertheless this matrix fulfills the Quasi-Newton equation \cref{quasi-NewtonEquation} \cite[p.~200]{GeigerKanzow:1999}. \\
In fact, there is no need to compute and save $B^{LBFGS}_k$ explicitly. Instead, one only saves the pairs $\{ s_i, y_i\}_{i=k-m}^{k-1}$ and computes $B^{LBFGS}_k g_k = B^{LBFGS}_k \nabla f(x_k)$ in each iteration \cite[p.~293]{SunYuan:2006}. This matrix-vector-product can be obtained by performing a sequence of inner products and vector summations involving $g_k$ and the pairs $\{ s_i, y_i\}_{i=k-m}^{k-1}$ \cite[p.~177]{NocedalWright:2006}. \\
So, we have 
\begin{equation*}
    \begin{split}
        B^{LBFGS}_k g_k = & \; (V^{\mathrm{T}}_{k-1} \cdots V^{\mathrm{T}}_{k-m})  B^{(0)}_k (V_{k-m} V_{k-m+1} \cdots V_{k-1}) g_k +\\
        & + \rho_{k-m} \; (V^{\mathrm{T}}_{k-1} \cdots V^{\mathrm{T}}_{k-m+1}) s_{k-m} s^{\mathrm{T}}_{k-m} (V_{k-m+1} \cdots V_{k-1}) g_k + \\
        & + \rho_{k-m+1} \; (V^{\mathrm{T}}_{k-1} \cdots V^{\mathrm{T}}_{k-m+2}) s_{k-m+1} s^{\mathrm{T}}_{k-m+1} (V_{k-m+2} \cdots V_{k-1}) g_k + \\
        & + \cdots + \\
        & + \rho_{k-1} \; s_{k-1} s^{\mathrm{T}}_{k-1} g_k
    \end{split}
\end{equation*}
\cite[p.~293]{SunYuan:2006}. Since $V_i g_k = g_k - \rho_i \; y_i s_i^{\mathrm{T}} g_k$ for $i = k-1, \cdots, k-m$, one can derive a recursive method to compute $B^{LBFGS}_k g_k$ efficiently \cite[p.~178]{NocedalWright:2006}:

\begin{algorithm}[H]
	\caption{LBFGS two-loop recursion for $B^{LBFGS}_k g_k$.} \label{LBFGSTwo-LoopRecursion}
	\begin{algorithmic}[1]
        \State $q = g_k$
        \For{$i = k-1, k-2, \cdots, k-m$}
            \State $\rho_i = \frac{1}{s^{\mathrm{T}}_{i} y_i}$
            \State $\alpha_i = \rho_i \; s^{\mathrm{T}}_i q$ 
            \State $q = q - \alpha_i y_i$
        \EndFor
        \State $r = B^{(0)}_k q$
        \For{$i = k-m, k-m+1, \cdots, k-1$}
            \State $\beta = \rho_i \; y^{\mathrm{T}}_i r$ 
            \State $r= r + s_i (\alpha_i - \beta)$
        \EndFor
		\State \textbf{Stop with result} $B^{LBFGS}_k g_k = r$.
    \end{algorithmic}
\end{algorithm}

Without considering the multiplication $B^{(0)}_k q$, \cref{LBFGSTwo-LoopRecursion} requires $4mn$ multiplications. If $B^{(0)}_k$ is a diagonal matrix, then $n$ additional multiplications are needed. Apart from being inexpensive, this recursion has the advantage that the multiplication by the initial matrix $B^{(0)}_k$ is isolated from the rest of the computations, which allows this matrix to be chosen freely and to vary between iterations. One may even use an implicit choice of $B^{(0)}_k$ by defining some initial approximation $H^{(0)}_k$ to the Hessian (not its inverse) and obtaining $r$ by solving the system $H^{(0)}_k r = q$ \cite[p.~178]{NocedalWright:2006}. \\
$B^{(0)}_k$ can be any $\spd$ matrix. In general $B^{(0)}_k$ will be a multiple of the identity matrix, so that it can be stored very easily \cite[p.~198]{GeigerKanzow:1999}. A method for choosing $B^{(0)}_k$ that has proven effective in practice is to set $B^{(0)}_k = c_k \; I_{n \times n}$, where
\begin{equation}\label{initialMatrixNorm}
    c_k = \frac{s^{\mathrm{T}}_{k-1} y_{k-1}}{y^{\mathrm{T}}_{k-1} y_{k-1}}.
\end{equation}
The scaling factor $c_k$ attempts to estimate the size of the true Hessian matrix along the most recent search direction. This choice helps to ensure that the search direction $d_k$ is well scaled, and as a result the stepsize $\alpha_k = 1$ is accepted in most iterations. It is important that the line search is based on the (strong) Wolfe conditions, so that the BFGS updating is stable \cite[p.~178-179]{NocedalWright:2006}. \\
These theoretical considerations combined lead to the following algorithm:

\begin{algorithm}[H]
	\caption{LBFGS Method} \label{LBFGSMethod}
    \begin{algorithmic}[1]
        \State Continuously differentiable real-valued function $f$ on $\mathbb{R}^n$, bounded below; initial iterate $x_0 \in \mathbb{R}^n$; convergence tolerance $\varepsilon > 0$; linesearch parameters $0 < c_1 < \frac{1}{2} < c_2 < 1$; memory size $m > 0$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Choose $B^{(0)}_k$ (e.g. $B^{(0)}_k = c_k \; I_{n \times n}$ from \cref{initialMatrixNorm}).
            \State Compute the search direction $d_k = - B^{LBFGS}_k \nabla f(x_k)$ by means of \cref{LBFGSTwo-LoopRecursion}.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies the (strong) Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2} 
            \StatexIndent[2] (or \cref{StrongWolfeCondition}). 
            \State Set $x_{k+1} = x_k + \alpha_k d_k$.
            \If{$k > m$}
                \State Discard the vector pair $\{ s_{k−m},y_{k−m}\}$ from storage. 
            \EndIf
            \State Set $s_k = x_{k+1} - x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$ and add $\{s_k, y_k \}$ into storage.
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

Unlike the conventional quasi-Newton methods, this one follows a different algorithmic sequence. Here, first the approximation of the Hessian inverse $B^{LBFGS}_k$ is calculated for the current iteration $x_k$ and then the next iterate $x_{k+1}$ is calculated. For example, in the “ordinary” BFGS method, \cref{InverseGlobalBFGS-Method}, the new iterate $x_{k+1}$ is calculated first and then the approximation of the Hessian inverse for the new iteration $B^{BFGS}_{k+1}$ is calculated. This explains the idea of this method. Instead of passing the completely calculated matrix, only the vector pairs $\{ s_i, y_i\}_{i=k-m}^{k-1}$ are passed in each iteration and at the beginning the approximation $B^{LBFGS}_k$ is created from these. The matrices $B^{LBFGS}_k$ are not explicitly stored, but only the vector pairs needed for the calculation and the start matrix $B^{(0)}_k$. For small values of $m$ and larger dimensions $n$, the memory requirement for the LBFGS method is thus considerably lower than for the “ordinary” BFGS method itself, namely $\mathcal{O}(mn)$ instead of $\mathcal{O}(n^2)$, which is due to the fact that the stepsize $d_k$ can be obtained with $\mathcal{O}(mn)$ operations \cite[p.~200-201]{GeigerKanzow:1999}. \\
In practical applications of the LBFGS method, the strong Wolfe-Powell stepsize strategy is used. This is because the LBFGS method seems to depend more on the choice of a “good” stepsize $\alpha_k > 0$ and because the “optimal” stepsize can be better approximated if the strong Wolfe conditions \cref{WolfeConditions1} and \cref{StrongWolfeCondition} are satisfied \cite[p.~212-213]{GeigerKanzow:1999}. \\ 
During its first $m − 1$ iterations, the LBFGS method, \cref{LBFGSMethod}, is equivalent to the BFGS method, \cref{InverseGlobalBFGS-Method}, if the initial matrix is the same in both methods (i.e. $B^{LBFGS}_0 = B^{BFGS}_0$) and if we choose $B^{(0)}_k = B^{LBFGS}_0$ in \cref{LBFGSTwo-LoopRecursion} at each iteration of \cref{LBFGSMethod} \cite[p.~179]{NocedalWright:2006}. \\
Before discussing the convergence results, we show that the LBFGS method, \cref{LBFGSMethod}, is well defined:

\begin{theorem}[{\cite[Note~12.3]{GeigerKanzow:1999}}] 
    If $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is continuously differentiable and bounded below, then $s^{\mathrm{T}}_k y_k > 0$ holds for the sequences $\{s_k\}_k$ and $\{y_k\}_k$ generated by the LBFGS method, \cref{LBFGSMethod}. Furthermore, the matrices of the sequence $\{B^{LBFGS}_k\}_k$ are $\spd$ and the LBFGS method, \cref{LBFGSMethod}, is well defined. 
\end{theorem}

The following statement can be made about global convergence: 

\begin{theorem}[{\cite[Theorem~5.7.4]{SunYuan:2006}}]
    Let $f \colon \; \mathbb{R}^n \to \mathbb{R}$ be a twice continuously differentiable and uniformly convex function. Then the iterative sequence $\{x_k\}_k$ generated by the LBFGS method, \cref{LBFGSMethod}, converges to the unique minimizer $x^*$ of $f$. 
\end{theorem}

The following statement can be made about the rate of convergence:

\begin{theorem}[{\cite[Theorem~5.7.7]{SunYuan:2006}}]
    Let $f \colon \; \mathbb{R}^n \to \mathbb{R}$ be a twice continuously differentiable and uniformly convex function. Assume that the iterative sequence $\{x_k\}_k$ generated by the LBFGS method, \cref{LBFGSMethod}, converges to the unique minimizer $x^*$ of $f$. Then the rate of convergence is at least R-linear, i.e. $\limsup_{k \rightarrow \infty} \lVert x_k - x^* \rVert^{1/k} = R$ with $R \in (0,1)$.
\end{theorem}

This theorem indicates that the LBFGS method often converges slowly, which leads to a relatively large number of function evaluations. Also, it is inefficient on highly ill-conditioned optimization problems. Though there are some weaknesses, this method is a main choice for large-scale problems in which the Hessian is not sparse, because, in this case, it may outperform other rival algorithms \cite[p.~300]{SunYuan:2006}. \\
The memoryless BFGS method should also be mentioned. This is a special case of the LBFGS method, where the operator 
\begin{equation*}
    B^{LBFGS}_k = \Big{(} I_{n \times n} - \frac{s_{k-1} y^{\mathrm{T}}_{k-1}}{s_{k-1}^{\mathrm{T}} y_{k-1}} \Big{)} \Big{(} I_{n \times n} - \frac{y_{k-1} s^{\mathrm{T}}_{k-1}}{s_{k-1}^{\mathrm{T}} y_{k-1}} \Big{)} + \frac{s_{k-1} s^{\mathrm{T}}_{k-1}}{s_{k-1}^{\mathrm{T}} y_{k-1}}.
\end{equation*}
is used, which means that $B^{(0)}_k$ is the identity matrix and only the vector pair $\{s_k, y_k\}$ is stored (i.e. $m=1$) in each iteration. This operator satisfies the quasi-Newton equation, \cref{quasi-NewtonEquation}, and the idea is to use only the information from the previous iteration \cite[p.~301]{SunYuan:2006}. \\
The principle of the cautious update, presented in \cref{Section2.5}, can also be applied here by deciding in each iteration on the basis of a decision rule whether the new vectors $\{s_k, y_k\}$ are stored or whether the current memory $\{ s_i, y_i\}_{i=k-m}^{k-1}$ is simply transferred for the next iteration. \\