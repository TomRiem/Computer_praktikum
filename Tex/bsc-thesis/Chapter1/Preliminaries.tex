\section{Preliminaries}
\label{Section2.1}

Before we can discuss variants of the BFGS method, and quasi-Newton methods in general, we introduce certain basic principles and aspects, which will be needed later, in this section. The theoretical concepts have been taken from \cite{GeigerKanzow:1999}, \cite{NocedalWright:2006} and \cite{SunYuan:2006}, where they are discussed in more detail and more background information can be found \\

In the Euclidean optimization a key problem is minimizing a real-valued function $f$ over the Euclidean space $\mathbb{R}^n$ ($n \geq 1$), i.e. our focus and efforts are centred on solving 
\begin{equation}\label{OptimizationProblem}
    \min f(x), \quad x \in \mathbb{R}^n
\end{equation}  
where $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is a smooth function. In this chapter we focus on smooth functions, by which we generally mean functions whose second derivatives exist and are continuous or formally $f \in C^2(\mathbb{R}^n)$, unless otherwise stated. \cref{OptimizationProblem} is called a (nonlinear) unconstrained optimization problem. \\
A solution of \cref{OptimizationProblem} is colloquially called a minimizer, but there are different types of minimizers, which in turn depend on the structure of the optimization problem and thus in particular on the function $f$. The best case would be to find a global minimizer of $f$, i.e. a point $x^*$ such that $f(x^*) \leq f(x)$ for all $x \in \mathbb{R}^n$. A global minimizer is often difficult to find, because the knowledge of $f$ is usually only local and therefore we do not have a good picture of the overall shape of $f$. The methods discussed in this chapter aim to find a local minimizer, i.e. a point $x^*$ around which there is a neighborhood $U$ such that $f(x^*) \leq f(x)$ for all $x \in U$. Such a point is sometimes called a weak local minimizer. This terminology distinguishes it from a strict local minimizer, which is the “winner” in its neighborhood or formally: A point $x^*$ is a strict (or strong) local minimizer if there is a neighborhood $U$ of $x^*$ such that $f(x^*) < f(x)$ for all $x \in U$ with $x \neq x^*$. There is also the so-called isolated local minimizer, which is a point $x^*$ around which there is a neighborhood $U$ of $x^*$ such that $x^*$ is the only local minimizer in $U$. It follows that all isolated local minimizers are strict \cite[p.~12-13]{NocedalWright:2006}. \\
To classify these different minimizers from each other, the following statements are needed: 

\begin{theorem}[{\cite[Theorem~2.2~+~Theorem~2.3~+~Theorem~2.4~+~Theorem~2.5]{NocedalWright:2006}}] \label{Minimizers} \ \\[-1.5\baselineskip]
    \begin{itemize}
        \item If $x^*$ is a local minimizer and $f$ is continuously differentiable in an open neighborhood of $x^*$, then $\nabla f(x^*) = 0$ (first-order necessary condition).
        \item If $x^*$ is a local minimizer of $f$ and $\nabla^2 f$ exists and is continuous in an open neighborhood of $x^*$, then $\nabla f(x^*) = 0$ and $\nabla^2 f (x^*)$ is positive semidefinite (second-order necessary conditions).
        \item Suppose that $\nabla^2 f$ is continuous in an open neighborhood of $x^*$ and that $\nabla f(x^*) = 0$ and $\nabla^2 f (x^*)$ is positive definite. Then $x^*$ is a strict local minimizer of $f$ (second-order sufficient conditions).
        \item When $f$ is convex, any local minimizer $x^*$ is a global minimizer of $f$. If in addition $f$ is differentiable, then any stationary point $x^*$ is a global minimizer of $f$.
    \end{itemize}
\end{theorem}

All methods in this chapter search for a point $x^*$ where the gradient vanishes, i.e. $\nabla f (x^*) = 0$, which is called a stationary point. Any local minimizer is a stationary point. \\

A popular class of methods to solve \cref{OptimizationProblem} are the so-called line search methods, which can be expressed as algorithms. They start with an initial point $x_0 \in \mathbb{R}^n$ and produce a sequence of iterates $\{x_k\}_k$ that we hope will converge towards a minimum of the problem. The algorithms follow the strategy of first determining a search direction $d_k \in \mathbb{R}^n$ (which is a descent direction) and then search along this direction from the current iterate $x_k$ for a new iterate with a lower function value. The distance to move along $d_k$ can be found by (approximately) solving the following one dimensional minimization problem:
\begin{equation}\label{OptimizationProblemStepsize}
    \min_{\alpha > 0} f(x_k + \alpha d_k).
\end{equation}
Most of the benefit of the search direction $d_k$ would be obtained by solving problem \cref{OptimizationProblemStepsize} exactly, but an exact minimization may be expensive and is usually unnecessary. Instead, numerical methods are used to find an approximated minimum of \cref{OptimizationProblemStepsize}. From now on we will refer to an (approximated) solution $\alpha_k > 0$ of \cref{OptimizationProblemStepsize} as stepsize. The next iterate is defined as 
\begin{equation}\label{IterativeUpdateScheme}
    x_{k+1} = x_k + \alpha_k d_k.
\end{equation}
At the new point $x_{k+1}$, a new search direction $d_{k+1}$ and stepsize $\alpha_{k+1}$ are computed, and the process is repeated \cite[p.~19]{NocedalWright:2006}. The method of summing up a scaled search direction in each iteration is often called iterative update scheme. \\

In this chapter the main focus is on the computation of the search direction $d_k$, but we would like to take a closer look at certain aspects of finding an appropriate stepsize $\alpha_k > 0$. The procedure of finding such a stepsize is called line search. There are two different types: A line search which solves \cref{OptimizationProblemStepsize} exactly is called exact line search or optimal line search, and $\alpha_k$ is then called optimal stepsize. If we choose $\alpha_k$ such that the objective function has an acceptable descent amount, i.e. such that the descent $f(x_k) - f(x_k + \alpha_k d_k) > 0$ is acceptable by the user, such a line search is called inexact line search, approximate line search or acceptable line search. \\
In practical computation, an optimal stepsize cannot be found in general, and it is also expensive to find almost an optimal stepsize, therefore the inexact line search with less computation load is highly popular \cite[p.~72]{SunYuan:2006}. \\
In this chapter it is assumed that the stepsize $\alpha_k$ is found by an inexact line search which generates a stepsize that achieves adequate reductions in $f$ at minimal cost. Typical inexact line search algorithms try out a sequence of candidate values for $\alpha$ and stop to accept one of these values when certain conditions are satisfied. \\
A popular inexact line search condition stipulates a stepsize $\alpha_k$ that should first of all give a sufficient decrease in the objective function $f$, as measured by the following inequality:
\begin{equation}\label{WolfeConditions1}
    f(x_k + \alpha_k d_k) \leq f(x_k) + c_1 \alpha_k \nabla f(x_k)^{\mathrm{T}} d_k
\end{equation}
for some constant $c_1 \in (0, 1)$. In other words, the reduction in $f$ should be proportional to the stepsize $\alpha_k$ and the directional derivative $\nabla f(x_k)^{\mathrm{T}} d_k$. \cref{WolfeConditions1} is often called Armijo condition. \\
\cref{WolfeConditions1} is not enough by itself to ensure that the algorithm makes reasonable progress because it is satisfied for all sufficiently small values of $\alpha_k$. To rule out unacceptably short steps a second condition is introduced, which requires $\alpha_k$ to satisfy
\begin{equation}\label{WolfeConditions2}
    \nabla f(x_k + \alpha_k d_k)^{\mathrm{T}} d_k \geq c_2 \nabla f(x_k)^{\mathrm{T}} d_k
\end{equation}
for some constant $c_2 \in (c_1, 1)$ \cite[p.~33]{NocedalWright:2006}. \cref{WolfeConditions2} is often referred to in the literature as “curvature condition”. This can lead to confusion in relation to quasi-Newton methods. \cref{WolfeConditions2} is an approximation of the orthogonal condition $\nabla f(x_{k+1})^{\mathrm{T}} d_k = 0$, which is satisfied by exact line search. The geometric interpretation of \cref{WolfeConditions2} is that the slope $\nabla f(x_k + \alpha_k d_k)^{\mathrm{T}} d_k$, which is simply the derivative of $f(x_k + \alpha_k d_k)$, at the acceptable point must be greater than or equal to some multiple $c_2$ of the initial slope $\nabla f(x_k)$. \cref{WolfeConditions1} and \cref{WolfeConditions2} are known as the Wolfe-Powell inexact line search rule, Wolfe-Powell rule or just Wolfe conditions with $0 < c_1 < c_2 < 1$ \cite[p.~104]{SunYuan:2006}. \\
Unfortunately, one possible disadvantage of \cref{WolfeConditions2} is that it does not reduce to an exact line search in the limit $c_2 \rightarrow 0$. In addition, a stepsize may satisfy the Wolfe conditions without being particularly close to a minimizer of $f(x_k + \alpha d_k)$. But \cref{WolfeConditions2} can be modified to force $\alpha_k$ to be in at least a broad neighborhood of a local minimizer or stationary point of $f(x_k + \alpha d_k)$. The strong Wolfe conditions (or strong Wolfe-Powell rule) require $\alpha_k$ to satisfy \cref{WolfeConditions1} and 
\begin{equation}\label{StrongWolfeCondition}
    \lvert \nabla f(x_k + \alpha_k d_k)^{\mathrm{T}} d_k \rvert \leq c_2 \lvert \nabla f(x_k)^{\mathrm{T}} d_k \rvert
\end{equation}
with $0 < c_1 < c_2 < 1$. The only difference with the Wolfe conditions is that we no longer allow the derivative $\nabla f(x_k + \alpha_k d_k)^{\mathrm{T}} d_k$ to be too positive. Hence, we exclude points that are far from stationary points of $f(x_k + \alpha d_k)$ \cite[p.~34]{NocedalWright:2006}. \\
In practice, $c_1$ is chosen to be quite small, e.g. $c_1 = 10^{-4}$, and a typical value of $c_2$ is $0.9$ if the search direction $d_k$ is chosen by a quasi-Newton method, what will be the case in this chapter. \\
The existence of stepsizes that meet the above mentioned conditions is given by:

\begin{lemma}[{\cite[Lemma~3.1]{NocedalWright:2006}}] \label{WolfeConditionsLemma}
    Suppose that $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is continuously differentiable. Let $d_k$ be a descent direction at $x_k$, and assume that $f$ is bounded below along the ray $\{x_k + \alpha d_k \vert \; \alpha >0 \}$. Then if $0 < c_1 < c_2 < 1$, there exist intervals of stepsizes satisfying the Wolfe conditions and the strong Wolfe conditions.
\end{lemma}

The Wolfe conditions are scale-invariant in a broad sense: Multiplying the objective function by a constant or making an affine change of variables does not alter them. They can be used in most line search methods, and are particularly important in the implementation of quasi-Newton methods \cite[p.~33-35]{NocedalWright:2006}. \\

In the context of stepsizes, the term stepsize strategy is often used. It refers to a map $T \colon \; \mathbb{R}^n \times \mathbb{R}^n \to \mathcal{P} ((0,\infty))$, which assigns a subset $T(x, d)$ of $\mathbb{R}$ to each pair $(x, d)$. We call such a stepsize strategy (under certain conditions) well-defined, if (under these conditions) the set $T(x, d)$ for each pair $(x,d)$ with $\nabla f(x)^{\mathrm{T}}d < 0$ is not empty {\cite[p.~27]{GeigerKanzow:1999}}. A stepsize strategy, which is called Wolfe-Powell stepsize strategy or Wolfe-Powell rule, introduced in \cite{Powell:1975} (see. \cite[Definition~2.2.]{Werner:1978}) meets the Wolfe conditions and belongs to a special subset of stepsize strategies, namely the efficient stepsize strategies introduced by \cite[Definition~0.1]{WarthWerner:1977}: 

\begin{definition}[{\cite[Definition~4.5]{GeigerKanzow:1999}}]\label{EfficientStepSize}
    Let $f \colon \; \mathbb{R}^n \to \mathbb{R}$ be continuously differentiable, $x \in \mathbb{R}^n$ and $d \in \mathbb{R}^n$ a descent direction from $f$ in $x$. A stepsize strategy $T$ is called efficient if there is a constant $\theta > 0$ independent of $x$ and $d$ with 
    \begin{equation*}
        f(x + \alpha d) \leq f(x) - \theta \bigg( \frac{\nabla f(x)^{\mathrm{T}}d}{\lVert d \rVert} \bigg)^2
    \end{equation*}
    for all $\alpha \in T(x,d)$.
\end{definition}

It should be noted that stepsize strategies are only used in theory. In praxis, algorithms generate stepsizes that meet the respective conditions. There are different algorithms that determine a stepsize $\alpha_k$ that meets the (strong) Wolfe conditions. A method, which computes a stepsize $\alpha_k$ that satisfies the (strong) Wolfe conditions, should always try the stepsize $\alpha_k = 1$ first, because this stepsize will eventually always be accepted (under certain conditions) and thereby producing superlinear convergence of the overall algorithm. Computational observations strongly suggest that it is more economical, in terms of function evaluations, to perform a fairly inaccurate line search \cite[p.~142]{NocedalWright:2006}. Such a method is e.g. \cite[Algorithmus~9.3]{UlbrichUlbrich:2012}. \\
The convergence rate of many optimization methods does not depend on the exact line search. Therefore, as long as an acceptable stepsize is chosen which ensures that the objective function has sufficient descent, the exact line search can be avoided and the computing efforts will be decreased greatly \cite[p.~102]{SunYuan:2006}. \\

We are also interested in the convergence behavior of the line search methods presented in this chapter. To obtain global convergence, we must not only have well chosen stepsizes but also well chosen search directions $d_k$. Therefore we present the following theorem, due to \cite{Zoutendijk:1970}, which quantifies the effect of properly chosen stepsizes $\alpha_k$. It describes how far $d_k$ can deviate from the steepest descent direction $-\nabla f(x_k)$ and still produce a globally convergent iteration \cite[p.~38]{NocedalWright:2006}.

\begin{theorem}[{\cite[Theorem~3.2.]{NocedalWright:2006}}]\label{ZoutendijkTheorem}
    Consider any iteration of the form $x_{k+1} = x_k + \alpha_k d_k$, where $d_k$ is a descent direction and $\alpha_k$ satisfies the Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2}. Suppose that $f$ is bounded below in $\mathbb{R}^n$ and that $f$ is continuously differentiable in an open set $\mathcal{N}$ containing the level set $\mathcal{L} = \{ x \in \mathbb{R}^n \colon \; f(x) \leq f(x_0) \}$, where $x_0$ is the starting point of the iteration. Assume also that the gradient $\nabla f$ is Lipschitz continuous on $\mathcal{N}$, that is, there exists a constant $L > 0$ such that
    \begin{equation*}
        \lVert \nabla f(x) - \nabla f(\tilde{x}) \rVert \leq L \lVert x - \tilde{x} \rVert, \quad \text{for all} \; x, \tilde{x} \in \mathcal{N}.
    \end{equation*}
    Then
    \begin{equation}\label{ZoutendijkCondition}
        \sum_{k \geq 0} \cos(\theta_k)^2 \lVert \nabla f(x_k) \rVert^2 < \infty,
    \end{equation}
    where
    \begin{equation*}
        \cos(\theta_k) = - \frac{\nabla f(x_k)^{\mathrm{T}} d_k}{\lVert \nabla f(x_k) \rVert \lVert d_k \rVert}.
    \end{equation*}    
\end{theorem}

\cref{ZoutendijkCondition}, often called Zoutendijk condition, implies
\begin{equation}\label{ZoutendijkConditionLimit}
    \lim\limits_{k \to \infty} \cos(\theta_k)^2 \lVert \nabla f(x_k) \rVert^2 = 0.
\end{equation}
This can be used in turn to derive global convergence results for line search algorithms. If the method for choosing the search direction $d_k$ ensures that the angle $\theta_k$ is bounded away from $90^{\circ}$, there is a positive constant $\delta$ such that $\cos(\theta_k) \geq \delta > 0$. It follows from \cref{ZoutendijkConditionLimit} that
\begin{equation}\label{GlobalConvergentFunction}
    \lim\limits_{k \to \infty} \lVert \nabla f(x_k) \rVert = 0.
\end{equation}
In other words, the gradient norms $\lVert \nabla f(x_k) \rVert$ converge to zero, provided that the search directions $d_k$ are never too close to orthogonality with the gradient. \\
We call a numerical method globally convergent if the property \cref{GlobalConvergentFunction} is satisfied. For line search methods of the form \cref{IterativeUpdateScheme}, the limit \cref{GlobalConvergentFunction} is the strongest global convergence result that can be obtained. It is not guaranteed that the method converges to a minimizer, but only that it is attracted by stationary points. By making additional requirements on the search direction $d_k$, these results can be strengthened to include convergence to a local minimum \cite[p.~39-40]{NocedalWright:2006}. \\

As an important comparative feature for iterative methods of solving an optimization problem, the rate of convergence from the resulting $\{ x_k \}_k$ sequence is taken. A sequence $\{ x_k \}_k$ converges to $x^*$ (at least) superlinear, if a zero sequence $\{ \varepsilon_k \}_k \subseteq \mathbb{R}_+$ exists such that $\lVert x_{k+1} - x^* \rVert \leq \varepsilon_k \lVert x_k - x^* \rVert$ for all $k \in \mathbb{N}$, or equivalent $\lVert x_{k+1} - x^* \rVert = o(\lVert x_k - x^* \rVert)$, which means
\begin{equation*}
    \lim\limits_{k \rightarrow \infty} \frac{\lVert x_{k+1} - x^* \rVert}{\lVert x_k - x^* \rVert} = 0.
\end{equation*}
In order to better understand certain statements about the convergence rate of the methods presented in this chapter, we present two theorems which provide hints about superlinear convergence:

\begin{corollary}[{\cite[Korollar~7.9.]{GeigerKanzow:1999}}]\label{corGeigerKanzow7.9}
    Let $f \colon \; \mathbb{R}^n \to \mathbb{R}$ be twice continuously differentiable, $\{H_k\}_k$ a sequence of regular matrices in $\mathbb{R}^{n \times n}$, $x_0 \in \mathbb{R}^n$ and $\{x_k\}_k \subseteq \mathbb{R}^n$ a sequence defined by
    \begin{equation*}
        x_{k+1} = x_k - H^{-1}_k \nabla f(x_k), \; k=0, \; 1,\; \cdots
    \end{equation*}
    with the limit $\lim\limits_{k \rightarrow \infty}{x_k} = x^*$, $x_k \neq x^*$ for all $k \in \mathbb{N}$ and $\nabla^2 f(x^*)$ regular. Then the following statements are equivalent
    \begin{enumerate}
        \item $\{x_k\}_k \to x^*$ superlinear and $\nabla f(x^*) = 0$.
        \item $\lVert (\nabla^2 f(x_k) - H_k)(x_{k+1}-x_k) \rVert = o( \lVert x_{k+1}-x_k \rVert )$. \label{corGeigerKanzow7.9Item2}
        \item $\lVert (\nabla^2 f(x^*) - H_k)(x_{k+1}-x_k) \rVert = o( \lVert x_{k+1}-x_k \rVert )$. \label{corGeigerKanzow7.9Item3}
    \end{enumerate}
\end{corollary}

These equivalent statements are often referred to as Dennis-Mor\'{e} condition. \cref{corGeigerKanzow7.9} shows that for superlinear convergence it is only important that the application of $H_k$ to the direction $x_{k+1}-x_k$ must be approximately the same as applying the matrix $\nabla^2 f(x^*)$ or $\nabla^2 f(x_k)$ to this vector. It is therefore not necessary that $H_k$ approximates the entire Hessian matrix $\nabla^2 f(x_k)$ well. We would like to point out that in \cref{corGeigerKanzow7.9} it is assumed that the stepsize is constant, i.e. $\alpha_k = 1$. There is also a generalized version with a non-constant stepsize, i.e. $x_{k+1} = x_k + \alpha_k d_k$ where $d_k = - H^{-1}_k \nabla f(x_k)$. For that we have similar equivalent statements under the same assumptions. In \cref{corGeigerKanzow7.9Item2} and \cref{corGeigerKanzow7.9Item3} only $x_{k+1}-x_k$ is replaced by $d_k$ and it must be additionally required that $\alpha_k \rightarrow 1$ applies (see \cite[Aufgabe~7.4.]{GeigerKanzow:1999}). \\
The next statement is closely related to \cref{corGeigerKanzow7.9} (and of course to \cite[Aufgabe~7.4.]{GeigerKanzow:1999}) and is also due to Dennis and Mor\'{e}. It shows under additional assumptions that the stepsize $\alpha_k = 1$ for sufficiently large $k$ meets the Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2}:

\begin{theorem}[{\cite[Theorem~3.6]{NocedalWright:2006}}] \label{DennisMoreCondition}
    Suppose that $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is twice continuously differentiable. Consider the iteration $x_{k+1} = x_k + \alpha_k d_k$, where $d_k$ is a descent direction and $\alpha_k$ satisfies the Wolfe conditions with $c_1 \leq \frac{1}{2}$. If the sequence $\{x_k\}_k$ converges to a point $x^*$ such that $\nabla f(x^*) = 0$ and $\nabla^2 f(x^*)$ is positive definite, and if the search direction satisfies
    \begin{equation}\label{DennisMoreLimit1}
        \lim\limits_{k \rightarrow \infty}{\frac{\lVert \nabla f(x_k) + \nabla^2 f(x_k)d_k \rVert}{\lVert d_k \rVert}} = 0
    \end{equation}
    then
    \begin{enumerate}
        \item the stepsize $\alpha_k = 1$ is admissible for all $k$ greater than a certain index $k_0$ and
        \item if $\alpha_k = 1$ for all $k>k_0$, $\{x_k\}_k$ converges to $x^*$ superlinearly.
    \end{enumerate}
\end{theorem} 

To calculate the search direction $d_k$ we will later need a so-called rank-two update of matrices. This means that a “new” matrix is obtained by adding two matrices with rank one to an “old” one. Since we will also be interested in the inverse of this matrix, we introduce the so-called Sherman-Morrison-Woodbury formula:

\begin{theorem}[{\cite[Theorem~1.2.16]{SunYuan:2006}}] 
    Let $A \in \mathbb{R}^{n \times n}$ be a nonsingular matrix, $U, V \in \mathbb{K}^{n \times m}$. If $I_{n \times n} + V^* A^{-1} U$ (where $V^*$ is the complex conjugate of $V$) is invertible, then $A + U V^*$ is invertible and
    \begin{equation}\label{Sherman-Morrison-WoodburyFormula}
        (A + U V^*)^{-1} = A^{-1} - A^{-1} U (I_{n \times n} + V^* A^{-1} U)^{-1} V^* A^{-1}.
    \end{equation}
\end{theorem} 

In fact, \cref{Sherman-Morrison-WoodburyFormula} only gives us an explicit formula of the inverse of a rank-one updated matrix. But, by considering the updates separately as rank one-updates, this formula can be extended to any number of rank-updates \cite[p.~70]{UlbrichUlbrich:2012}.