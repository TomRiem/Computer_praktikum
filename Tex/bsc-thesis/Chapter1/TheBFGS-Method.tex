\section{The BFGS Method}
\label{Section2.4}

We present a globalized BFGS method. For that, it must be ensured that the curvature condition, \cref{CurvatureCondition}, is fulfilled by choosing the stepsize accordingly. The globalization of the BFGS method is similar to the globalization of Newton's method. In contrast the Wolfe-Powell stepsize strategy, which ensures $s^{\mathrm{T}}_k y_k > 0$ for all $k \in \mathbb{N}_0$, is chosen and not the Armijo rule (a stepsize strategy which determines a stepsize $\alpha_k$ only satisfying \cref{WolfeConditions1}). \\
We call the following algorithm “Inverse Global BFGS Method” because the updating formula for the approximation of the Hessian inverse ($B^{BFGS}_k \mapsto B^{BFGS}_{k+1}$) is used, since we are spared the solving of a system of equations and we only have to work with matrix-vector-multiplications. The algorithm could be formulated with the approximation of the actual Hessian, $H^{BFGS}_k$, but that would increase the effort again to $\mathcal{O}(n^3)$, which is not desirable \cite[p.~141]{NocedalWright:2006}. In practice, it must be decided whether solving a system of equations or matrix-vector-multiplication is more advantageous for the underlying problem. We assume in this thesis that the latter is the better choice. \\

\begin{algorithm}[H]
    \caption{Inverse Global BFGS Method}\label{InverseGlobalBFGS-Method}
    \begin{algorithmic}[1]
        \State Continuously differentiable real-valued function $f$ on $\mathbb{R}^n$, bounded below; initial iterate $x_0 \in \mathbb{R}^n$; initial $\spd$ matrix $B^{BFGS}_0 \in \mathbb{R}^{n \times n}$; convergence tolerance $\varepsilon > 0$; linesearch parameters $0 < c_1 < \frac{1}{2} < c_2 < 1$. Set $k = 0$.
        \While{$\lVert \nabla f(x_k) \rVert > \varepsilon$}
            \State Compute the search direction $d_k = - B^{BFGS}_k \nabla f(x_k)$.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies the (strong) Wolfe conditions \cref{WolfeConditions1} and \cref{WolfeConditions2} 
            \StatexIndent[2] (or \cref{StrongWolfeCondition}). 
            \State Set $x_{k+1} = x_k + \alpha_k d_k$.
            \State Set $s_k = x_{k+1} - x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$.
            \State Compute $B^{BFGS}_{k+1} \in \mathbb{R}^{n \times n}$ by means of \cref{inverseBFGSformula}. 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

It can be shown that \cref{InverseGlobalBFGS-Method} is well defined: 

\begin{theorem}[{\cite[Theorem~11.37]{GeigerKanzow:1999}}]
    Let $f \colon \; \mathbb{R}^n \to \mathbb{R}$ be continuously differentiable and bounded from below. Then for the globalized BFGS method, \cref{InverseGlobalBFGS-Method}:
    \begin{enumerate}
        \item $s^{\mathrm{T}}_k y_k > 0$ for all $k \in \mathbb{N}$.
        \item The matrices $B^{BFGS}_k$ are $\spd$ for all $k \in \mathbb{N}$.
        \item The method is well defined. 
    \end{enumerate}
\end{theorem}

The derivation to this theorem in \cite{GeigerKanzow:1999} shows that finding a stepsize $\alpha_k$, which satisfies the Wolfe conditions (\cref{WolfeConditions1} and \cref{WolfeConditions2}) or strong Wolfe conditions (\cref{WolfeConditions1} and \cref{StrongWolfeCondition}), is crucial \cite[p.~166]{GeigerKanzow:1999}. \cref{CurvatureConditionWolfe} shows that the stepsize ensures the curvature condition $s^{\mathrm{T}}_k y_k > 0$. This in turn ensures that the positive-definiteness of the matrix $B^{BFGS}_k$ is passed to $B^{BFGS}_{k+1}$, see \cref{thmUlbrichUlbrich13.4}. \\
Let us now turn to the convergence results of \cref{InverseGlobalBFGS-Method}. It is desirable that each limit point of a sequence $\{x_k\}_k$, generated by \cref{InverseGlobalBFGS-Method}, is a stationary point of $f$ and that we get locally superlinear convergence. Unfortunately, neither of these statements is true in general \cite[p.~167]{GeigerKanzow:1999}. \\
Let us first deal with the global convergence. The difficulty and importance of the convergence problem of whether the BFGS method with the Wolfe-Powell stepsize strategy converges globally for general functions has been addressed in many situations. But recent studies provide a negative answer to it for nonconvex functions (see e.g. \cite{Dai:2002} or \cite{Mascarenhas:2004}) \cite[p.~3]{Dai:2012}. \\
We present a statement about global convergence from \cite{NocedalWright:2006}, which is based on results of \cite{Powell:1975} and is widely known. For that, we require that the objective function is convex. To be more precise, the following assumptions must be made for a reasonable convergence statement:

\begin{assumption}[{\cite[Assumption~6.1.]{NocedalWright:2006}}]\label{AssumptionsGlobalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The objective function $f$ is twice continuously differentiable.
        \item The level set $\mathcal{L} = \{ x \in \mathbb{R}^n \colon \; f(x) \leq f(x_0) \}$ is convex, and there exist positive constants $m$ and $M$ such that \begin{equation*} m \lVert z \rVert^2 \leq z^{\mathrm{T}} \nabla^2 f(x) z \leq M \lVert z \rVert^2 \end{equation*} for all $z \in \mathbb{R}^n$ and $x \in \mathcal{L}$.
    \end{enumerate}
\end{assumption}

The second part of \cref{AssumptionsGlobalConvergence} implies that $\nabla^2 f(x)$ is positive definite on $\mathcal{L}$ and that $f$ has an unique minimizer $x^*$ in $\mathcal{L}$ \cite[p.~153]{NocedalWright:2006}.

\begin{theorem}[{\cite[Theorem~6.5.]{NocedalWright:2006}}]\label{GlobalConvergence}
    Let $H_0$ be any symmetric positive definite initial matrix, and let $x_0$ be a starting point for which \cref{AssumptionsGlobalConvergence} is satisfied. Then the sequence $\{ x_k \}_k$ generated by \cref{InverseGlobalBFGS-Method} (with $\varepsilon = 0$) converges to the minimizer $x^*$ of $f$.
\end{theorem}

The proof can be found in \cite[p.~154]{NocedalWright:2006}. Nevertheless, it should be noted that \cref{ZoutendijkCondition}, the Zoutendijk condition, is crucial to prove global convergence. \\
We see that the BFGS Method with the Wolfe-Powell stepsize strategy applied to a smooth convex function $f$ from an arbitrary starting point $x_0 \in \mathbb{R}^n$ and from any initial approximation $B^{BFGS}_0 \in \mathbb{R}^{n \times n}$, that is $\spd$, is globally convergent. This is a very strong convergence result for the BFGS method, and it is currently not known whether this also applies to the DFP method \cite[p.~156]{NocedalWright:2006}. \\
It can also be shown that \cref{InverseGlobalBFGS-Method} not only converges globally by using the Wolfe-Powell stepsize strategy but also by using a great number of inexact, efficient stepsize strategies (see \cref{EfficientStepSize}) used in practice for uniformly convex objective functions, which can be seen as an indication of the numerical stability of this method \cite[p.~327]{Werner:1978}. \\
The local BFGS method (a variant where the stepsize is always equal one, i.e. $\alpha_k = 1$ for all $k$) achieves superlinear convergence to a point that meets the second-order sufficient conditions (see \cref{Minimizers}) \cite[Satz~11.33]{GeigerKanzow:1999}. The crux, however, is that this stepsize $\alpha_k = 1$ must satisfy the Wolfe conditions to be accepted and thus we get superlinear convergence for \cref{InverseGlobalBFGS-Method}. \\
In practical implementations of \cref{InverseGlobalBFGS-Method} this unit stepsize is usually used as the first trial stepsize. Under suitable assumptions on $f$, this stepsize will be accepted by the line search as the iterates tend to the solution and will enable superlinear convergence \cite[p.~6]{Dai:2012}. In \cite{DennisMore:1974} this idea is pursued and a detailed derivation is given. But we want to present a very well-known result from \cite{NocedalWright:2006}, which applies to general objective functions and is also based on the results of \cite{DennisMore:1974}. 

\begin{assumption}[{\cite[Assumption~6.2.]{NocedalWright:2006}}]\label{AssumptionsSuperlinearConvergence}
    The Hessian matrix is Lipschitz continuous at $x^*$, that is,
    \begin{equation*}
        \lVert \nabla^2 f(x) - \nabla^2 f(x^*) \rVert \leq L \lVert x - x^* \rVert,
    \end{equation*}
    for all $x$ near $x^*$, where $L$ is a positive constant.
\end{assumption}

\begin{theorem}[{\cite[Theorem~6.6.]{NocedalWright:2006}}]\label{SuperlinearConvergence}
    Suppose that $f$ is twice continuously differentiable and that the iterates $\{ x_k \}_k$ generated by \cref{InverseGlobalBFGS-Method} converge to a minimizer $x^*$ at which \cref{AssumptionsSuperlinearConvergence} holds. Suppose also that
    \begin{equation}\label{SumSuperlinearConvergence}
        \sum^{\infty}_{k=0} \lVert x - x^* \rVert_2 < \infty
    \end{equation}
    holds. Then $\{ x_k \}_k$ converges to $x^*$ at a superlinear rate.
\end{theorem}

A note on the proof: it is shown that \cref{corGeigerKanzow7.9Item3} from \cref{corGeigerKanzow7.9} holds and if $d_k$ is a quasi-Newton direction, then is 
\begin{equation}\label{DennisMoreLimit2}
    \lim_{k \to \infty} \frac{\lVert (H_k - \nabla^2 f(x^*)) d_k \rVert}{\lVert d_k \rVert} = 0
\end{equation}
equivalent to \cref{DennisMoreLimit1}. Therefore \cref{DennisMoreCondition} holds and implies that the unit stepsize $\alpha_k = 1$ will satisfy the Wolfe conditions near the solution, and hence that the rate of convergence is superlinear. So we see, that \cref{DennisMoreLimit2} is both necessary and sufficient for the superlinear convergence of quasi-Newton methods \cite[p.~47]{NocedalWright:2006}. \\
In practice it is often observed that the sequence of matrices $\{ H^{BFGS}_k \}_k$, generated by \cref{InverseGlobalBFGS-Method}, using the direct formula \cref{directBFGSformula} converges to $\nabla^2 f(x^*)$. In this case, the superlinear convergence follows directly from \cref{corGeigerKanzow7.9}, \cite[p.~167-168]{GeigerKanzow:1999}. \\

% In fact, the convergence of  $\{ H^{BFGS}_k \}_k$ against the exact Hesse matrix $\nabla^2 f(x^*)$ can be proved for certain classes of functions, not only for the BFGS method, but for a whole series of further quasi-Newton methods. The problem is, for generic functions the convergence of the sequence $\{H^{BFGS}_k \}_k$ against $\nabla^2 f(x^*)$ cannot be proven. Even if the sequence $\{H^{BFGS}_k \}_k$ converges, it does not necessarily converge against the Hessian $\nabla^2 f(x^*)$. Fortunately, this is not necessary to prove superlinear convergence \cite[p.~167-168]{GeigerKanzow:1999}. 

% In \cite{GeigerKanzow:1999} an idea for proving superlinear convergence was presented without depending on th requirement that the iterates $\{ x_k \}_k$ converge to a minimizer $x^*$, but it brings us back to convexity. Nevertheless it should be mentioned. \\
% It can be shown that \cref{InverseGlobalBFGS-Method} converges locally superlinear for twice continuous differentiable and uniformly convex functions with any start vector $x_0 \in \mathbb{R}^n$ and any $\spd$ start matrix $B^{BFGS}_0 \in \mathbb{R}^{n \times n}$. For that we need \cref{AssumptionsSuperlinearConvergence} and we have to assume that the stepsize $\alpha_k = 1$ is always chosen, if it is sufficient for the Wolfe conditions. It can be shown that locally this is always the case for the globalized BFGS method. Due \cref{GlobalConvergence} we know that the the sequence $\{ x_k \}_k$ generated by \cref{InverseGlobalBFGS-Method} converges to a minimizer $x^*$. The trick for proving superlinear convergence is now to show that $\{ x_k \}_k$ already converges sufficiently fast towards $x^*$, so that \cref{SumSuperlinearConvergence} holds, which implies that  \cite[p.~174]{GeigerKanzow:1999}. \\