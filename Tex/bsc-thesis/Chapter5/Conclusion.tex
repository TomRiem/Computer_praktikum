\chapter{Conclusion}
\label{Chapter6}

This thesis has set itself the goal of examining the BFGS method and variants of it on Riemannian manifolds. First the quasi-Newton methods for the Euclidean case were presented  in general, the BFGS formula and its properties were considered, the resulting BFGS method was examined and its convergence behavior was summarized. Because of the requirement of convexity of the objective function, the cautious BFGS method from \cite{LiFukushima:2001} was discussed and the limited-memory BFGS method, which proved itself in practical application due to memory advantages, was explained. Statements on their convergence have been provided for both methods. The most important components about differential geometry in general and about Riemannian manifolds in particular were summarized to prepare the necessary structure for Riemannian line search methods. Quasi-Newton methods on Riemannian manifolds in general were considered, a derivation of a quasi-Newton equation was generalized to Riemannian manifolds and it was shown that in each iteration of a Riemannian quasi-Newton method, which uses the exponential map as retraction and parallel transport as vector transport, the Riemannian curvature condition is satisfied for $\mu$-strongly convex functions. The RBFGS formula for operators on tangent spaces was derived and its properties were studied. The general RBFGS algorithm was presented, but since the abstract definitions of vector transport and retraction did not allow statements about convergence, the RBFGS method form \cite{Qi:2011} was presented, which allows a generalization of the Euclidean convergence results by the intuitive use of exponential map and parallel transport. As these operations are in general not always given, the RBFGS method form \cite{HuangGallivanAbsil:2015} was presented, which allows the use of other isometric vector transports with associated retraction by introducing the locking condition, as this guarantees the fulfillment of the Riemannian curvature condition. In order to get away from the convexity assumption for proving global convergence of the RBFGS methods and at the same time to avoid the dependence of a vector transport by differentiated retraction, we considered the cautious RBFGS method from \cite{HuangAbsilGallivan:2018} and could see that similar convergence results can be achieved as for the Euclidean cautious BFGS method from \cite{LiFukushima:2001}. A detailed generalization of the LBFGS method to Riemannian manifolds was made and core aspects were discussed. An idea for the realization of an implementation of the RBFGS update was presented, which avoids the actual update of an operator. The advantage is that the update of an operator can be reduced to the update of a matrix, which simplifies the implementation. It was shown that this approach leads to an equivalent computation of the search direction and therefore a method which follows this approach is well defined. The \lstinline!quasi_Newton()!-function from the Julia package \lstinline!Manopt.jl! was presented and explained how it is executed. If a variant of the RBFGS method is used, where the update of the approximating operator on the tangent space is required, the method used in the function follows the approach of representing the operator as a matrix. The potential of the RBFGS method from the Julia package \lstinline!Manopt.jl! was illustrated using Rayleigh quotient minimization problem on $\mathbb{S}^{99}$ and on $\mathbb{S}^{299}$, comparison to the results from \cite{Qi:2011} were drawn. The results showed that the RBFGS method from the package \lstinline!Manopt.jl!, which uses the exponential map and the parallel transport, is fast and stable. It can be assumed that it is therefore a reasonable choice as a numerical method for solving optimization problems defined on Riemannian manifolds, where these operations are available. The potential of the cautious LRBFGS method from the Julia package \lstinline!Manopt.jl! was illustrated using a range of problem sizes for minimizing the Brockett cost function and comparisons to the results from \cite{HuangGallivanAbsil:2015} were drawn. The results showed that although the method converges, it performs much worse than the LRBFGS method from \cite{HuangGallivanAbsil:2015} in terms of average time. This leads to the conclusion that the locking condition is also numerically very important and should therefore be implemented in the future. The efficient computation of the stepsize has also become a central issue due to this experiment. Especially if the selected vector transport is not simultaneously the vector transport by differentiated retraction of the selected retraction and isometric, then the stepsize computation does not ensure the fulfillment of the curvature condition.\\
In this thesis the core aspects of the Riemannian BFGS method were discussed, certain variants, which try to improve different aspects, were presented. Comparisons of key properties of the corresponding Euclidean counterparts were drawn. The importance of the choice of a vector transport with associated retraction, which fulfill the locking condition, was discussed. An idea for the implementation of the update formula was presented and numerical experiments with a method using this approach were performed. Nevertheless, other aspects remain to be addressed for the future. Above all, no statements about the convergence behavior of the LRBFGS method could be presented in this thesis. It remains to be assumed that the results from the Euclidean case can simply be generalized to the Riemannian setup, this would be worth further consideration. In the package \lstinline!Manifolds.jl!, vector transports with associated retraction which fulfill the locking condition are generally not implemented. Since exponential map and parallel transport are not always available or computationally too expensive, a way must be found to ensure that the methods of the \lstinline!quasi_Newton()!-function from the Julia package \lstinline!Manopt.jl! generate descent directions in each iteration. An efficient computation of an appropriate stepsize must also be done.


\begin{lstlisting}
    using Manopt, Manifolds, Random
    Random.seed!(42)
    A = randn(100,100)
    A_symm = (A+A')/2
    M = Sphere(99)
    F(X::Array{Float64,1}) = X'*A_symm*X
    grad_F(X::Array{Float64,1}) = 2*(A_symm*X-X*X'*A_symm*X)
    x = random_point(M)
        
    quasi_Newton(M, F, grad_F, x;
    memory_size = -1,
    stopping_criterion =
    StopWhenGradientNormLess(norm(M, x, grad_F(x)) * 10^(-6)))
\end{lstlisting}