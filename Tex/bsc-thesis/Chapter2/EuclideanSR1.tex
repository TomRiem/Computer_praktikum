\chapter{The Euclidean Symmetric Rank-One Quasi-Newton Method}

In the Euclidean optimization a key problem is minimizing a real-valued function $f$ over the Euclidean space $\mathbb{R}^n$ ($n \geq 1$), i.e. our focus and efforts are centred on solving 
\begin{equation}\label{OptimizationProblem}
    \min f(x), \quad x \in \mathbb{R}^n
\end{equation}  
where $f \colon \; \mathbb{R}^n \to \mathbb{R}$ is a smooth function. In this chapter we focus on smooth functions, by which we generally mean functions whose second derivatives exist and are continuous or formally $f \in C^2(\mathbb{R}^n)$, unless otherwise stated. \cref{OptimizationProblem} is called a (nonlinear) unconstrained optimization problem. \\
In this work we consider numerical methods belonging to the class of quasi-Newton methods, which in turn belong to the class of line search methods. These can be formulated as algorithms where the next iterate is obtained by the iterative update scheme

\begin{equation*}
    x_{k+1} = x_k + \alpha_k d_k.
\end{equation*}

This means these methods start with an initial point $x_0 \in \mathbb{R}^n$ and produce a sequence of iterates $\{x_k\}_k$ that we hope will converge towards a minimum of \cref{OptimizationProblem}. The algorithms follow the strategy of first determining a search direction $d_k \in \mathbb{R}^n$ and then a suitable stepsize $\alpha_k > 0$ is searched for along this search direction $d_k$. \\
In quasi-Newton methods, 

\begin{equation*}
    d_k = -{H_k}^{-1} \nabla f(x_k) = -B_k \nabla f(x_k)
\end{equation*}

is chosen as search direction, where the matrix $H_k \in \mathbb{R}^{n \times n}$ approximates the action of the objective's Hessian $\nabla^{2} f(\cdot)$ in the direction of $s_k$ at the current iterate $x_k$ and $B_k = {H_k}^{-1}$, which means that $B_k$ approximates the action of ${\nabla^{2} f(x_k)}^{-1}$ in the direction of $s_k$. These matrices are not calculated anew in each iteration, but $H_k$ or $B_k$ are updated to new matrices $H_{k+1}, B_{k+1} \in \mathbb{R}^{n \times n}$ using the information obtained during the iteration about the curvature of the objective function $f$. It is required that matrices generated by the update fulfil the so-called quasi-Newton equation, which reads as 

\begin{equation*}
    H_{k+1} (x_{k+1} - x_k) = \nabla f(x_{k+1}) - \nabla f(x_k) \quad \text{or} \quad B_{k+1} (\nabla f(x_{k+1}) - \nabla f(x_k)) = x_{k+1} - x_k.
\end{equation*}

For the sake of simplicity, we introduce the notations $s_k = x_{k+1} - x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$, thus we obtain

\begin{equation}\label{quasi-NewtonEquation}
    H_{k+1} s_k = y_k \quad \text{or} \quad B_{k+1} y_k = s_k.
\end{equation}

The fulfillment of the quasi-Newton equation is the distinguishing feature of quasi-Newton methods. This means that quasi-Newton methods, like steepest descent, require only the gradient of the objective function to be supplied at each iterate. \\
The idea now is to find a convenient formula for updating the matrix H, which produces a matrix that satisfies the quasi-Newton equation and also carries other positive properties for the method. Instead of recomputing the approximate Hessian (or inverse Hessian) from scratch at every iteration, we apply a simple modification that combines the most recently observed information about the objective function with the existing knowledge embedded in our current Hessian approximation \cite[p.~139]{NocedalWright:2006}. \\
There are different formulas for updating the matrix, which of course differentiates the quasi-Newton methods. Probably the best-known method is based on the BFGS update, where the updated matrix Bk+1 (or Hk+1) differs from its
predecessor Bk (or Hk ) by a rank-2 matrix:

\begin{equation*}
    H^{BFGS}_{k+1} = H^{BFGS}_k + \frac{y_k y^{\mathrm{T}}_k}{s^{\mathrm{T}}_k y_k} - \frac{H^{BFGS}_k s_k s^{\mathrm{T}}_k H^{BFGS}_k}{s^{\mathrm{T}}_k H^{BFGS}_k s_k}
\end{equation*}

or 

\begin{equation}\label{InverseBFGSformula}
    B^{BFGS}_{k+1} = \Big{(} I_{n \times n} - \frac{s_k y^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} B^{BFGS}_k \Big{(} I_{n \times n} - \frac{y_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k} \Big{)} + \frac{s_k s^{\mathrm{T}}_k}{s_k^{\mathrm{T}} y_k}.
\end{equation}

There is a simpler rank-1
update that maintains symmetry of the matrix and allows it to satisfy the secant equation.
Unlike the rank-two update formulae, this symmetric-rank-1, or SR1, update does not
guarantee that the updated matrix maintains positive definiteness. Good numerical results
have been obtained with algorithms based on SR1, so we derive it here and investigate its
properties.

The sequence of Hessian approximations generated by the SR1 method converges to the true Hessian under mild conditions, in theory; in practice, the approximate Hessians generated by the SR1 method show faster progress towards the true Hessian than do popular alternatives (BFGS or DFP), in preliminary numerical experiments.