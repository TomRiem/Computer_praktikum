\section{The Riemannian BFGS Method}
\label{Section4.4}

The general form of the Riemannian BFGS method, short RBFGS method, which uses only abstract operations on the manifold, is 

\begin{algorithm}[H]
    \caption{General Form of the RBFGS method}\label{InverseGlobalRiemannianBFGS-General}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; vector transport $\vectorTransportSymbol$ with associated retraction $\retractionSymbol$; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial positive definite and self-adjoint operator $\mathcal{B}^{RBFGS}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; convergence tolerance $\varepsilon > 0$; linesearch parameters $0 < c_1 < \frac{1}{2} < c_2 < 1$. Set $k = 0$.
        \While{$\lVert \operatorname{grad} f(x_k) \rVert_{x_k} > \varepsilon$}
            \State Compute the search direction $\eta_k = - \mathcal{B}^{RBFGS}_k [\operatorname{grad} f(x_k)]$.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1}. 
            \State Set $x_{k+1} = \retract{x_k}(\alpha_k \eta_k)$.
            \State Set $\widetilde{\mathcal{B}}^{RBFGS}_k = \vectorTransportDir{x_k}{\alpha_k \eta_k} \circ \mathcal{B}^{RBFGS}_k \circ {\vectorTransportDir{x_k}{\alpha_k \eta_k}}^{-1}$, $s_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)$ and 
            \StatexIndent[2] $y_k = \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))$.
            \State Compute $\mathcal{B}^{RBFGS}_{k+1} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ by means of \cref{RiemannianInverseBFGSFormula}. 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

In the previous sections, we have seen that the choice of retraction and vector transport seems to be restricted. This is on the one hand, because it is required, that the stepsize $\alpha_k > 0$ meets \cref{RiemannianWolfeConditions2.1}, in which the vector transport by differentiated retraction, $\vectorTransportSymbol^{\retractionSymbol}$ is used, and on the other hand, because an isometric vector transport, $\vectorTransportSymbol^{S}$, must be used in the update \cref{RiemannianInverseBFGSFormula} for preserving the positive-definiteness and self-adjointness (see \cref{RiemannianPositiveDefiniteUpdate}). The optimal case would be, if the vector transport by differentiated retraction is isometric, i.e. $\vectorTransportSymbol^{\retractionSymbol} = \vectorTransportSymbol^{S}$. Then a stepsize $\alpha_k > 0$ that meets \cref{RiemannianWolfeConditions2.1}, would lead to the fulfillment of the curvature condition, \cref{RiemannianCurvatureCondition} (see \cref{RiemannianWolfeConditionResult}), and the vector transport by differentiated retraction, $\vectorTransportSymbol^{\retractionSymbol}$, would lead to a positive update \cref{RiemannianInverseBFGSFormula}, since it is isometric. Unfortunately, this is generally not the case, but nevertheless there are RBFGS methods, which are well defined by a certain choice of retraction and vector transport, which we will present in this section. \\
We first present the most natural BFGS method on Riemannian manifolds, which uses exponential map, $\expOp$, as retraction and parallel transport, $\parallelTransportSymbol$, as vector transport. This Riemannian BFGS method, RBFGS method for short, was first presented in \cite{Gabay:1982}. This intuitive choice of exponential map and parallel transport allows to generalize the convergence statements of the Euclidean case. The following algorithm is a modification of \cite[Algorithm~2]{Qi:2011}:

\begin{algorithm}[H]
    \caption{Inverse Global RBFGS Method}\label{InverseGlobalRiemannianBFGS-Method}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial positive definite and self-adjoint operator $\mathcal{B}^{RBFGS}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; convergence tolerance $\varepsilon > 0$; linesearch parameters $0 < c_1 < \frac{1}{2} < c_2 < 1$. Set $k = 0$.
        \While{$\lVert \operatorname{grad} f(x_k) \rVert_{x_k} > \varepsilon$}
            \State Compute the search direction $\eta_k = - \mathcal{B}^{RBFGS}_k [\operatorname{grad} f(x_k)]$.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1}. 
            \State Set $x_{k+1} = \exponential{x_k} (\alpha_k \eta_k)$.
            \State Set $\widetilde{\mathcal{B}}^{RBFGS}_k = \parallelTransportDir{x_k}{\alpha_k \eta_k} \circ \mathcal{B}^{RBFGS}_k \circ {\parallelTransportDir{x_k}{\alpha_k \eta_k}}^{-1}$, $s_k = \parallelTransportDir{x_k}{\alpha_k \eta_k} (\alpha_k \eta_k)$ and 
            \StatexIndent[2] $y_k = \operatorname{grad} f(x_{k+1}) - \parallelTransportDir{x_k}{\alpha_k \eta_k} (\operatorname{grad} f(x_k))$.
            \State Compute $\mathcal{B}^{RBFGS}_{k+1} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ by means of \cref{RiemannianInverseBFGSFormula}. 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

As with the Euclidean BFGS algorithm, \cref{InverseGlobalRiemannianBFGS-Method} can also be formulated to work with the direct Hessian approximation $\mathcal{H}^{RBFGS}_k$ using \cref{directRiemannianBFGSFormula}. This yields a mathematically equivalent algorithm. Again, the question arises, which variant is more suitable in practice for the underlying problem. \cref{RiemannianInverseBFGSFormula} makes it possible to cheaply compute an approximation of the inverse of the Hessian. This may make \cref{InverseGlobalRiemannianBFGS-Method} advantageous even in the case, where we have a cheap exact formula for the Hessian but not for its inverse or when the cost of solving linear systems is unacceptably high \cite[p.~13]{Qi:2011}. \\
We note, that the operators $\mathcal{B}^{RBFGS}_{k+1}$, generated in \cref{InverseGlobalRiemannianBFGS-Method}, satisfy \cref{RiemannianQuasi-NewtonEquationExpParallel}. Since the parallel transport, $\parallelTransportSymbol$, is the vector transport by differentiated retraction, where the retraction is the exponential map, $\expOp$, and this is an isometric vector transport, a stepsize $\alpha_k > 0$ that satisfies \cref{RiemannianWolfeConditions2.1} ensures the fulfillment of the Riemannian curvature condition (see \cref{RiemannianWolfeConditionResult}). This in turn ensures, that all operators of the sequence $\{ \mathcal{B}^{RBFGS}_k \}_k$ are positive definite and self-adjoint (see \cref{RiemannianPositiveDefiniteUpdate}). \\
Let us now turn to the convergence results of \cref{InverseGlobalRiemannianBFGS-Method}. In the Euclidean case, a sufficient condition to achieve global convergence for a convex objective function and local superlinear convergence, is preserving the symmetry and positive-definiteness when updating the Hessian approximation $H^{BFGS}_k$ (or its inverse $B^{BFGS}_k$) that defines the step \cite[p.~20-21]{Qi:2011}. We know, that the operators $\mathcal{B}^{RBFGS}_{k}$ generated by \cref{InverseGlobalRiemannianBFGS-Method} preserve positive-definiteness and self-adjointness. Now we have to clarify, whether the same convergence results can be shown in the Riemannian setup. To show global convergence, the following assumptions must be made: 

\begin{assumption}[{\cite[Assumptions~2.4.2.]{Qi:2011}}]\label{RiemannianAssumptionsGlobalConvergence} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The objective function $f$ is twice continuously differentiable.
        \item The level set $\Omega = \{ x \in  \mathcal{M}: f(x) \leq f(x_0) \}$ is geodesically convex. Let $(\mathcal{M}, g)$ be a Riemannian manifold. A subset $C$ of $\mathcal{M}$ is said to be a geodesically convex set if, given any two points in $C$, there is a geodesic arc contained within $C$ that joins those two points.
        \item There exists positive constants $n$ and $N$ such that \begin{equation*} n \; g_x(\xi_x, \xi_x) \leq g_x(G(x)[\xi_x],\xi_x) \leq N \; g_x (\xi_x,\xi_x) \quad \text{for all} \; \xi_x \in \tangent{x} \; \text{and} \; x \in \Omega, \end{equation*} where $G(x)$ denotes the lifted Hessian $G(x) = \operatorname{Hess} \hat{f_x} (0_x) = \operatorname{Hess} f(\retract{x}(0_x))$. \label{RiemannianAssumptionsGlobalConvergence1}
    \end{enumerate}
\end{assumption}

For the exponential map coincides the lifted Hessian $G(x)$ with the Riemannian Hessian, i.e. $\operatorname{Hess} f(x) = \operatorname{Hess} f(\exponential{x}(0_x))$ holds for all $x \in \mathcal{M}$ \cite[p.~106]{AbsilMahonySepulchre:2008}. Therefore implies \cref{RiemannianAssumptionsGlobalConvergence1} that $\operatorname{Hess} f(x)$ is positive definite on the level set $\Omega$. For a twice continuously differentiable function $f$ it is known, that $f$ is convex, if its Hessian is at least positive semidefinite \cite[p.~5]{NetoMeloSousa:2017}. The following theorem about the global convergence of \cref{InverseGlobalRiemannianBFGS-Method} is a generalization of \cref{GlobalConvergence}: 

\begin{theorem}[{\cite[Theorem~2.4.3.]{Qi:2011}}] \label{RiemannianGlobalConvergence}
    Let $x_0 \in \mathcal{M}$ be starting point for which \cref{RiemannianAssumptionsGlobalConvergence} is satisfied and let $\mathcal{B}^{RBFGS}_{0}$ be any linear transformation on $\tangent{x_0}$, which is self-adjoint and positive definite with respect to the Riemannian metric $g$. The sequence $\{ x_k \}_k$ generated by \cref{InverseGlobalRiemannianBFGS-Method} using parallel transport as vector transport and the exponential map as the retraction converges to the minimizer $x^*$ of $f$.
\end{theorem}

The convexity of the objective function $f$ is needed to guarantee that there is a unique minimizer. One way for this to happen is, if $f$ is a convex function for the entire domain of interest \cite[p.~28]{Qi:2011}. \\
In the proof of \cref{RiemannianGlobalConvergence}, the choice of exponential map, $\expOp$, and parallel transport, $\parallelTransportSymbol$, is justified by the fact that instead of establishing a bound on the condition number of the approximations $\{ \mathcal{B}^{RBFGS}_{k} \}_k$, one estimates the size of the largest and smallest eigenvalues of the operators and shows that the search directions and stepsizes satisfy the conditions of \cref{RiemannianZoutendijkCondition}. This requires the use of parallel transport, $\parallelTransportSymbol$, in the definition of the average Riemannian Hessian, \cref{RiemannianAverageHessian}, which is used for the estimation of the eigenvalues, and since the “exponential map version” of the Zoutendijk condition, \cref{RiemannianZoutendijkCondition}, is used, the line search is restricted to use the exponential map, $\expOp$, as retraction to define the next iterate $x_{k+1}$ \cite[p.~25]{Qi:2011}. \\
\cref{RiemannianGlobalConvergence} can be used to justify two important conclusions for a more general nonconvex objective function $f$.

\begin{corollary}[{\cite[Corollary~2.4.1.]{Qi:2011}}] 
    Suppose $f$ is a nonconvex objective function on $\mathcal{M}$ and let $x^{*} \in \mathcal{M}$ be a nondegenerate local minimizer of $f$, i.e., $\operatorname{grad} f(x^{*})$ and $\operatorname{Hess} f(x^{*})$ is positive definite. Let $x_0$ be starting point that is close enough to $x^{*}$ so that it is in the neighborhood around $x^{*}$ where the Hessian is positive definite, i.e., for which \cref{RiemannianAssumptionsGlobalConvergence} are satisfied and let $\mathcal{B}^{RBFGS}_{0}$ be any linear transformation on $\tangent{x_0}$ that is self-adjoint and positive definite with respect to the Riemannian metric $g$. \\
    The sequence $\{ x_k \}_k$ generated by \cref{InverseGlobalRiemannianBFGS-Method} using parallel transport, $\parallelTransportSymbol$, and the exponential map, $\expOp$, as the retraction converges to the minimizer $x^{*}$ of $f$, i.e., it is locally convergent to any nondegenerate minimizer. \\
    Additionally, if the convexity assumption is removed from \cref{RiemannianAssumptionsGlobalConvergence}, then from any $x_0$ the sequence $\{ x_k \}_k$ generated by \cref{InverseGlobalRiemannianBFGS-Method} using parallel transport and the exponential map as the retraction converges to a set of critical points of $f$, i.e., there is global convergence to such a set.
\end{corollary}

We have seen, that \cref{InverseGlobalRiemannianBFGS-Method} converges globally under certain conditions. We are now interested in achieving acceptably rapid convergence rate for \cref{InverseGlobalRiemannianBFGS-Method}, e.g., superlinear, as it is guaranteed with \cref{InverseGlobalBFGS-Method} on $\mathbb{R}^n$. This requires the following assumption:

\begin{assumption}[{\cite[Assumptions~2.4.4.]{Qi:2011}}] \label{RiemannianAssumptionsSuperlinearConvergence}
    Let $x^* \in \mathcal{M}$ be a nondegenerate local minimizer of $f$, i.e., $\operatorname{grad} f(x^*) = 0$ and $\operatorname{Hess} f(x^*)$ is positive definite. There is $L > 0$ such that, for all $\xi_{x^*} \in \tangent{x^*}$ and all $\eta_x \in \tangent{x}$ small enough, we have
    \begin{equation*}
        \begin{split}
            & \lVert \parallelTransport{\geodesic<l>{x}{\eta_x}(t)}{x}) \circ \operatorname{Hess} f(\geodesic<l>{x}{\eta_x}(t)) \circ \parallelTransport{x}{\geodesic<l>{x}{\eta_x}(t)} - \parallelTransport{x^*}{x}  \circ \operatorname{Hess} f(x^*) \circ \parallelTransport{x}{x^*} \rVert \leq \\ & \leq L \max \{ \operatorname{dist} (y, x^*), \operatorname{dist} (x, x^*)\} 
        \end{split}
    \end{equation*}
    for $0 \leq t \leq 1$, where $x = \exponential{x^*}(\xi_{x^*})$, $y = \exponential{x}(\eta_x)$ and $\geodesic<l>{x}{\eta_x}(t)$ is the associated geodesic.
\end{assumption}

\begin{theorem}[{\cite[Theorem~2.4.5.]{Qi:2011}}] 
    Suppose that $f$ is twice continuously differentiable and that the iterates, $x_k$, generated by the RBFGS Algorithm using parallel transport and the exponential map, converge to a nondegenerate minimizer $x^* \in \mathcal{M}$, at which \cref{RiemannianAssumptionsSuperlinearConvergence} holds. If
    \begin{equation}
        \sum^{\infty}_{k=0} \operatorname{dist} (x_k, x^*) < \infty
    \end{equation}
    holds, then $x_k$ converges to $x^*$ superlinearly.
\end{theorem}

To prove superlinear convergence for \cref{InverseGlobalBFGS-Method}, \cref{DennisMoreCondition} was used. The same strategy is used in the Riemannian setup. In \cref{RiemannianDennisMoreCondition} a key requirement on the evolution of the action of $\mathcal{B}^{RBFGS}_{k}$ in the direction of $\eta_k$ relative to the action of the covariant derivative is identified. The requirement is quite general and only requires the transport be twice continuously differentiable \cite[p.~29]{Qi:2011}. \\

For certain manifolds, either the exponential map, $\expOp$, or the parallel transport, $\parallelTransportSymbol$, or even both are not available at all, or are computationally too costly. In order to create a well-defined, convergent RBFGS method without exponential map, $\expOp$, and parallel transport, $\parallelTransportSymbol$, the so-called “locking condition” was introduced in \cite{HuangGallivanAbsil:2015}. The idea is to use an isometric vector transport, $\vectorTransportSymbol^S$, with associated retraction, $\retractionSymbol$, in the update \cref{RiemannianInverseBFGSFormula}, which interacts with the vector transport by differentiated retraction, $\vectorTransportSymbol^{\retractionSymbol}$, where the retraction is the associated retraction of $\vectorTransportSymbol^S$. \cref{RiemannianWolfeConditions2.1} with this information about the vector transport by differentiated retraction, $\vectorTransportSymbol^{\retractionSymbol}$, is used to guarantee the curvature condition \cref{RiemannianCurvatureCondition}. The locking condition requires that
\begin{equation}\label{LockingCondition}
	\vectorTransportDir{x}{\xi_x}(\xi_x)[S] = \beta \vectorTransportDir{x}{\xi_x}(\xi_x)[\retractionSymbol], \quad \beta = \frac{\lVert \xi_x \rVert_x}{\lVert \vectorTransportDir{x}{\xi_x}(\xi_x)[\retractionSymbol] \rVert_{\retract{x}(\xi_x)}}
\end{equation}
holds for all $\xi_x \in \tangent{x}$ and all $x \in \mathcal{M}$. The differentiated retraction $\vectorTransportSymbol^{\retractionSymbol}$ only appears in the form $\vectorTransportDir{x}{\xi}(\xi)[\retractionSymbol]$, which is equal to $\frac{\mathrm{d}}{\mathrm{d}t} \retract{x_k}(t \xi)\vert_{t=1}$. Hence $\vectorTransportDir{x}{\alpha_k \eta_k}(\alpha_k \eta_k)[\retractionSymbol]$ is the velocity vector of the line search curve $\alpha \mapsto \retract{x_k}(\alpha \eta_k)$ at time $\alpha_k$, and we only require to be able to evaluate the differentiated retraction in the direction transported. Procedures, which can produce a retraction or an isometric vector transport such that this pair satisfies the locking condition, \cref{LockingCondition}, are described in \cite[4.~Ensuring~the~locking~condition]{HuangGallivanAbsil:2015}. \\
This results in one big difference to \cref{InverseGlobalRiemannianBFGS-Method}. We use the isometric vector transport, $\vectorTransportSymbol^S$, in \cref{RiemannianInverseBFGSFormula}, i.e. $s_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S]$ and $\widetilde{\mathcal{B}}^{BFGS}_k = \vectorTransport{x_k}{x_{k+1}}[S] \circ \mathcal{B}^{BFGS}_k \circ \vectorTransport{x_{k+1}}{x_k}[S]$, but we define $y_k = {\beta_k}^{-1} \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S]$, where 
\begin{equation}\label{LockingConditionParameter}
    \beta_k = \frac{\lVert \alpha_k \eta_k \rVert_{x_k}}{\lVert \vectorTransportDir{x}{\alpha_k \eta_k}(\alpha_k \eta_k)[\retractionSymbol] \rVert_{x_{k+1}}}.
\end{equation}
This ensures that under the assumption, that the isometric vector transport, $\vectorTransportSymbol^S$, fulfills the locking condition, \cref{LockingCondition}, and the stepsize $\alpha_k > 0$ satisfies \cref{RiemannianWolfeConditions2.1}, the curvature condition, \cref{RiemannianCurvatureCondition}, holds. We set $m_k(t) = f(\retract{x_k}(t \; \eta_k / \lVert \eta_k \rVert_{x_k}))$ and obtain
\begin{equation*}
    \begin{split}
        g_{\retract{x_k}(\alpha_k \eta_k)}(s_k,y_k) & = g_{\retract{x_k}(\alpha_k \eta_k)}(\vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S], {\beta_k}^{-1} \operatorname{grad} f(\retract{x_k}(\alpha_k \eta_k)) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S] ) = \\
        & = g_{\retract{x_k}(\alpha_k \eta_k)}(\vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S], {\beta_k}^{-1} \operatorname{grad} f(\retract{x_k}(\alpha_k \eta_k))) - \\
        & -  g_{\retract{x_k}(\alpha_k \eta_k)} ( \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S] , \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S] ) = \\
        & = g_{\retract{x_k}(\alpha_k \eta_k)}({\beta_k}^{-1} \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S], \operatorname{grad} f(\retract{x_k}(\alpha_k \eta_k))) -  \alpha_k g_{x_k} ( \eta_k, \operatorname{grad} f(x_k)) = \\
        & = g_{\retract{x_k}(\alpha_k \eta_k)}(\vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[\retractionSymbol], \operatorname{grad} f(\retract{x_k}(\alpha_k \eta_k))) -  \alpha_k g_{x_k} ( \eta_k, \operatorname{grad} f(x_k)) = \\
        & = \alpha_k \lVert \eta_k \rVert_{x_k} (\frac{\mathrm{d} m_k(\alpha_k \lVert \eta_k \rVert_{x_k})}{\mathrm{d}t} - \frac{\mathrm{d} m_k(0)}{\mathrm{d}t}).
    \end{split}
\end{equation*}
From \cref{RiemannianWolfeConditions2.1} we get 
\begin{equation*}
    \frac{\mathrm{d} m_k(\alpha_k \lVert \eta_k \rVert_{x_k})}{\mathrm{d}t} \geq c_2 \frac{\mathrm{d} m_k(0)}{\mathrm{d}t}.
\end{equation*}
Therefore, it follows, that
\begin{equation*}
    \frac{\mathrm{d} m_k(\alpha_k \lVert \eta_k \rVert_{x_k})}{\mathrm{d}t} - \frac{\mathrm{d} m_k(0)}{\mathrm{d}t} \geq (c_2 - 1)\frac{\mathrm{d} m_k(0)}{\mathrm{d}t} = (c_2 - 1) \frac{1}{\lVert \eta_k \rVert_{x_k}} g_{x_k} ( \eta_k, \operatorname{grad} f(x_k)),
\end{equation*}
since $c_2 < 1$ and $\eta_k$ is a descent direction, the curvature condition, \cref{RiemannianCurvatureCondition}, holds \cite[p.~1664]{HuangGallivanAbsil:2015}. This in turn implies that all operators of the sequence $\{ \mathcal{B}^{BFGS}_k \}_k$, generated by \cref{RiemannianInverseBFGSFormula}, are self-adjoint and positive definite (see \cref{RiemannianPositiveDefiniteUpdate}), which is essential for global convergence. We get the following algorithm: 

\begin{algorithm}[H]
    \caption{Inverse Global RBFGS Method with locking condition}\label{InverseGlobalRiemannianBFGS-MethodLockingCondition}
    \begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; isometric vector transport $\vectorTransportSymbol^S$ with $\retractionSymbol$ as associated retraction, which satisfies \cref{LockingCondition}; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial iterate $x_0 \in \mathcal{M}$; initial positive definite and self-adjoint operator $\mathcal{B}^{RBFGS}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; convergence tolerance $\varepsilon > 0$; linesearch parameters $0 < c_1 < \frac{1}{2} < c_2 < 1$. Set $k = 0$.
        \While{$\lVert \operatorname{grad} f(x_k) \rVert_{x_k} > \varepsilon$}
            \State Compute the search direction $\eta_k = - \mathcal{B}^{RBFGS}_k [\operatorname{grad} f(x_k)]$.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1}. 
            \State Set $x_{k+1} = \retract{x_k}(\alpha_k \eta_k)$.
            \State Set $\widetilde{\mathcal{B}}^{RBFGS}_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}[S] \circ \mathcal{B}^{RBFGS}_k \circ {\vectorTransportDir{x_k}{\alpha_k \eta_k}[S]}^{-1}$, $s_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S]$ and 
            \StatexIndent[2] $y_k = \beta^{-1}_k \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S]$, where $\beta_k$ by means of \cref{LockingConditionParameter}.
            \State Compute $\mathcal{B}^{RBFGS}_{k+1} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ by means of \cref{RiemannianInverseBFGSFormula}. 
            \State Set $k = k+1$.
        \EndWhile
        \State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

We note, that the operators $\mathcal{B}^{RBFGS}_{k+1}$, generated in \cref{InverseGlobalRiemannianBFGS-Method}, satisfy the Riemannian quasi-Newton equation of the form
\begin{equation}\label{RiemannianQuasi-NewtonEquationLockinConditon}
    \mathcal{B}^{RBFGS}_{k+1}[{\beta_k}^{-1} \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S]] = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S].
\end{equation}
By introducing \cref{LockingCondition} we achieve global convergence of \cref{InverseGlobalRiemannianBFGS-MethodLockingCondition} under the following assumptions:

\begin{assumption}[{\cite[Assumptions~3.1.+3.2.+3.3.]{HuangGallivanAbsil:2015}}]\label{RiemannianAssumptionsGlobalConvergenceLockingCondition} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The objective function $f$ is twice continuously differentiable.
        \item Let $\widetilde{\Omega}$ be a neighborhood of $x^*$ and let $\rho$ be a positive constant such that, for all $y \in \widetilde{\Omega}$, $\retract{y}(\mathbb{B}(0_y, \rho)) \supset \widetilde{\Omega}$ and $\retract{y}(\cdot)$ is a diffeomorphism on $\mathbb{B}(0_y, \rho)$. The iterates $x_k$ stay continuously in $\widetilde{\Omega}$, meaning that $\retract{x_k}(t \eta_k) \in \widetilde{\Omega}$ for all $t \in [0, \alpha_k]$. 
        \item $f$ is strongly retraction-convex with respect to the retraction $\retractionSymbol$ in $\widetilde{\Omega}$, i.e. $m_{x, \eta_x}(t) = f(\retract{x}(t \; \eta_x))$ is strongly convex, which means, there exist two constants $0 < a_0 < a_1$ such that $a_0 \leq \frac{\mathrm{d}^2}{\mathrm{d}t^2} m_{x, \eta_x}(t) \leq a_1$ for all $x \in \widetilde{\Omega}$, all $\eta_x \in \tangent{x}$ with $\lVert \eta_x \rVert_x = 1$, and all $t$ such that $\retract{x}(\tau \; \eta_x)$ for all $\tau \in [0,t]$ \cite[Definition~3.1.]{HuangGallivanAbsil:2015}.
    \end{enumerate}
\end{assumption}

The definition of retraction-convexity generalizes standard Euclidean and Riemannian concepts. It is easily seen, that (strong) retraction-convexity reduces to (strong) convexity, when the function is defined on Euclidean space. It can be shown that when $\retractionSymbol = \expOp$ is the exponential mapping, (strong) retraction-convexity is ordinary (strong) convexity for a function, based on the definiteness of its Hessian. \cite[Lemma~3.1]{HuangGallivanAbsil:2015} proofs that such a neighborhood $\widetilde{\Omega}$, in which the function $f$ is strongly retraction-convex, always exists around a nondegenerate minimizer $x^*$ \cite[p.~1665-1666]{HuangGallivanAbsil:2015}. \\

\begin{theorem}[{\cite[Theorem~3.1.]{HuangGallivanAbsil:2015}}] \label{RiemannianGlobalConvergenceLockingCondition}
    Suppose \cref{RiemannianAssumptionsGlobalConvergenceLockingCondition} holds. Then the sequence $\{x_k\}_k$ generated by \cref{InverseGlobalRiemannianBFGS-MethodLockingCondition} converges to a minimizer $x^*$ of $f$.
\end{theorem}

The exponential map, $\expOp$, and parallel transport, $\parallelTransportSymbol$, satisfy \cref{LockingCondition} with $\beta = 1$. This specific choice is legitimate in \cref{RiemannianAssumptionsGlobalConvergenceLockingCondition}, since \cref{RiemannianGlobalConvergenceLockingCondition} subsumes the global convergence results for \cref{InverseGlobalRiemannianBFGS-Method} \cite[p.~1663-1664]{HuangGallivanAbsil:2015}. It can also be shown that the sequence $\{x_k\}_k$ generated by \cref{InverseGlobalRiemannianBFGS-MethodLockingCondition} converges superlinearly \cite[p.~1684]{HuangGallivanAbsil:2015}. 


% We note that the selection of vector transport and retraction of course also influences the update formula \cref{RiemannianInverseBFGSFormula}. On the one hand, the current operator $\mathcal{B}^{RBFGS}_k \mapsto \vectorTransportDir{x_k}{\alpha_k \eta_k} \circ \mathcal{B}^{RBFGS}_k \circ (\vectorTransportDir{x_k}{\alpha_k \eta_k})^{-1}$, the current search direction times stepsize $\alpha_k \eta_k \mapsto \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)$ (in $s_k$) and the current gradient $\operatorname{grad} f(x_k) \mapsto \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))$ (in $y_k$) are mapped into the new tangent space using the selected vector transport $\vectorTransportSymbol$, and on the other hand, the new iterate $x_{k+1}$ is determined by the selected retraction $\retractionSymbol$, and this affects also the definition of $y_k$, since $\operatorname{grad} f(x_{k+1})$ is used. \\
