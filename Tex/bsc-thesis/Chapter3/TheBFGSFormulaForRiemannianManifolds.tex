\section{The Riemannian BFGS Formula}
\label{Section4.3}

In this section we derive the Riemannian BFGS update formula, short RBFGS formula, for the approximation of the Hessian $\operatorname{Hess}f(x_k)$ and the RBFGS formula for the approximation of the Hessian inverse ${\operatorname{Hess}f(x_k)}^{-1}$, which generalize the formulae \cref{directBFGSformula} and \cref{inverseBFGSformula} from the Euclidean setup. As we have seen in \cref{Section2.3}, the success of the Euclidean BFGS update \cref{directBFGSformula} (or \cref{inverseBFGSformula}) for $H_{k+1}$ was based on the properties of positive-definiteness, symmetry and closeness to the former approximation $H_k$. We transfer these properties to the Riemannian setup, as far as possible in order to obtain a suitable update formula for a stable method. We therefore require that the updated approximation $\mathcal{H}_{k+1}$ of the Hessian operator $\operatorname{Hess} f(x_{k+1})$ meets the following characteristics:
\begin{enumerate}
    \item $\mathcal{H}_{k+1}$ is a positive definite, self-adjoint, linear operator on $\tangent{x_{k+1}}$.
    \item $\mathcal{H}_{k+1}$ satisfies the Riemannian quasi-Newton equation \cref{RiemannianQuasi-NewtonEquation}.
    \item $\mathcal{H}_{k+1}$ is “near” $\mathcal{H}_k$.
\end{enumerate}
Of course, the approximation of the Hessian inverse, $\mathcal{B}_{k+1}$, should satisfy the same. As we have seen in \cref{Section4.2}, positive-definiteness and self-adjointness are fundamental requirements for the operators $\mathcal{H}_k$ in Riemannian quasi-Newton methods. Since we want the update formula to be independent of the choice of retraction and vector transport, in order to be used in a large number of Riemannian BFGS methods, we therefore assume at first that these maps are not determined further than in \cref{RiemannianQuasi-NewtonEquation}, which means that $s_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)$ and $y_k = \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))$, where $\retractionSymbol$ is the associated retraction of $\vectorTransportSymbol$, are used. We also hope that by requiring, that $\mathcal{H}_{k+1}$ should be near $\mathcal{H}_k$, we will be able to prove the uniqueness of the formula. \\
As in \cref{Section2.3} we first consider the case that we want to generate an approximation of the inverse ${\operatorname{Hess} f(x_{k+1})}^{-1}$, which is motivated by getting a search direction $\eta_k$ by simply applying the operator $\mathcal{B}_{k+1}$ on the gradient $\operatorname{grad}f(x_{k+1})$. Let $\mathcal{B}_k$ be a positive definite, self-adjoint, linear operator on $\tangent{x_k}$. We want to create an operator $\mathcal{B}_{k+1}$ on $\tangent{x_{k+1}}$ from the operator $\mathcal{B}_k$ using the tangent vectors $y_k, s_k \in \tangent{x_{k+1}}$. As in the Euclidean case, the new operator $\mathcal{B}_{k+1}$ should emerge from the old one $\mathcal{B}_{k}$ by a rank-two update. In \cref{Section2.3} we saw that this is done by adding two outer products of vectors, which are rank one matrices, multiplied by a scalar, to the old matrix $B_k$. \\
This cannot be realized one-to-one in the Riemannian setup. First of all, $\mathcal{B}_k$ is an operator on the tangent space $\tangent{x_k}$ and can't transform tangent vectors of the the tangent space at the new iterate $x_{k+1} = \retract{x_{k}}(\alpha_k \eta_k)$. To overcome this obstacle, we introduce the following 
\begin{equation}\label{TransportedOperator}
    \widetilde{\mathcal{B}}_k = \vectorTransport{x_k}{x_{k+1}} \circ \mathcal{B}_k \circ \vectorTransport{x_{k+1}}{x_k} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}},
\end{equation}
where $\vectorTransport{x_k}{x_{k+1}} = \vectorTransportDir{x_k}{\alpha_k \eta_k} \colon \; \tangent{x_k} \to \tangent{x_{k+1}}$ denotes the vector transport used in \cref{RiemannianQuasi-NewtonEquation} and $\vectorTransport{x_{k+1}}{x_k}$ its inverse, i.e. $\vectorTransport{x_{k+1}}{x_k} = {\vectorTransportDir{x_k}{\alpha_k \eta_k}}^{-1}$. The vector transport is used here to transport the linear operator $\mathcal{B}_k$, so that it can operate in the tangent space $\tangent{x_{k+1}}$ \cite[p.~20]{Huang:2013}.\\
Now we have to find a similar concept for the outer product in the Riemannian setup. For that we use the musical isomorphism $\flat \colon \; \tangent{x_{k+1}} \ni \eta_{x_{k+1}} \mapsto \eta^{\flat}_{x_{k+1}} \in \cotangent{x_{k+1}}$ (see \cref{MusicallyIsomorphism}). Put simply, it means: $\eta^{\flat}_{x_{k+1}} \in \cotangent{x_{k+1}}$ represents the ﬂat of $\eta_{x_{k+1}} \in \tangent{x_{k+1}}$, i.e., $\eta^{\flat}_{x_{k+1}} \colon \; \tangent{x_{k+1}} \to \mathbb{R}, \;  \xi_{x_{k+1}} \mapsto \eta^{\flat}_{x_{k+1}}[\xi_{x_{k+1}}] = g_{x_{k+1}} (\eta_{x_{k+1}}, \xi_{x_{k+1}})$. With that we can create rank one operators in the tangent space $\tangent{x_{k+1}}$. Overall, this enables us to write the following update:  
\begin{equation*}
    \mathcal{B}_{k+1} [\cdot] = \widetilde{\mathcal{B}}_k [\cdot] + a \; u_{x_{k+1}}u^{\flat}_{x_{k+1}} [\cdot] + b \; v_{x_{k+1}}v^{\flat}_{x_{k+1}} [\cdot],
\end{equation*}
where $u_{x_{k+1}}, v_{x_{k+1}} \in \tangent{x_{k+1}}$ and $a, b \in \mathbb{R}$ are to be determined. We see immediately that $u_{x_{k+1}}u^{\flat}_{x_{k+1}} [\cdot]$ and $v_{x_{k+1}}v^{\flat}_{x_{k+1}} [\cdot]$ are self-adjoint and positive definite operators on $\tangent{x_{k+1}}$. \\
From now on, the vector transport $\vectorTransportSymbol$ must be assumed to be isometric, i.e. $\vectorTransportSymbol = \vectorTransportSymbol^S$ (see \cref{IsometricVectorTransport}). We require that $\mathcal{B}_{k+1}$ is a self-adjoint operator. Therefore it makes sense to require that $\widetilde{\mathcal{B}}_k$ is also a self-adjoint operator, so that this property is preserved by the update. Looking at \cref{TransportedOperator} we know an isometric vector transport $\vectorTransportSymbol^S$ guarantees that $\widetilde{\mathcal{B}}_k$ is self-adjoint if $\mathcal{B}_k$ is self-adjoint \cite[p.~20]{Huang:2013}. \\
With an isometric vector transport, $\vectorTransportSymbol^S$, and the demand, that this new operator fulfills \cref{RiemannianQuasi-NewtonEquation}, follows:
\begin{equation*}
    \mathcal{B}_{k+1} [y_k] = \widetilde{\mathcal{B}}_k [y_k] + a \; u_{x_{k+1}}u^{\flat}_{x_{k+1}}[y_k] + b \; v_{x_{k+1}}v^{\flat}_{x_{k+1}}[y_k] = s_k.
\end{equation*}
Clearly, $u_{x_{k+1}}, v_{x_{k+1}} \in \tangent{x_{k+1}}$ are not uniquely determined. One possible choice is
\begin{equation*}
    u_{x_{k+1}} = s_k, \quad v_{x_{k+1}} = \widetilde{\mathcal{B}}_k [y_k].
\end{equation*}
Since the vector transport is assumed to be isometric and $\mathcal{B}_k$ is a self-adjoint operator, we obtain ${\widetilde{\mathcal{B}}_k}^* = \widetilde{\mathcal{B}}_k$, and hence for all $\xi \in \tangent{x_{k+1}}$:
\begin{equation*}
    v^{\flat}_{x_{k+1}}[\xi] = (\widetilde{\mathcal{B}}_k [y_k])^{\flat}[\xi] = g_{x_{k+1}}(\widetilde{\mathcal{B}}_k [y_k], \xi) = g_{x_{k+1}}(y_k, \widetilde{\mathcal{B}}_k [\xi]) = y^{\flat}_k [\widetilde{\mathcal{B}}_k [\xi]].
\end{equation*}
Hence we obtain
\begin{equation*}
    a = \frac{1}{u^{\flat}_{x_{k+1}} [y_k]} = \frac{1}{s^{\flat}_k [y_k]}, \quad b = - \frac{1}{v^{\flat}_{x_{k+1}} [y_k]} = \frac{1}{y^{\flat}_k [\widetilde{\mathcal{B}}_k [y_k]]}.
\end{equation*}
With these constructions and updating the notation to $\mathcal{B}_k = \mathcal{B}^{RDFP}_k$ for all $k \in \mathbb{N}_0$ we get:
\begin{equation}\label{RDFPUpdate}
    \mathcal{B}^{RDFP}_{k+1} [\cdot] = \widetilde{\mathcal{B}}^{RDFP}_k [\cdot] + \frac{s_k s^{\flat}_k[\cdot]}{s^{\flat}_k [y_k]} - \frac{\widetilde{\mathcal{B}}^{RDFP}_k [y_k]  y^{\flat}_k (\widetilde{\mathcal{B}}^{RDFP}_k [\cdot])}{y^{\flat}_k (\widetilde{\mathcal{B}}^{RDFP}_k [y_k])}.
\end{equation}
This is the Riemannian DFP update formula, short RDFP formula, for approximating the Hessian inverse $(\operatorname{Hess} f(x_{k+1}))^{-1}$. Since the direct DFP update is known for the approximation of the Hessian (see e.g. \cite[p.~19]{Huang:2013}), one can show that \cref{RDFPUpdate} is its inverse. \\
Also in the Riemannian setup, the idea of quasi-Newton methods is very clear. Instead of calculating a complete new approximation of the Hessian $\operatorname{Hess} f(x_{k+1})$ or its inverse at every iteration, the previous operator is simply updated using the obtained information included in $s_k, y_k \in \tangent{x_{k+1}}$ from the iteration. \\
We now follow the same strategy as in \cref{Section2.3} and replace the triple $(\mathcal{B}^{DFP}_k,s_k,y_k)$ by the triple $(\mathcal{H}^{BFGS}_k,y_k,s_k)$ in \cref{RDFPUpdate}, and obtain a direct update for the Hessian:
\begin{equation}\label{directRiemannianBFGSFormula}
    \mathcal{H}^{RBFGS}_{k+1} [\cdot] = \widetilde{\mathcal{H}}^{RBFGS}_k [\cdot] + \frac{y_k y^{\flat}_k[\cdot]}{s^{\flat}_k [y_k]} - \frac{\widetilde{\mathcal{H}}^{RBFGS}_k [s_k] s^{\flat}_k (\widetilde{\mathcal{H}}^{RBFGS}_k [\cdot])}{s^{\flat}_k (\widetilde{\mathcal{H}}^{RBFGS}_k [s_k])}.
\end{equation}
This is the direct Riemannian BFGS formula for approximating the Hessian $\operatorname{Hess} f(x_{k+1})$. It turns out, that even in the Riemannian case we can speak of dual update formulae. We see, that the computation of $\mathcal{H}^{RBFGS}_{k+1}$ requires only first-order information, namely the gradient at $x_k$ and $x_{k+1}$, which is a big advantage over the operator $\operatorname{Hess} f(x_{k+1})$ used in the Riemannian Newton method, which involves second-order information \cite[p.~206]{Gabay:1982}. \\
It can be shown that this operator $\mathcal{H}^{RBFGS}_{k+1}$, generated by \cref{directRiemannianBFGSFormula}, fulfills the above mentioned characteristics. At first, the positive-definiteness and self-adjointness:

\begin{lemma}[{\cite[Lemma~2.4.1~+~Lemma~2.4.2]{Qi:2011}}]\label{RiemannianPositiveDefiniteUpdate} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item Let $\vectorTransportSymbol^S$ be an isometric vector transport with associated retraction $\retractionSymbol$, $x_{k+1} = \retract{x_k}(\alpha_k \eta_k)$, $y_k = \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S] \in \tangent{x_{k+1}}$, $s_k =  \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S] \in \tangent{x_{k+1}}$ and $s_k \neq 0$. Let $\{ \mathcal{H}_{k} \}_k$ be a sequence of bounded invertible linear transformation of $\tangent{x_k}$, where $k = 0, 1, 2, \cdots$. If $\mathcal{H}_{k}$ on $\tangent{x_k}$ is self-adjoint and positive definite with respect to the Riemannian metric, then there exists an invertible linear transformation, $\mathcal{J}_{k+1}$, on $\tangent{x_{k+1}}$ such that \begin{equation*} y_k = \mathcal{J}_{k+1} \circ \mathcal{J}^{*}_{k+1} [s_k], \end{equation*} if and only if \begin{equation*} g_{x_{k+1}}(s_k, y_k) > 0. \end{equation*}        
        \item Under the same assumptions, the sequence of linear transformations $\{ \mathcal{H}_{k} \}_k$ defined by $\mathcal{H}_{k+1} = \mathcal{J}_{k+1} \circ \mathcal{J}^{*}_{k+1}$ is the same as the the sequence defined by \cref{directRiemannianBFGSFormula}.
    \end{enumerate}
\end{lemma}

$\mathcal{J}^{*}_{k+1}$ denotes the adjoint operator of $\mathcal{J}_{k+1}$, i.e. $\mathcal{J}^{*}_{k+1}$ satisfies $g_{x_{k+1}}(\mathcal{J}_{k+1}[\xi_{x_{k+1}}], \eta_{x_{k+1}}) = g_{x_{k+1}}(\xi_{x_{k+1}}, \mathcal{J}^{*}_{k+1}[\eta_{x_{k+1}}])$ for all $\xi_{x_{k+1}}, \eta_{x_{k+1}} \in \tangent{x_{k+1}}$. Since $\mathcal{J}_{k+1} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ is an invertible linear operator, we see immediately, that $\mathcal{H}^{RBFGS}_{k+1} = \mathcal{J}_{k+1} \circ \mathcal{J}^{*}_{k+1}$ is a self-adjoint positive definite operator on $\tangent{x_{k+1}}$. \\
We point out that a stepsize that meets \cref{RiemannianWolfeConditions2.1} does not automatically lead to a positive definite update, because \cref{RiemannianWolfeConditions2.1} uses the vector transport by differentiated retraction, $\vectorTransportSymbol^{\retractionSymbol}$. With this vector transport the statements of \cref{RiemannianPositiveDefiniteUpdate} do not apply, as it is in general not isometric. If $\vectorTransportSymbol^{\retractionSymbol}$ is isometric, we have seen in \cref{RiemannianWolfeConditionResult} that the curvature condition, \cref{RiemannianCurvatureCondition}, is fulfilled and if this specific vector transport $\vectorTransportSymbol^{\retractionSymbol}$ is used in \cref{directRiemannianBFGSFormula}, i.e. $\widetilde{\mathcal{H}}^{RBFGS}_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}[\retractionSymbol] \circ \mathcal{H}^{RBFGS}_k \circ {\vectorTransportDir{x_k}{\alpha_k \eta_k}[\retractionSymbol]}^{-1}$, $y_k = \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[\retractionSymbol]$, $s_k =  \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[\retractionSymbol]$, it leads to a positive definite update! \\
There is of course also an update formula for the approximation of the Hessian inverse ${\operatorname{Hess} f(x_{k+1})}^{-1}$. This is also derived by generalizing its Euclidean counterpart, \cref{inverseBFGSformula}. But one could also apply the Sherman-Morrison-Woodbury identity for operators (cf. \cite[Theorem~1.1]{Deng:2011}) on \cref{directRiemannianBFGSFormula}, it results in the same update:
\begin{equation}\label{RiemannianInverseBFGSFormula}
    \begin{split}
        \mathcal{B}^{RBFGS}_{k+1} [\cdot] &= \widetilde{\mathcal{B}}^{RBFGS}_k [\cdot] -  \frac{s_k y^{\flat}_k[\widetilde{\mathcal{B}}^{RBFGS}_k [\cdot]]}{y^{\flat}_k [s_k]} - \frac{\widetilde{\mathcal{B}}^{RBFGS}_k [y_k]  s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} + \frac{s_k y^{\flat}_k[\widetilde{\mathcal{B}}^{RBFGS}_k [y_k]]s^{\flat}_k [\cdot]}{(y^{\flat}_k [s_k])^2} + \frac{s_k s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} = \\
        &= \Big{(} \id_{\tangent{x_{k+1}}}[\cdot] - \frac{s_k y^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} \Big{)} \widetilde{\mathcal{B}}^{RBFGS}_k [\cdot] \Big{(} \id_{\tangent{x_{k+1}}}[\cdot] - \frac{y_k s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]} \Big{)} + \frac{s_k s^{\flat}_k [\cdot]}{s^{\flat}_k [y_k]}.
    \end{split}
\end{equation}
As the inverse of $\mathcal{H}^{RBFGS}_{k+1}$ fulfills $\mathcal{B}^{RBFGS}_{k+1}$, created by \cref{RiemannianInverseBFGSFormula}, also the positive-definiteness, the self-adjointness and the Riemannian quasi-Newton equation, \cref{RiemannianQuasi-NewtonEquation}, for all $k$. \\
In \cref{Section2.3} we have seen that the third characteristic ($H_{k+1}$ is “near” $H_k$) is important for showing the uniqueness of \cref{inverseBFGSformula} and to some extent for determining the convergence rate. This was achieved by showing, that \cref{inverseBFGSformula} is the unique solution of an optimization problem (see \cref{thrGeigerKanzow11.8}). Unfortunately, this characteristic of closeness between the operators $\mathcal{B}^{RBFGS}_{k+1}$ and $\mathcal{B}^{RBFGS}_k$ has not yet been extensively investigated in the Riemannian case. Nevertheless, according to \cite{Huang:2013}, if the method uses exponential map, $\expOp$, and parallel transport, $\parallelTransportSymbol$, and thus satisfies \cref{RiemannianQuasi-NewtonEquationExpParallel}, it can be shown that \cref{BFGSFormulaOptimizationProblem} generalizes to a Riemannian manifold as
\begin{equation}\label{RBFGSOptimizationProblem}
    \begin{aligned}
        \min_{\mathcal{B} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}} \quad  \lVert \mathcal{B} - \widetilde{\mathcal{B}}^{RBFGS}_k \rVert_{\mathcal{W}^2} \\
        \textrm{s.t.} \quad \mathcal{B} = \mathcal{B}^{*}, \quad \mathcal{B}[y_k] = s_k 
    \end{aligned}
\end{equation}
where $\widetilde{\mathcal{B}}^{RBFGS}_k = \parallelTransport{x_k}{x_{k+1}} \circ \mathcal{B}^{RBFGS}_k \circ \parallelTransport{x_{k+1}}{x_k}$ and $s_k, y_k$ as in \cref{RiemannianQuasi-NewtonEquationExpParallel}. In this case is $\mathcal{W}^2$ a self-adjoint and positive definite operator on $\tangent{x_{k+1}}$, satisfying $\mathcal{W}^2[s_k] = y_k$ and the weighted norm is defined by $\lVert \mathcal{A} \rVert_{\mathcal{W}^2} = \lVert \hat{\mathcal{W}} G^{\frac{1}{2}} \hat{\mathcal{A}} G^{-\frac{1}{2}} \hat{\mathcal{W}} \rVert_{\mathrm{F}}$, where $G$ is the matrix expression of the metric and $\hat{\mathcal{A}},\hat{\mathcal{W}}$ denote the matrix expression of the operators $\mathcal{A}, \mathcal{W}$. The solution of \cref{RBFGSOptimizationProblem} is ${\mathcal{H}^{RBFGS}_{k+1}}^{-1} = \mathcal{B}^{RBFGS}_{k+1}$ \cite[p.~19]{Huang:2013}. As in the Euclidean case, a possible choice for ${\mathcal{W}}^2$ would be the average Hessian, \cref{RiemannianAverageHessian}, but it is in general not positive definite. \\
Since we want the positive-definiteness and self-adjointness to be passed on, the first approximation $\mathcal{H}^{RBFGS}_0$ or $\mathcal{B}^{RBFGS}_0$ should definitely fulfill these properties. Of course, for reasons of quick availability, the choice often falls on the identity operator $\mathcal{B}^{RBFGS}_0 = \id_{\tangent{x_0}}$ or on the multiplication of the tangent vector by a positive number $\kappa > 0$, i.e. $\mathcal{B}^{RBFGS}_0 [\xi_{x_0}] = \kappa \; \xi_{x_0}$. The efficient heuristic from \cref{Section2.3} can also be used without any problems, since it is defined only by inner products. We scale the initial operator $\mathcal{B}^{RBFGS}_0 = \id_{\tangent{x_0}}$ after the first step, i.e. after computing $x_{1} = \retract{x_0}(\alpha_0 \eta_0)$, but before the first RBFGS update is performed, i.e. $\mathcal{B}^{RBFGS}_0$ is changed by setting
\begin{equation*}
    \mathcal{B}^{RBFGS}_0 = \frac{g_{x_1}(y_0, s_0)}{g_{x_1}(y_0, y_0)} \; \id_{\tangent{x_0}},
\end{equation*}
and we use this in \cref{RiemannianInverseBFGSFormula} to obtain $\mathcal{B}^{RBFGS}_1$. However, it is not yet clear, whether this has certain advantages over using just the identity. 

