\section{Preliminaries}
\label{Section4.1}

Before we enter the theory about quasi-Newton methods and in particular the BFGS method on Riemannian manifolds, we introduce certain basic principles and aspects in this section, which will be needed later. \\

The goal in this chapter is to solve Riemannian optimization problems, which consider finding an optimum of a real-valued function $f$ defined on a Riemannian manifold, i.e.
\begin{equation}\label{RiemannianOptimizationProblem}
    \min f(x), \quad x \in \mathcal{M}
\end{equation}
where $\mathcal{M}$ is a Riemannian manifold. From now on we assume that $\mathcal{M}$ is a $n$-dimensional geodesically complete Riemannian manifold. We further assume that the manifold $\mathcal{M}$ is embedded in a real-valued space (e.g. $\mathcal{M} \subseteq \mathbb{R}^m$) and connected, which means there exist no two disjoint, nonempty, open subsets of $\mathcal{M}$ whose union is $\mathcal{M}$. Throughout the chapter we assume that $f \colon \; \mathcal{M} \to \mathbb{R}$ is a twice continuously differentiable function, i.e. $f \in C^2(\mathcal{M})$, by which we mean functions whose Hessian operator $\operatorname{Hess} f(x)$ exists for all $x \in \mathcal{M}$ and is continuous, unless otherwise stated. \\
Solutions of \cref{RiemannianOptimizationProblem} are called minimizers. The characterizations for these can be adopted from \cref{Section2.1}. We call $x^* \in \mathcal{M}$ a global minimizer of $f$ if $f(x^*) \leq f(x)$  holds for all $x \in \mathcal{M}$. Of course, global minimizers are also hard to find on Riemannian manifolds, which is why we consider local minimizers, i.e. a point $x^* \in \mathcal{M}$ around which there is an open neighborhood $U \subset \mathcal{M}$ such that $f(x^*) \leq f(x)$ for all $x \in U$. The point $x^*$ is a strict local minimizer of $f$ if there is an open neighborhood $U$ of $x^*$ such that $f(x^*) < f(x)$ for all $x \in U$ with $x \neq x^*$. \\
Points $x \in \mathcal{M}$ where $\operatorname{grad}f(x) = 0_x \in \tangent{x}$ are called stationary points of $f$. In order to characterize a local minimizer we need information about the second derivative of $f \colon \; \mathcal{M} \to \mathbb{R}$. We characterize a local minimizer of $f$ as a point $x^* \in \mathcal{M}$ at which $\operatorname{grad} f(x^*) = 0_{x^*}$, where $0_{x^*} \in \tangent{x^*}$ is the zero tangent vector, holds and $\operatorname{Hess} f(x^*) \colon \; \tangent{x^*} \to \tangent{x^*}$ is a positive semidefinite operator, i.e. $g_{x^*} (\operatorname{Hess} f(x^*)[\xi_{x^*}], \xi_{x^*}) \geq 0$ for all $\xi_{x^*} \in \tangent{x^*}$. We characterize a strict local minimizer of $f$ as a point $x^* \in \mathcal{M}$ at which $\operatorname{grad} f(x^*) = 0_{x^*}$ holds and $\operatorname{Hess} f(x^*)$ is a positive definite operator, i.e. $g_{x^*} (\operatorname{Hess} f(x^*)[\xi_{x^*}], \xi_{x^*}) > 0$ for all $\xi_{x^*} \in \tangent{x^*} \backslash 0_{x^*}$. All methods in this chapter search for a point $x^*$ where the gradient of $f$ is the zero tangent vector, i.e. $\operatorname{grad} f(x^*) = 0_{x^*}$. Any local minimizer is a stationary point. \newpage

Riemannian quasi-Newton methods are Riemannian line search methods, which are numerical methods for solving \cref{RiemannianOptimizationProblem}. These can be expressed as algorithms and are based on the update formula
\begin{equation}\label{RiemannianIterativeUpdateScheme}
    x_{k+1} = \retract{x_k}(\alpha_k \eta_k),
\end{equation}
where $\eta_k \in \tangent{x_k}$ and $\alpha_k > 0$. \cref{RiemannianIterativeUpdateScheme} is the generalization of \cref{IterativeUpdateScheme} to a Riemannian manifold $\mathcal{M}$, which consists of selecting a search direction $\eta_k \in \tangent{x_k}$ and performing a line search along a curve $\geodesic<s>(\alpha) = \retract{x_k}(\alpha \; \eta_k)$ with $\dot{\geodesicSymbol}(0) = \eta_k$. The definition of the curve relies on the chosen retraction. Therefore the choice of a computationally efficient retraction is an important decision in the design of high-performance numerical algorithms on manifolds \cite[p.~54]{AbsilMahonySepulchre:2008}. Once a retraction is selected, it is used until the algorithm stops. As in the Euclidean case, Riemannian line search methods also follow the methodology of first determining a search direction $\eta_k \in \tangent{x_k}$ and then determining a stepsize $\alpha_k > 0$ depending on the search direction $\eta_k$. \\

There are two approaches to determine the stepsize $\alpha_k > 0$. The inexact line search and the exact line search. The latter deals with the exact solution of 
\begin{equation}\label{RiemannianOptimizationProblemStepsize}
    \min_{\alpha > 0} f(\geodesic<s>(\alpha)) = f(\retract{x_k}(\alpha \; \eta_k)).
\end{equation}
Of course, an exact solution of \cref{RiemannianOptimizationProblemStepsize} is in the most cases computationally too costly, which is why in most methods the inexact line search, which determines a $\alpha_k > 0$ such that the objective function has an acceptable descent amount, i.e. such that the descent $f(x_k) - f(\retract{x_k}(\alpha \; \eta_k)) > 0$ is acceptable, is used. Of course, such an $\alpha_k$ must be considered as an approximate solution of \cref{RiemannianOptimizationProblemStepsize}. Inexact line search methods on Riemannian manifolds follow the same methodology as their Euclidean counterparts, i.e. a stepsize $\alpha_k > 0$ is determined along the curve $\geodesic<s>(\alpha) = \retract{x_k}(\alpha \; \eta_k)$, which must fulfill certain conditions. \\
In order to select a suitable stepsize, a generalization of the Wolfe conditions to a Riemannian manifold can be used. The generalized Armijo condition, \cref{WolfeConditions1}, is
\begin{equation}\label{RiemannianWolfeConditions1}
    f( \retract{x_k}(\alpha_k \eta_k)) \leq f(x_k) + c_1 \alpha_k g_{x_k} (\operatorname{grad} f(x_k), \eta_k)
\end{equation}
and the generalized “curvature condition”, \cref{WolfeConditions2}, is
\begin{equation}\label{RiemannianWolfeConditions2.1}
    \frac{\mathrm{d}}{\mathrm{d}t} f(\retract{x_k}(t \; \eta_k)) \vert_{t=\alpha_k} \geq c_2 \frac{\mathrm{d}}{\mathrm{d}t} f(\retract{x_k}(t \; \eta_k)) \vert_{t=0},
\end{equation}
with $0 < c_1 < c_2 < 1$. Other generalizations of the Wolfe conditions are possible \cite[p.~12]{Qi:2011}. Note that \cref{RiemannianWolfeConditions2.1} can be rewritten using the differentiated retraction (see \cref{DifferentiatedRetraction}) as
\begin{equation}\label{RiemannianWolfeConditions2.2}
    g_{\retract{x_k}(\alpha_k \eta_k)} (\operatorname{grad} f(\retract{x_k}(\alpha_k \eta_k)), \vectorTransportDir{x_k}{\alpha_k \eta_k}(\eta_k)[\retractionSymbol]) \geq c_2 g_{x_k} (\operatorname{grad} f(x_k), \eta_k).
\end{equation}
\cref{RiemannianWolfeConditions2.2} transports the tangent vector $\eta_k$ in the tangent space of the potential next iterate $\retract{x_k}(\alpha_k \eta_k)$ and applies \cref{WolfeConditions2}. \\

We are interested in the convergence behavior of the Riemannian line search methods presented in this chapter. To obtain global convergence, we must not only have well chosen stepsizes but also well chosen search directions $\eta_k$. If we assume that the retraction used in \cref{RiemannianIterativeUpdateScheme} is the exponential map, $\expOp$, and the stepsize $\alpha_k > 0$ satisfies the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1} using the parallel transport, $\parallelTransportSymbol$, we have the following strong statement about the angles between the search directions and gradients at each step, called Riemannian exponential map Zoutendijk condition, which generalizes \cref{ZoutendijkTheorem}:

\begin{theorem}[{\cite[Theorem~2.4.1]{Qi:2011}}] \label{RiemannianZoutendijkCondition}
    Consider any iteration of form $x_{k+1} = \exponential{x_k}(\alpha_k \eta_k)$, where $\eta_k$ is a descent direction and $\alpha_k$ satisfies the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1}. Suppose that $f$ is bounded below on $\mathcal{M}$ and that $f$ is continuously differentiable in an open set $\mathcal{N}$ containing the level set $\mathcal{L} = \{ x \colon \; f(x) \leq f(x_0)\}$, where $x_0$ is the starting point of the iteration. Assume also that the gradient $\operatorname{grad} f$ is Lipschitz continuous on $\mathcal{N}$ (see \cite[Theorem 2.4.1]{Qi:2011}), then
    \begin{equation}\label{RiemannianZoutendijklimit}
        \sum_{k \geq 0} \cos(\theta_k)^2 \lVert \operatorname{grad} f(x_k) \rVert^2_{x_k} < \infty,
    \end{equation}
    where
    \begin{equation*}
        \cos(\theta_k) = - \frac{g_{x_k}(\operatorname{grad} f(x_k), \eta_k)}{\lVert \operatorname{grad} f(x_k) \rVert_{x_k} \lVert \eta_k \rVert_{x_k}}.
    \end{equation*}    
\end{theorem}

If the method for choosing the search direction $\eta_k$ ensures that the angle $\theta_k$ is bounded away from $90^{\circ}$, then follows from \cref{RiemannianZoutendijklimit} 
\begin{equation}\label{RiemannianGlobalConvergentFunction}
    \lim\limits_{k \to \infty} \lVert \operatorname{grad} f(x_k) \rVert_{x_k} = 0.
\end{equation}
In other words, the gradient norms $\lVert \operatorname{grad} f(x_k) \rVert_{x_k}$ converge to zero and the method would achieve global convergence to a set of stationary points. \\
We call a numerical method on a Riemannian manifold globally convergent, if \cref{RiemannianGlobalConvergentFunction} is satisfied. For Riemannian line search methods of the form \cref{RiemannianIterativeUpdateScheme}, the limit \cref{RiemannianGlobalConvergentFunction} is the strongest global convergence result that can be obtained. \\

Of course we are also interested in the convergence rate of a sequence $\{ x_k \}_k$ generated by a Riemannian line search method. We say that the sequence $\{ x_k \}_k$ on $\mathcal{M}$ converges superlinear to $x^*$, if 
\begin{equation*}
    \lim\limits_{k \to \infty} \frac{\operatorname{dist}(x_{k+1}, x^*)}{\operatorname{dist}(x_k, x^*)} = 0
\end{equation*}
holds. \\
In \cref{Chapter1} superlinear convergence could be proven in the Euclidean case by a result due to Dennis and Mor\'{e} (see \cref{DennisMoreCondition}). The following theorem, which generalizes an important result from \cite[Theorem~8.2.4]{DennisSchnabel:1996} that is also due to Dennis and Mor\'{e}, gives a necessary and sufficient condition for proving superlinear convergence of a basic Riemannian line search method:

\begin{theorem}[{\cite[Theorem~2.3.1]{Qi:2011}}]\label{RiemannianDennisMoreCondition}
    Let $\mathcal{M}$ be a manifold endowed with a $C^2$ vector transport $\vectorTransportSymbol$ and an associated retraction $\retractionSymbol$. Let $F$ be a $C^2$ tangent vector field on $\mathcal{M}$. Also let $\mathcal{M}$ be endowed with an affine connection $\nabla$. Let $\mathbb{D}F(x)$ denote the linear transformation of $\tangent{x}$ defined by $\mathbb{D}F(x)[\xi_x] = \nabla_{\xi_x} F$ for all tangent vectors $\xi_x$ to $\mathcal{M}$ at $x$. Let $\{ \mathcal{H}_k \}_k$ be a sequence of bounded nonsingular linear transformations of $\tangent{x_k}$, where $k = 0, 1, \cdots$, $x_{k+1} = \retract{x_k}(\eta_k)$, and $\eta_k = - {\mathcal{H}_k}^{-1} [F(x_k)]$. Assume that $\mathbb{D}F(x^*)$ is nonsingular, $x_k \neq x^*$, for all $k$, and $\lim\limits_{k \to \infty} x_k = x^*$. Then $\{ x_k \}_k$ converges superlinearly to $x^*$ and $F(x^*) = 0$ if and only if
    \begin{equation*}
        \lim\limits_{k \to \infty} \frac{ \lVert \mathcal{H}_k[\eta_k] - \vectorTransport{x^*}{\xi_k} \circ \mathbb{D}F(x^*) \circ {\vectorTransport{x^*}{\xi_k}}^{-1}[\eta_k] \rVert_{x^*} }{ \lVert \eta_k \rVert_{x^*}} = 0,
    \end{equation*}
    where $\xi_k \in \tangent{x^*}$ is defined by $\xi_k = \inverseRetract{x^*}(x_k)$, i.e. $\retract{x^*}(\xi_k) = x_k$.
\end{theorem}
