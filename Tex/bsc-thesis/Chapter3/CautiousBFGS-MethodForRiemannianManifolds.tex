\section{Cautious RBFGS Method}
\label{Section4.5}

As we have seen in the previous chapter and as confirmed by other sources (see e.g. \cite{RingWirth:2012}), for global convergence the objective function is required to satisfy a Riemannian version of convexity. Therefore the idea was to generalize a variant of the Euclidean BFGS method, which avoids the requirement of a convex objective function, to the Riemannian setting. \\
We present the cautious RBFGS method, CRBFGS for short, introduced by \cite{HuangAbsilGallivan:2018} based on a Riemannian generalization of the cautious update by \cite{LiFukushima:2001} and a weak line search condition, which was introduced in \cite{ByrdNocedal:1989}. The motivation is to develop a method, that does not need a vector transport by differentiated retraction, which allows the use of line search conditions other than the Wolfe conditions, and convexity of the cost function. \\
First, we discuss the new conditions which the stepsize must meet. The following conditions for the stepsize $\alpha_k$ of the Euclidean BFGS method were introduced in \cite{ByrdNocedal:1989}. It is assumed, that $\alpha_k$ is chosen such that either
\begin{equation}\label{CautiousStepsize1}
    h_k(\alpha_k) - h_k(0) \leq - \chi_1 \frac{{h^{\prime}_k(0)}^2}{\lVert \eta_k \rVert^2_{x_k}}
\end{equation}
\textbf{or}
\begin{equation}\label{CautiousStepsize2}
    h_k(\alpha_k) - h_k(0) \leq \chi_2 h^{\prime}_k(0)
\end{equation}
holds in every iteration, where $\chi_1, \chi_2$ are positive constants and $h_k(t) = f(x_k + td_k)$. The motivation for this is, that the analysis would cover a large class of line search strategies \cite[p.~732]{ByrdNocedal:1989}. We see immediately, that \cref{CautiousStepsize1} is an efficient stepsize strategy (see \cref{EfficientStepSize}). The line search conditions \cref{CautiousStepsize1}, \cref{CautiousStepsize2} are weak, since it has been shown in \cite{ByrdNocedal:1989} and \cite{Werner:1978} that many search conditions, including the Wolfe conditions, imply either \cref{CautiousStepsize1} or \cref{CautiousStepsize2}, if the gradient of the objective function is Lipschitz continuous (see \cref{CautiousAssumptionLipschitz}). \\
In the Riemannian case, a stepsize $\alpha_k$ is chosen which fulfills either condition \cref{CautiousStepsize1} or condition \cref{CautiousStepsize2} for the function $h_k(t) = f(\retract{x_k}(t \eta_k))$. Since $f \circ \retract{x_k} \colon \; \tangent{x_k} \to \mathbb{R}$ is defined on a linear space, the Euclidean results about line search are applicable. This means, many search conditions, including the Wolfe conditions, imply either \cref{CautiousStepsize1} or \cref{CautiousStepsize2}, if $f \circ \retract{x_k}$ is a radially Lipschitz continuously differentiable function, i.e. there exist reals $b > 0$ and $d > 0$ such that, for all $x \in \mathcal{M}$, for all $\xi_x \in \tangent{x}$ with $\lVert \xi_x \rVert_x = 1$, and for all $t < d$, it holds that
\begin{equation}\label{RiemannianRadiallyLipschitzContinuouslyDifferentiable}
    \lvert \frac{\mathrm{d}}{\mathrm{d} \tau} f (\retract{x_k}(\tau \; \xi_x))\vert_{\tau = t} - \frac{\mathrm{d}}{\mathrm{d} \tau} f (\retract{x_k}(\tau \; \xi_x))\vert_{\tau = 0} \rvert \leq b \; t,
\end{equation}
\cite[p.~146]{AbsilMahonySepulchre:2008}. We see in both \cref{CautiousStepsize1} and \cref{CautiousStepsize2}, that information about the vector transport by differentiated retraction is not needed, since $h^{\prime}_k(0) = \frac{\mathrm{d}}{\mathrm{d} t} f(\retract{x_k}(t \eta_k))\vert_{t=0} = \operatorname{grad} f(x_k)$, which can be seen as an advantage. \\
Similar to \cref{Section2.5} we want to decide by a rule, if the new operator $\mathcal{B}^{CRBFGS}_{k+1}$ in the CRBFGS method is calculated according to \cref{RiemannianInverseBFGSFormula}, or if we simply transport the current operator into the new tangent space, i.e. $\mathcal{B}^{CRBFGS}_{k+1} = \widetilde{\mathcal{B}}^{CRBFGS}_k$ via an isometric vector transport, $\vectorTransportSymbol^{S}$. In \cite{HuangAbsilGallivan:2018}, the update from \cite{LiFukushima:2001} was generalized in the following way:
\begin{equation}\label{RiemannianCautiousUpdate}
    \mathcal{B}^{CRBFGS}_{k+1} = \begin{cases} \text{using \cref{RiemannianInverseBFGSFormula}}, & \; \frac{g_{x_{k+1}}(y_k,s_k)}{\lVert s_k \rVert^{2}_{x_{k+1}}} \geq \theta(\lVert \operatorname{grad} f(x_k) \rVert_{x_k}), \\ \widetilde{\mathcal{B}}^{CRBFGS}_k, & \; \text{otherwise}, \end{cases}
\end{equation}
where $\theta$ is a monotone increasing function satisfying $\theta(0) = 0$ and $\theta$ is strictly increasing at $0$. As we have seen at the end of \cref{Section2.5}, the choice of such a function is also possible for the Euclidean case.\\
Instead of enforcing the curvature condition, \cref{RiemannianCurvatureCondition}, the self-adjointness and positive-definiteness of $\mathcal{B}^{CRBFGS}_{k+1}$ is guaranteed by \cref{RiemannianCautiousUpdate}, which reduces to \cref{CautiousUpdate}, when $\mathcal{M}$ is a Euclidean space. The inheritance of these properties is achieved, because we only update the positive definite operator, if the curvature condition, \cref{RiemannianCurvatureCondition}, is already fulfilled, since $g_{x_{k+1}}(y_k,s_k) \geq \lVert s_k \rVert^{2}_{x_{k+1}} \theta(\lVert \operatorname{grad} f(x_k) \rVert_{x_k}) > 0$ for $\operatorname{grad} f(x_k) \neq 0_{x_k}$. Thus, all operators of the sequence $\{\mathcal{B}^{CRBFGS}_k\}_k$ are positive definite and self-adjoint. This gives us more options for the stepsize strategy, as it only has to meet either \cref{CautiousStepsize1} or \cref{CautiousStepsize2}. If the decision rule is not fulfilled, i.e. $\frac{g_{x_{k+1}}(y_k,s_k)}{\lVert s_k \rVert^{2}_{x_{k+1}}} \ngeq \theta(\lVert \operatorname{grad} f(x_k) \rVert_{x_k})$ applies, then $\mathcal{B}^{CRBFGS}_{k+1}$ can also be set to any other positive definite self-adjoint linear operator, e.g. the identity operator $\mathcal{B}^{CRBFGS}_{k+1} = \id_{\tangent{x_{k+1}}}$. Compared to the Euclidean setting, $B_{k+1}$ is equal to $B_k$, if the decision rule is not fulfilled. If this is the case in the Riemannian setting, $\mathcal{B}^{CRBFGS}_{k+1}$ is still different from $\mathcal{B}^{CRBFGS}_k$ due to the usage of the vector transport $\vectorTransportSymbol^S$. \\
All this leads to the following algorithm, which allows the use of line search conditions other than the Wolfe condition and the requirement of differentiated retraction is completely avoided:

\begin{algorithm}[H]
	\caption{Cautious Riemannian BFGS-Algorithm}\label{CautiousRBFGSMethod}
	\begin{algorithmic}[1]
        \State Riemannian manifold $(\mathcal{M}, g)$; isometric vector transport $\vectorTransportSymbol^S$ with associated retraction $\retractionSymbol$; continuously differentiable real-valued function $f$ on $\mathcal{M}$, bounded below; initial positive definite and self-adjoint operator $\mathcal{B}^{CRBFGS}_0 \colon \; \tangent{x_0} \to \tangent{x_0}$; initial iterate $x_0 \in \mathcal{M}$; convergence tolerance $\varepsilon > 0$; linesearch parameters $\chi_1, \chi_2 > 0$; monotone increasing function $\theta$ satisfying $\theta(0) = 0$ and $\theta$ is strictly increasing at $0$. Set $k = 0$.
        \While{$\lVert \operatorname{grad} f(x_k)\rVert_{x_k} > \varepsilon$}
            \State Compute the search direction $\eta_k = - \mathcal{B}^{CRBFGS}_k [\operatorname{grad} f(x_k)]$.
            \State Determine a stepsize $\alpha_k > 0$ that satisfies \cref{CautiousStepsize1} \textbf{or} \cref{CautiousStepsize1}.
            \State Set $x_{k+1} = \retract{x_k}(\alpha_k \eta_k)$.
            \State Set $\widetilde{\mathcal{B}}^{CRBFGS}_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}[S] \circ \mathcal{B}^{CRBFGS}_k \circ {\vectorTransportDir{x_k}{\alpha_k \eta_k}[S]}^{-1}$, $s_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S]$ and 
            \StatexIndent[2] $y_k = \beta^{-1}_k \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S]$, where $\beta_k$ by means of \cref{LockingConditionParameter}.
            \State Compute $\mathcal{B}^{CRBFGS}_{k+1} \colon \; \tangent{x_{k+1}} \to \tangent{x_{k+1}}$ by means of \cref{RiemannianCautiousUpdate}. 
			\State Set $k = k+1$.
		\EndWhile
		\State \textbf{Return} $x_k$.
    \end{algorithmic}
\end{algorithm}

In \cref{CautiousRBFGSMethod}, $\beta_k$ can be any number, that satisfies
\begin{equation*}
    \lvert \beta_k - 1 \rvert \leq L_{\beta} \lVert \alpha_k \eta_k \rVert_{x_k} \quad \text{and} \quad \lvert \beta^{-1}_k - 1 \rvert \leq L_{\beta} \lVert \alpha_k \eta_k \rVert_{x_k},
\end{equation*}
where $L_{\beta} > 0$ is a constant. \cite{HuangAbsilGallivan:2018} state, that the motivation for introducing $\beta_k$ is to make this update subsume the update in \cref{InverseGlobalRiemannianBFGS-MethodLockingCondition}, where \cref{LockingConditionParameter} is used for $\beta_k$ in the definition of $y_k$. However, it can also be assumed that $\beta_k = 1$ applies in each iteration \cite[p.~473]{HuangAbsilGallivan:2018}. \\ 
Let us turn to the convergence results of this method. For proving global convergence of \cref{CautiousRBFGSMethod}, we need the following assumptions:

\begin{assumption}[{\cite[Assumptions~4.1.+4.2.]{HuangAbsilGallivan:2018}}]\label{AssumptionGlobalConvergenceCautiousRBFGS} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The level set $\Omega = \{x \in \mathcal{M} \colon \; f(x) \leq f(x_0) \}$ is compact.
        \item The function $f$ is Lipschitz continuously differentiable with respect to the isometric vector transport $\vectorTransportSymbol^S$ on $\Omega$, i.e. there exists a constant $L > 0$ such that \begin{equation} \lVert \vectorTransportDir{x}{\eta}(\operatorname{grad} f(x))[S] - \operatorname{grad} f (\retract{x}(\eta)) \lVert \leq L \lVert \eta \rVert \label{RiemannianLipschitzContinuouslyDifferentiable} \end{equation} for all $x \in \Omega$, $\eta \in \tangent{x}$ such that $\retract{x}(\eta) \in \Omega$, where $\retractionSymbol$ is the associated retraction of $\vectorTransportSymbol^S$ \cite[Definition~4.1.]{HuangAbsilGallivan:2018}.
    \end{enumerate}
\end{assumption}

\cref{AssumptionGlobalConvergenceCautiousRBFGS} generalizes \cref{CautiousAssumption}. We must be careful with the two definitions \cref{RiemannianRadiallyLipschitzContinuouslyDifferentiable} and \cref{RiemannianLipschitzContinuouslyDifferentiable}. Multiple versions of Lipschitz continuous differentiable functions on Riemannian manifolds have been defined. However, \cref{RiemannianRadiallyLipschitzContinuouslyDifferentiable} refers to the pullback $\hat{f}_{x_k} = f \circ \retract{x_k}$ of $f$ and if $\hat{f}_{x_k}$ is a radially Lipschitz continuously differentiable function, then the Wolfe conditions imply either \cref{CautiousStepsize1} or \cref{CautiousStepsize2}.

\begin{theorem}[{\cite[Theorem~4.2.]{HuangAbsilGallivan:2018}}] 
    Let $\{ x_k \}_k$ be sequences generated by \cref{CautiousRBFGSMethod}. If \cref{AssumptionGlobalConvergenceCautiousRBFGS} holds, then
    \begin{equation*}
        \liminf_{k \to \infty} \lVert \operatorname{grad} f(x_k) \rVert = 0.
    \end{equation*}
\end{theorem}

This is the Riemannian generalization of \cref{CautiousGlobalConvergence1}. The global convergence of \cref{CautiousRBFGSMethod} does not depend on the assumption about convexity of the objective function. \\
At the end of \cref{Section2.5} we have noted, that in the proof of \cref{CautiousLocalConvergence} it is shown that the \cref{CautiousBFGSMethod} reduces to \cref{InverseGlobalBFGS-Method}, if $k$ is sufficiently large. This means, that the decision rule is satisfied in every iteration. Similar can be shown for \cref{CautiousRBFGSMethod}. The following assumptions are needed:

\begin{assumption}[{\cite[Assumptions~5.1.+5.2.+5.3.]{HuangAbsilGallivan:2018}}]\label{AssumptionLocalConvergenceCautiousRBFGS} \ \\[-1.5\baselineskip]
    \begin{enumerate}
        \item The objective function $f$ is twice continuously differentiable in the level set $\Omega$.
        \item The retraction $\retractionSymbol$ is twice continuously differentiable.
        \item The isometric vector transport $\vectorTransportSymbol^S$ with associated retraction $\retractionSymbol$ is continuous and satisfies $\vectorTransportDir{x}{0_x}(\xi_x)[S] = \xi_x$ for all $\xi_x \in \tangent{x}$. Additionally, given any $x \in \mathcal{M}$, there exists a neighborhood $\mathcal{U}$ of $x$ such that $\vectorTransportSymbol^S$ satisfies $\lVert \vectorTransportDir{x}{\eta_x}[S] - \vectorTransportDir{x}{\eta_x}[\retractionSymbol] \rVert \leq L \lVert \eta_x \rVert_x$ and $\lVert {\vectorTransportDir{x}{\eta_x}[S]}^{-1} - {\vectorTransportDir{x}{\eta_x}[\retractionSymbol]}^{-1} \rVert \leq L \lVert \eta_x \rVert_x$, where $\retract{x}(t \; \eta_x) \in \mathcal{U}$ for all $t \in [0,1]$ and $L$ is a positive constant. \label{AssumptionLocalConvergenceCautiousRBFGS1}
        \item There exists a $K > 0$ such that the iterates $x_k$ stay continuously in a neighborhood $\widetilde{\Omega}$ of $x^*$ for all $k \geq K$, meaning that $\retract{x_k}(t \eta_k) \in \widetilde{\Omega} \subseteq \Omega$ for all $t \in [0, \alpha_k]$. \label{AssumptionLocalConvergenceCautiousRBFGS2}
        \item There exists a $r > 0$ such that for each $x \in \widetilde{\Omega}$, $\retract{x}(\mathbb{B}(0_x, r)) \supset \widetilde{\Omega}$ and $\retract{x}(\cdot)$ is a diffeomorphism on $\mathbb{B}(0_x, r)$. \label{AssumptionLocalConvergenceCautiousRBFGS3}
    \end{enumerate}  
\end{assumption}

It follows, that $f \circ \retractionSymbol$ is a twice continuously differentiable function. As in the Euclidean, a twice continuously differentiable function $f$ on a compact set implies its gradient is Lipschitz continuous on this set, i.e., $f \in C^2 (\Omega)$ implies that the function $f$ satisfies \cref{AssumptionGlobalConvergenceCautiousRBFGS} but also \cref{RiemannianRadiallyLipschitzContinuouslyDifferentiable}. Since $\eta_x = 0_x$ implies $\lVert \vectorTransportDir{x}{\eta_x}[S] - \vectorTransportDir{x}{\eta_x}[\retractionSymbol] \rVert = 0$ and $\lVert {\vectorTransportDir{x}{\eta_x}[S]}^{-1} - {\vectorTransportDir{x}{\eta_x}[\retractionSymbol]}^{-1} \rVert = 0$, we have that $\vectorTransportDir{x}{}[S] \in C^1$ implies \cref{AssumptionLocalConvergenceCautiousRBFGS1}, therefore \cref{AssumptionLocalConvergenceCautiousRBFGS1} is weaker than $\vectorTransportDir{x}{}[S] \in C^1$. The “continuously” assumption in \cref{AssumptionLocalConvergenceCautiousRBFGS2} cannot be removed and \cref{AssumptionLocalConvergenceCautiousRBFGS3} is used to guarantee that $\inverseRetract{x}(y)$ is well-defined. \\
With these assumptions, the following theorem was established which states that if the iterates $\{ x_k \}_k$ stay in a sufficiently small neighborhood of a nondegenerate minimizer $x^*$, then $\{ x_k \}_k$ converges to $x^*$ and \cref{CautiousRBFGSMethod} reduces to an “ordinary” RBFGS method, i.e. the decision rule $\frac{g_{x_{k+1}}(y_k,s_k)}{\lVert s_k \rVert^{2}_{x_{k+1}}} \geq \theta(\lVert \operatorname{grad} f(x_k) \rVert_{x_k})$ is always satisfied and for $\mathcal{B}^{CRBFGS}_{k+1}$ the update formula \cref{RiemannianInverseBFGSFormula} is used in each iteration. This in turn builds a connection between the global and local convergence analyses of \cref{CautiousRBFGSMethod}.

\begin{theorem}[{\cite[Theorem~5.1.]{HuangAbsilGallivan:2018}}] 
    Under \cref{AssumptionLocalConvergenceCautiousRBFGS}, if $s_k \to 0$ and $x^*$ is an accumulation point of $\{ x_k \}_k$ generated by \cref{CautiousRBFGSMethod}, then the sequence $\{ x_k \}_k$ converges to $x^*$ and \cref{CautiousRBFGSMethod} reduces to the ordinary RBFGS, when $\{ x_k \}_k$ is sufficiently close to $x^*$.
\end{theorem}

Under certain conditions and further preparatory work it can be shown that \cref{CautiousRBFGSMethod} even converges superlinearly, but to discuss this would exceed the scope of the work. Nevertheless, we would like to mention important aspects from the local convergence analysis in \cite{HuangAbsilGallivan:2018}: Superlinear convergence of \cref{CautiousRBFGSMethod} is shown in \cite[Corollary~5.2.]{HuangAbsilGallivan:2018}, which in the methodology is a combination of \cite[Theorem~5.3.]{HuangAbsilGallivan:2018} (a generalization of \cite[Theorem~3.2]{ByrdNocedal:1989}) and a Riemannian version of the Dennis-Mor\'{e} condition. In this corollary it is assumed, among other things, that $f$ fulfills a Riemannian form of twice Hölder continuously differentiability, i.e. there exists a constant $a > 0$ such that for all $y \in \widetilde{\Omega}$
\begin{equation*}
    \lVert \operatorname{Hess} f(y) - \vectorTransportDir{x^*}{\eta_{x^*}}[S] \circ \operatorname{Hess} f(x^*) \circ {\vectorTransportDir{x^*}{\eta_{x^*}}[S]}^{-1} \rVert \leq a \lVert \eta_{x^*} \rVert_{x^*},
\end{equation*}
where $\eta_{x^*} = \inverseRetract{x^*}(y)$, holds. We note, that in \cref{CautiousLocalConvergence} it was required that $\nabla^2 f(\cdot)$ is Hölder continuous. Furthermore, it is assumed that the stepsize $\alpha_k = 1$ is used for all $k \geq k_0$ (in \cite[Corollary~5.2.]{HuangAbsilGallivan:2018} it is shown that a $k_0$ exists such that $\alpha_k = 1$ satisfies either \cref{CautiousStepsize1} or \cref{CautiousStepsize2} for all $k \geq k_0$). In \cite[Theorem~5.2.4]{Huang:2013} is shown that $\alpha_k = 1$ eventually satisfies the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1}. Therefore, if $\alpha_k = 1$ is attempted first in the Wolfe conditions, the superlinear convergence of \cref{CautiousRBFGSMethod} is obtained \cite[p.~286]{HuangAbsilGallivan:2018}. \\
Finally, it should be noted that it is important to ensure $\theta(0) = 0$ and $\theta$ is strictly increasing at $0$. If the latter is not the case, then \cref{RiemannianCautiousUpdate} still guarantees the positive-definiteness of sequence $\{ \mathcal{B}^{CRBFGS}_k \}_k$, but the global convergence for nonconvex problems may not hold. If $\theta(0) > 0$, then the global convergence of \cref{CautiousRBFGSMethod} holds for nonconvex problems but the local superlinear convergence does not hold in general. On the other hand if $\theta(0) < 0$, then $\eta_k$ may not be a descent direction \cite[p.~474]{HuangAbsilGallivan:2018}. In practice, the function $\theta(x) = x \cdot 10^{-4}$ is often used. 
