\section{Approach Of Representing The RBFGS Formula For Operators Via Matrices}
\label{Section5.1}

If we would consider $\mathcal{B}^{RBFGS}_{k}$ as a linear operator from $\tangent{x_k}$ to $\tangent{x_k}$, then his acting on a tangent vector would be understood as a function in the package \lstinline!Manopt.jl!, i.e. $\mathcal{B}^{RBFGS}_{k}$ would get a tangent vector $\xi_{x_k} \in \tangent{x_k}$ as an argument and return a tangent vector $\mathcal{B}^{RBFGS}_{k} [\xi_{x_k}] \in \tangent{x_k}$ as function-value. If we look at the update formula \cref{RiemannianInverseBFGSFormula} we see that in order to execute the update, functions would have to be multiplied by a scalar, added to each other and then saved again as a function. This methodology seems to us to be too elaborate, which is the reason why we follow the idea to express the operator $\mathcal{B}^{RBFGS}_{k}$ by a matrix with real-valued entries, which is updated in every iteration by means of \cref{RiemannianInverseBFGSFormula}. \\

In this section we start from the following setup, which results from the requirements of \cref{InverseGlobalRiemannianBFGS-Method}, \cref{InverseGlobalRiemannianBFGS-MethodLockingCondition} and \cref{CautiousRBFGSMethod}: We assume that we have an optimization problem given by a continuously differentiable real-valued function $f$ on a Riemannian manifold $(\mathcal{M}, g)$. The initial point is $x_0 \in \mathcal{M}$. We assume that the isometric vector transport, $\vectorTransportSymbol^S$, with associated retraction, $\retractionSymbol$, fulfill the locking condition, \cref{LockingCondition}. \\
First we create an orthonormal basis $(e_1, \cdots, e_n)$ in the tangent space $\tangent{x_0}$, i.e. 
\begin{equation*}
    g_{x_0}(e_i, e_j) = \delta_{ij} = \begin{cases} 1 &: i=j \\ 0 &: i \neq j \end{cases}.
\end{equation*}
It follows that the inner product of two tangent vectors in $\tangent{x_0}$ is equal to the Euclidean inner product of their coordinate representations with respect to this basis, i.e. for all $\xi_{x_0}, \eta_{x_0} \in \tangent{x_0}$ it holds $g_{x_0}(\xi_{x_0}, \eta_{x_0}) = \hat{\xi_{x_0}}^{\mathrm{T}} \hat{\eta_{x_0}}$ where $\hat{\xi_{x_0}}, \hat{\eta_{x_0}} \in \mathbb{R}^n$ with $\xi_{x_0} = (\hat{\xi_{x_0}})_1 e_1 + \cdots + (\hat{\xi_{x_0}})_n e_n$ (and the same relation holds for $\eta_{x_0}$ and $\hat{\eta_{x_0}}$). \\
As already mentioned, the operator $\mathcal{B}^{RBFGS}_k$ which in theory approximates the Hessian inverse is stored as a real matrix. As explained in \cref{Section3.3}, there exist coordinate representations of operators on tangent spaces with respect to a chosen basis. If we assume that the first operator $\mathcal{B}^{RBFGS}_0$ is the identity operator on $\tangent{x_0}$ (or a multiple of it), its coordinate representation is the identity matrix (multiplied by the corresponding factor) with respect to the orthonormal basis $(e_1, \cdots, e_n)$, i.e.
\begin{equation*}
    c \; \id_{\tangent{x_0}}[\cdot] = \mathcal{B}^{RBFGS}_0[\cdot] \simeq \hat{\mathcal{B}}^{RBFGS}_0[\cdot] = B^{RBFGS}_0 = c \; I_{n \times n} \in \mathbb{R}^{n \times n}.
\end{equation*}
This matrix is of course $\spd$ as long as $c > 0$. One could also take any other $\spd$ matrix as initial matrix, because it represents the coordinates of a positive definite self-adjoint operator and vice versa. For convenience, we assume that we are initializing the method with a multiple of the identity. \\
Instead of transporting the operator $\mathcal{B}^{RBFGS}_k$ in each iteration ($\mathcal{B}^{RBFGS}_k \mapsto \widetilde{\mathcal{B}}^{RBFGS}_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}[S] \circ \mathcal{B}^{RBFGS}_k \circ {\vectorTransportDir{x_k}{\alpha_k \eta_k}[S]}^{-1}$) for updating it, we transport the orthonormal basis, through which $B^{RBFGS}_k$ will be the coordinate representation of the operator $\mathcal{B}^{RBFGS}_k$, into the tangent space of the next iterate $\tangent{x_{k+1}}$, i.e. $\tilde{e}_j = \vectorTransportDir{x_k}{\alpha_k \eta_k}[S] \circ \vectorTransportDir{x_{k-1}}{\alpha_{k-1} \eta_{k-1}}[S] \circ \cdots \circ \vectorTransportDir{x_0}{\alpha_0 \eta_0}[S] (e_j)$. In order to avoid further indexes, from now on we denote $(\tilde{e}_1, \cdots, \tilde{e}_n)$ the tangent vectors in the corresponding tangent space, originated from $(e_1, \cdots, e_n) \subset \tangent{x_0}$ with any number of applications of the vector transport $\vectorTransportSymbol^S$ to them. Since the vector transport $\vectorTransportSymbol^S$ is assumed to be isometric, the family $(\tilde{e}_1, \cdots, \tilde{e}_n)$ remains an orthonormal basis of the corresponding tangent space after any number of iterations. \\
The $k$-iteration now runs as follows: We calculate the gradient at $x_k$, i.e. $\operatorname{grad} f(x_k) \in \tangent{x_k}$. We determine its coordinates with respect to the orthonormal basis $\operatorname{grad} f(x_k) \simeq \widehat{\operatorname{grad} f(x_k)}\in \mathbb{R}^n$. Then we calculate the coordinates of the tangent vector $\eta_k$ as in the Euclidean case, i.e. $\hat{\eta_k} = - B^{RBFGS}_k \widehat{\operatorname{grad} f(x_k)} \in \mathbb{R}^n$. We construct the search direction from the vector $\hat{\eta_k}$ and the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$:
\begin{equation*}
    \eta_k = (\hat{\eta_k})_1 \tilde{e}_1 + \cdots + (\hat{\eta_k})_n \tilde{e}_n.
\end{equation*}
To make sure that $\eta_k$ is a descent direction, we require that $B^{RBFGS}_k$ is a positive definite matrix, since $g_{x_k}(\operatorname{grad} f(x_k), \eta_k) = - \widehat{\operatorname{grad} f(x_k)}^{\mathrm{T}} B^{RBFGS}_k \widehat{\operatorname{grad} f(x_k)}$. \\
Let $\alpha_k > 0$ be a suitably chosen stepsize that meets the Wolfe conditions. The next steps as usual, i.e. $x_{k+1} = \retract{x_k}(\alpha_k \eta_k)$, $s_k = \vectorTransportDir{x_k}{\alpha_k \eta_k}(\alpha_k \eta_k)[S]$ and $y_k = \beta^{-1}_k \operatorname{grad} f(x_{k+1}) - \vectorTransportDir{x_k}{\alpha_k \eta_k}(\operatorname{grad} f(x_k))[S]$, where $\beta_k$ by means of \cref{LockingConditionParameter}. \\
Instead of using the tangent vectors $s_k, y_k \in \tangent{x_{k+1}}$ to update the operator $\mathcal{B}^{RBFGS}_k$ by means of \cref{RiemannianInverseBFGSFormula}, we use their coordinates $\hat{s_k}, \hat{y_k} \in \mathbb{R}^n$ to update the matrix $B^{RBFGS}_k$ by means of \cref{inverseBFGSformula}. For this we transport the orthonormal basis into the tangent space of the new iterate, i.e. $(\tilde{e}_1, \cdots, \tilde{e}_n) \subset \tangent{x_{k+1}}$ and determine the coordinates of $s_k$ and $y_k$ with respect to this basis. It is obvious that $\hat{s_k}$ and $\hat{y_k}$ fulfill \cref{CurvatureCondition}, because by choosing $\alpha_k > 0$ that satisfies \cref{RiemannianWolfeConditions2.1}, $s_k$ and $y_k$ fulfill \cref{RiemannianCurvatureCondition} and it holds
\begin{equation}\label{RiemannianCurvatureConditionCurvatureCondition}
    0 < g_{x_{k+1}}(s_k, y_k) = \hat{s}^{\mathrm{T}}_k \hat{y}_k.
\end{equation}
Now we come to the step where the previously mentioned obstacles can easily be avoided by working with matrices. At the end of each iteration the matrix $B^{RBFGS}_k$ is updated by means of \cref{inverseBFGSformula} and stored. The new matrix $B^{RBFGS}_{k+1}$ is also $\spd$, because $B^{RBFGS}_k$ inherits these characteristics to it since \cref{RiemannianCurvatureConditionCurvatureCondition} holds (see \cref{thmUlbrichUlbrich13.4}). \\

It must be shown that the descent directions $\eta_k$ generated using the operator $\mathcal{B}^{RBFGS}_k$ are the same as those generated from the (transported) orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n) \subset \tangent{x_k}$ and the real-valued valued vector obtained by multiplying the matrix $B^{RBFGS}_k$ with the coordinate representation of the gradient $\widehat{\operatorname{grad} f(x_k)}$ in each iteration. We show by induction that the coordinates of $\mathcal{B}^{RBFGS}_k [\operatorname{grad} f(x_k)]$ with respect to the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$ are the same as the vector $B^{RBFGS}_k \widehat{\operatorname{grad} f(x_k)}$ in each iteration. \\

Therefore we first have to take a closer look at the linear operator $\mathcal{B}^{RBFGS}_k$ with respect to the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n) \subset \tangent{x_k}$. The application of $\mathcal{B}^{RBFGS}_k$ to a tangent vector $\xi_{x_k} \in \tangent{x_k}$ can be written as
\begin{equation}\label{OperatorRepresentationTangentVectors}
    \mathcal{B}^{RBFGS}_k [\xi_{x_k}] = \sum^{n}_{i=1} {b^{k}_i}^{\flat} (\xi_{x_k}) \tilde{e}_i = \sum^{n}_{i=1} g_{x_k} (b^{k}_i, \xi_{x_k}) \tilde{e}_i \in \tangent{x_k},
\end{equation}
where $(b^{k}_1, \cdots, b^{k}_n) \subset \tangent{x_k}$ can been understood as a representation of the operator $\mathcal{B}^{RBFGS}_k$. By using the musical isomorphism $({b^{k}_1}^{\flat}, \cdots, {b^{k}_n}^{\flat}) \subset \cotangent{x_k}$ and since $\cotangent{x_k}$ is the space of the real-valued linear functions on $\tangent{x_k}$, we immediately see that \cref{OperatorRepresentationTangentVectors} is for all $\xi_{x_k} \in \tangent{x_k}$ a linear combination of the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$ and therefore is $\mathcal{B}^{RBFGS}_k [\cdot] = \sum^{n}_{i=1} {b^{k}_i}^{\flat} (\cdot) \tilde{e}_i$ a linear operator from $\tangent{x_k}$ to $\tangent{x_k}$. \\
The coordinates of the resulting tangent vector $\mathcal{B}^{RBFGS}_k [\xi_{x_k}]$ with respect to the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$ are:
\begin{equation*}
    \widehat{\mathcal{B}^{RBFGS}_k [\xi_{x_k}]} = (g_{x_k} (b^{k}_1, \xi_{x_k}), \cdots, g_{x_k} (b^{k}_n, \xi_{x_k}))^{\mathrm{T}} = (\hat{b^{k}_1}^{\mathrm{T}} \hat{\xi_{x_k}}, \cdots, \hat{b^{k}_n}^{\mathrm{T}} \hat{\xi_{x_k}})^{\mathrm{T}} \in \mathbb{R}^n,
\end{equation*}
where $\hat{b^{k}_i}$ and $\hat{\xi_{x_k}}$ are the coordinate representations of $\xi_{x_k}, b^{k}_i \in \tangent{x_k}$ with respect to the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$. We require that the coordinate representation of the tangent vector $\mathcal{B}^{RBFGS}_k [\xi_{x_k}]$ is equal to the vector resulting from the coordinate representation of the operator, $\hat{\mathcal{B}}^{RBFGS}_k$, multiplied by the coordinate representation of the tangent vector, $\hat{\xi_{x_k}}$, i.e.
\begin{equation*}
    (\hat{b^{k}_1}^{\mathrm{T}} \hat{\xi_{x_k}}, \cdots, \hat{b^{k}_n}^{\mathrm{T}} \hat{\xi_{x_k}})^{\mathrm{T}} = \hat{\mathcal{B}}^{RBFGS}_k \hat{\xi_{x_k}}.
\end{equation*}
Thus we see that the lines of $\hat{\mathcal{B}}^{RBFGS}_k$ must be equal to the coordinate representations of $b^{k}_i$ for $i = 1, \cdots, n$, i.e. $(\hat{\mathcal{B}}^{RBFGS}_k)_{ij} = (\hat{b^{k}_i})_j = \hat{b^{k}}_{ij}$ where $\hat{b^{k}}_{ij}$ is the $j$-th entry of the vector $\hat{b^{k}}_i \in \mathbb{R}^n$. \\

Now we come to the actual induction. In the first iteration we represent the operator $\mathcal{B}^{RBFGS}_0[\cdot] = c \; \id[\cdot]$ by the basis tangent vectors multiplied by $c$, i.e. $(b^{0}_1, \cdots, b^{0}_n) = (c \; e_1, \cdots, c \; e_n)$, since $\operatorname{grad} f(x_0) = (\widehat{\operatorname{grad} f(x_0)})_1 e_1 + \cdots + (\widehat{\operatorname{grad} f(x_0)})_n e_n$, it holds
\begin{align*}
    \mathcal{B}^{RBFGS}_0[\operatorname{grad} f(x_0)] & = \sum^{n}_{i=1} g_{x_0} (b^{0}_i, \operatorname{grad} f(x_0)) e_i = \sum^{n}_{i=1} g_{x_0} (c \; e_i, \operatorname{grad} f(x_0)) e_i = \\
     & = c \; \sum^{n}_{i=1} (\widehat{\operatorname{grad} f(x_0)})_i e_i = c \operatorname{grad} f(x_0).
\end{align*}
The coordinate representation of $(c \; e_1, \cdots, c \; e_n)$ with respect to the orthonormal basis $(e_1, \cdots, e_n)$ are of course the canonical unit vectors multiplied by $c$ in $\mathbb{R}^n$. Therefore, since the coordinate representation of the tangent vectors $(b^{0}_1, \cdots, b^{0}_n)$ must correspond to the lines of the matrix $B^{RBFGS}_0$, the initial matrix is $B^{RBFGS}_0 = c \; I_{n \times n} \in \mathbb{R}^{n \times n}$. \\
We have to find an update formula for the tangent vectors $(b^{k}_1, \cdots, b^{k}_n)$ representing the operator $\mathcal{B}^{RBFGS}_k$ which is equal to \cref{RiemannianInverseBFGSFormula} and show that the coordinates of the resulting tangent vectors $(b^{k+1}_1, \cdots, b^{k+1}_n)$ in the new tangent space $\tangent{x_{k+1}}$ are the same as the line entries of the matrix $B^{RBFGS}_{k+1}$, which was generated by \cref{inverseBFGSformula}. Let $(\tilde{e}_1, \cdots, \tilde{e}_n) = (\vectorTransportDir{x_k}{\alpha_k \eta_k} (e_1), \cdots, \vectorTransportDir{x_k}{\alpha_k \eta_k} (e_n))$ be the orthonormal basis of $\tangent{x_{k+1}}$ and $s_k, y_k \in \tangent{x_{k+1}}$ such that $g_{x_{k+1}}(s_k, y_k) > 0$. It is required that 
\begin{align*}
    & \mathcal{B}^{RBFGS}_{k+1} [\xi_{x_{k+1}}] = \\ 
    = & \widetilde{\mathcal{B}}^{RBFGS}_k [\xi_{x_{k+1}}] - \frac{s_k y^{\flat}_k[\widetilde{\mathcal{B}}^{RBFGS}_k [\xi_{x_{k+1}}]]}{y^{\flat}_k [s_k]} - \frac{\widetilde{\mathcal{B}}^{RBFGS}_k [y_k] s^{\flat}_k [\xi_{x_{k+1}}]}{s^{\flat}_k [y_k]} + \frac{s_k y^{\flat}_k[\widetilde{\mathcal{B}}^{RBFGS}_k [y_k]]s^{\flat}_k [\xi_{x_{k+1}}]}{(y^{\flat}_k [s_k])^2} + \frac{s_k s^{\flat}_k [\xi_{x_{k+1}}]}{s^{\flat}_k [y_k]} = \\
     = &\sum^{n}_{i=1} g_{k+1} (b^{k+1}_i, \xi_{x_{k+1}}) \tilde{e}_i,
\end{align*}
where $\widetilde{\mathcal{B}}^{RBFGS}_k [\xi_{x_{k+1}}] = \sum^{n}_{i=1} g_{x_k} (\widetilde{b}^{k}_i, \xi_{x_k}) \tilde{e}_i$ with $\widetilde{b}^{k}_i = \vectorTransport{x_k}{x_{k+1}} (b^{k}_i) = \hat{b^{k}}_{i1} \vectorTransportDir{x_k}{\alpha_k \eta_k} (e_1) + \cdots + \hat{b^{k}}_{in} \vectorTransportDir{x_k}{\alpha_k \eta_k} (e_n)$. We see immediately that $\widetilde{b}^{k}_i \in \tangent{x_{k+1}}$ and $b^{k}_i \in \tangent{x_k}$ have the same coordinate representation $\hat{b^{k}_i} \in \mathbb{R}^n$ but in different tangent spaces, therefore $\widetilde{\mathcal{B}}^{RBFGS}_k \simeq B^{RBFGS}_k \in \mathbb{R}^{n \times n}$. After some calculations, the following formula can be used for the update of the tangent vectors:
\begin{equation}\label{InverseRBFGSUpdateRows}
    b^{k+1}_{i} = \widetilde{b}^{k}_i - \frac{(\hat{s_k})_i \; \widetilde{\mathcal{B}}^{RBFGS}_k [y_k]}{g_{x_{k+1}}(s_k, y_k)} - \frac{(\widehat{\widetilde{\mathcal{B}}^{RBFGS}_k [y_k]})_i \; s_k}{g_{x_{k+1}}(s_k, y_k)} + \frac{g_{x_{k+1}}(y_k, \widetilde{\mathcal{B}}^{RBFGS}_k [y_k]) \; (\hat{s_k})_i \; s_k}{(g_{x_{k+1}}(s_k, y_k))^2} + \frac{(\hat{s_k})_i \; s_k}{g_{x_{k+1}}(s_k, y_k)},
\end{equation}
where $(\hat{s_k})_i$ and $(\widehat{\widetilde{\mathcal{B}}^{RBFGS}_k [y_k]})_i$ is the $i$-th entry of the corresponding coordinate representation. \\
Next we look at the update of the matrix $B^{RBFGS}_k \mapsto B^{RBFGS}_{k+1}$ generated by \cref{inverseBFGSformula}, also line by line. Let $\hat{s_k}, \hat{y_k}$ be the coordinate representation of $s_k, y_k$ with respect to the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n) \subset \tangent{x_{k+1}}$. With simple transformations it can be shown that the lines of the matrix are updated in each iteration as follows:
\begin{equation*}
    (B^{RBFGS}_{k+1})_{i:} = b^{k+1}_i = b^{k}_i - \frac{(\hat{s_k})_i \; B^{RBFGS}_k \hat{y}_k}{\hat{s}^{\mathrm{T}}_k \hat{y}_k} - \frac{(B^{RBFGS}_k \hat{y}_k)_i \; \hat{s}_k}{\hat{s}^{\mathrm{T}}_k \hat{y}_k} + \frac{\hat{y}^{\mathrm{T}}_k B^{RBFGS}_k \hat{y}_k \; (\hat{s_k})_i \; \hat{s}_k}{(\hat{s}^{\mathrm{T}}_k \hat{y}_k)^2} + \frac{(\hat{s_k})_i \; \hat{s}_k}{\hat{s}^{\mathrm{T}}_k \hat{y}_k},
\end{equation*}
where $b^{k}_i$ is the $i$-th line of the matrix $B^{RBFGS}_k$, $(\hat{s_k})_i$ and $(B^{RBFGS}_k \hat{y}_k)_i$ is the $i$-th entry of the corresponding vector. \\
We see immediately that this corresponds to the coordinate representation of \cref{InverseRBFGSUpdateRows} with respect to the basis $(\tilde{e}_1, \cdots, \tilde{e}_n) \subset \tangent{x_{k+1}}$, since $\hat{\widetilde{b}^{k}_i} = \hat{b^{k}_i} = b^{k}_i$, $g_{x_{k+1}}(s_k, y_k) = \hat{s}^{\mathrm{T}}_k \hat{y}_k$ and $\widehat{\widetilde{\mathcal{B}}^{RBFGS}_k [y_k]} = \hat{\widetilde{\mathcal{B}}}^{RBFGS}_k \hat{y_k} = B^{RBFGS}_k \hat{y_k}$ holds. \\
It follows that the coordinate representation of the operator $\mathcal{B}^{RBFGS}_k$, which is updated by means of \cref{RiemannianInverseBFGSFormula}, with respect to the transported basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$ is equal to the matrix $B^{RBFGS}_k$, which is updated by means of \cref{inverseBFGSformula}, in each iteration. Thus the effect on the coordinates of $\operatorname{grad} f(x_k)$ caused by the application of the operator $\mathcal{B}^{RBFGS}_k$ with respect to the orthonormal basis is the same as that caused by the matrix-vector-multiplication $B^{RBFGS}_k \widehat{\operatorname{grad} f(x_k)}$ in each iteration. This means that a method in which the operator is expressed from the beginning by a matrix, which is updated and stored, is equivalent to the usual RBFGS method. \\

For the cautious RBFGS method, \cref{CautiousRBFGSMethod}, it should be noted that if the decision rule is not fulfilled, the matrix $B^{CRBFGS}_k$ is not updated, i.e. $B^{CRBFGS}_{k+1} = B^{CRBFGS}_k$, but the orthonormal basis $(\tilde{e}_1, \cdots, \tilde{e}_n)$ is still transported into the new tangent space $\tangent{x_{k+1}}$, because it is needed for the coordinate representation of the tangent vectors $\operatorname{grad} f(x_{k+1}), \eta_{k+1}, s_{k}, y_{k} \in \tangent{x_{k+1}}$ occurring there. \\
For the direct RBFGS method (in which the approximation $\mathcal{H}^{RBFGS}_k$ of the Hessian is used) this approach can also be followed, but then of course the update formula \cref{directBFGSformula} is used. One has to keep in mind that for an efficient method an efficient way to solve the linear system $\hat{\mathcal{H}}^{RBFGS}_k \hat{\eta}_k = -\widehat{\operatorname{grad} f(x_k)}$ is needed. 

