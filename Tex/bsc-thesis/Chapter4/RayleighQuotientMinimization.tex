\subsection{Rayleigh Quotient Minimization}
\label{Section5.2.1}

To show the performance of the RBFGS method, implemented in \lstinline!Manopt.jl!, which follows the concept from \cref{Section5.1}, we compare the average time per run and the average number of iterations with the results from \cite[p.~84]{Qi:2011} with respect to the Rayleigh quotient minimization problem on the sphere $\mathbb{S}^{n-1}$. For a symmetric matrix $A \in \mathbb{R}^{n \times n}$, the unit-norm eigenvector, $v \in \mathbb{R}^n$, corresponding to the smallest eigenvalue, defines the two global minima, $\pm v$, of the Rayleigh quotient  
\begin{equation}\label{RayleighQuotient}
    \begin{split}
        f \colon \; \mathbb{S}^{n-1} & \to \mathbb{R} \\
        x & \mapsto x^{\mathrm{T}} A x 
    \end{split}
\end{equation}   
with its gradient 
\begin{equation*}
    \operatorname{grad} f(x) = 2(Ax - x x^{\mathrm{T}} A x).
\end{equation*}
To apply the RBFGS method, implemented in \lstinline!Manopt.jl!, to the optimization problem defined by the cost function \cref{RayleighQuotient} for $n=100$, \cref{RayleighCode} must be executed in Julia. The problem is defined by setting \lstinline!A_symm = ( A + A' ) / 2!, where the elements of \lstinline!A! are drawn from the standard normal distribution using Julia’s \lstinline!randn(n,n)! with seed \lstinline!42!. With \lstinline!random_point(M)!, a random point is created on the given manifold \lstinline!M!. The stopping criterion requires to abort the method, that the ratio of the norm of the initial gradient and the norm of the current gradient is less than $10^{-6}$.\\ 

\begin{lstlisting}[caption={The Rayleigh quotient minimization experiment in Julia for $n = 100$.}, label={RayleighCode}]
    using Manopt, Manifolds, Random
    Random.seed!(42)
    n = 100
    A = randn(n,n)
    A_symm = ( A + A' ) / 2
    M = Sphere(n - 1)
    F(X::Array{Float64,1}) = X' * A_symm * X
    grad_F(X::Array{Float64,1}) = 2 * ( A_symm * X - X * X' * A_symm * X )
    x = random_point(M)
    
    quasi_Newton(M, 
        F, 
        grad_F, 
        x; 
        memory_size = -1, 
        stopping_criterion = StopWhenGradientNormLess(norm(M, x, grad_F(x)) * 10^(-6))) 
\end{lstlisting}

\begin{table}[H]\label{tab:RayleigResults}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l l l l l l l }
            \toprule
            Manifold & \multicolumn{3}{c}{$\mathbb{S}^{99}$}& \multicolumn{3}{c}{$\mathbb{S}^{299}$} \\ 
            \midrule
            Method & RBFGS & RBFGS-Qi-1 & RBFGS-Qi-2 & RBFGS & RBFGS-Qi-1 & RBFGS-Qi-2  \\ 
            \midrule
            Time in seconds & $0.15$ & $0.21$ & $0.54$ & $0.96$ & $4.6$ & $11.0$ \\ 
            \midrule
            Iterations & $72$ & $68$ & $72$ & $70$ & $92$ & $97$ \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of RBFGS from \lstinline!Manopt.jl! with RBFGS from \cite{Qi:2011} on $\mathbb{S}^{n-1}$ for $n=100,300$.}
\end{table}


\cref{tab:RayleigResults} contains the results of the RBFGS method from \lstinline!Manopt.jl! and the RBFGS method from \cite{Qi:2011} on $\mathbb{S}^{99}$ and $\mathbb{S}^{299}$. The RBFGS columns represent the results obtained by \lstinline!quasi_Newton()!. \cref{RayleighCode} was used in the experiments depending on the parameter \lstinline!n!. For the time measurement the package BenchmarkTools.jl was used. The time given is the average of $10$ random runs, where the time in each run was measured with a benchmark of $10$ samples and $1$ evaluation per sample. The iterations are the average of the measured iterations from $10$ random runs. In columns RBFGS-Qi-1 and RBFGS-Qi-2, the results from \cite[Table~5.1]{Qi:2011} have been included. \\
For the manifold $\mathbb{S}^{99}$, the values generated by executing the different methods do not differ significantly. But for $\mathbb{S}^{299}$ the RBFGS method from \lstinline!Manopt.jl! seems to achieve better times and lesser iterations. The big time difference can be explained by the fact that both methods from \cite{Qi:2011} seem to be implemented and tested in \textsc{Matlab}, since reference is made to the \lstinline!GenRTR! package (Generic Riemannian Trust-Region, available at \url{https://www.math.fsu.edu/~cbaker/GenRTR/}) and therefore time differences are not surprisingly. \\
More interesting are the average iterations, which are also less for the RBFGS method from \lstinline!Manopt.jl! than for the RBFGS methods from \cite{Qi:2011}. This can be explained by the fact that all three methods have a different approach, how the operators $\mathcal{B}^{RBFGS}_k$ are realized and how the search direction $\eta_k$ results from it. \\
The method generating the results for RBFGS-Qi-1 follows the approach that $\mathcal{M}$ is considered as a submanifold of $\mathbb{R}^{m}$ and its tangent spaces $\tangent{x}$ are identified with subspaces of $\mathbb{R}^{m}$. It realizes the operator $\mathcal{B}^{RBFGS-Qi-1}_k$ as a matrix $B^{(m)}_k \in \mathbb{R}^{m \times m}$ such that $B^{(m)}_k i_{x_k}(\xi_{x_k}) = i_{x_k}(\mathcal{B}^{RBFGS-Qi-1}_k[\xi_{x_k}])$ holds, where $i_x \colon \; \tangent{x} \to \mathbb{R}^{m}, \; \xi_x \mapsto i_x(\xi_x)$ denotes the natural inclusion of $\tangent{x}$ in $\mathbb{R}^{m}$. The descent direction is given by $\eta_k = i^{-1}_{x_k}(-B^{(m)}_k i_{x_k}(\operatorname{grad} f(x_k)))$. Of course this matrix $B^{(m)}_k$ is also updated by means of \cref{inverseBFGSformula} using $i_{x_{k+1}}(s_k), i_{x_{k+1}}(y_k) \in \mathbb{R}^m$ and $\widetilde{B}^{(m)}_k = T^{(m)}_{\alpha_k \eta_k} B^{(m)}_k (T^{(m)}_{\alpha_k \eta_k})^{\dagger}$ where $T^{(m)}_{\alpha_k \eta_k} \in \mathbb{R}^{m \times m}$ is the matrix representation of the chosen vector transport $\vectorTransportDir{x_k}{\alpha_k \eta_k}$ satisfying $T^{(m)}_{\alpha_k \eta_k} i_{x_k}(\xi_{x_k}) = i_{x_k}(\vectorTransportDir{x_k}{\alpha_k \eta_k}(\xi_{x_k}))$ and $\dagger$ denotes the pseudoinvers. For this approach, the typical situation is that $T^{(m)}_{\alpha_k \eta_k}$ is expressed efficiently via some projection-like expression (see \cite[p.~44-46]{Qi:2011}) and $B^{(m)}_k, \widetilde{B}^{(m)}_k, B^{(m)}_{k+1}$ will be expressed as dense $m \times m$ matrices that are singular but $\spd$ on the subspaces representing the appropriate tangent space of $\mathcal{M}$ (see \cite[3.3.1~Approach~1]{Qi:2011}). \\
The RBFGS method from \lstinline!Manopt.jl! and the method generating the results for RBFGS-Qi-2 methods are similar. RBFGS-Qi-2 realizes the operator $\mathcal{B}^{RBFGS-Qi-2}_k$ as a matrix $B^{(n)}_k \in \mathbb{R}^{n \times n}$ and a basis $(e_1, \cdots, e_n)$ of $\tangent{x_k}$, where $n$ is the dimension of the manifold $\mathcal{M}$. The coordinates of the search direction $\eta_k$ are given by $\hat{\eta_k} = - B^{(n)}_k \widehat{\operatorname{grad} f(x_k)} \in \mathbb{R}^n$ and the resulting search direction is $\eta_k = \sum^{n}_i (\hat{\eta_k})_i e_i \in \tangent{x_k}$. At the end, the matrix $B^{(n)}_k$ is updated by means of \cref{inverseBFGSformula} (see \cite[3.3.2~Approach~2]{Qi:2011}). \cite{Qi:2011} states that the main efficiency concern of this approach is the cost of transport and/or creation of the required bases and the cost of obtaining the coordinates. In general, the RBFGS-Qi-1 is also not completely independent of tangent space bases computationally, because of its vector transport. Since RBFGS-Qi-2 works with the coordinates with respect to a basis, it uses the basis not only for its vector transport but also for updating the matrix $B^{(n)}_k$. As a result, it is expected to be computationally more costly. Which of these methods is superior depends on the manifold type and the size of the manifold which is critical in the complexity of creating basis of the tangent space. \cite{Qi:2011} further states that RBFGS-Qi-2 has potential, as the dimension of the manifold is in general smaller than the space in which it is embedded, but only if there is an efficient manner of obtaining the coordinates with respect to the basis. \\
Exactly this last aspect seems to have been well realized in the RBFGS method from \lstinline!Manopt.jl! by using the functions \lstinline!get_coordinates(M,x,xi,basis)!, which returns the coordinates of a tangent vector \lstinline!xi! in the tangent space of \lstinline!x! with respect to a \lstinline!basis!, and \lstinline!get_vector(M,x,coordinates,basis)!, which returns a tangent vector of the tangent space of \lstinline!x! formed from the \lstinline!coordinates! and the \lstinline!basis!, from the package \lstinline!ManifoldsBase.jl!. This could be the decisive reason for the fewer iterations, as it can be assumed that its implementation makes the computation of the descent directions “more accurate” in each iteration and thus the algorithm reaches a stationary point faster. 