\section{Experiments}

In this section the performance of the RBFGS method and the cautious LRBFGS method, implemented in \lstinline!Manopt.jl!, are investigated. \\
The methods are applied to Riemannian optimization problems which are defined in Julia. Essential for those are a manifold \lstinline!M!, created with constructors from the package \lstinline!Manifolds.jl!, a real-valued function \lstinline!F!, a gradient \lstinline!grad_F!, which assigns a tangent vector to each point on the manifold, and a starting point \lstinline!x!, which in most cases is randomly generated by \lstinline!random_point(M)!. \\
To apply the LRBFGS method with a memory size of $20$, \cref{quasi_NewtonCall} must be executed. That means, if \lstinline!quasi_Newton(M,F,grad_F,x)! is executed with the only necessary parameters that define the optimization problem, the choice goes to the LRBFGS method. 

\begin{lstlisting}[caption={Call of the \lstinline!quasi_Newton!-function from \lstinline!Manopt.jl! with the necessary input arguments.}, label={quasi_NewtonCall}]
    quasi_Newton(M, F, grad_F, x) 
\end{lstlisting}

The name \lstinline!quasi_Newton()! implies that of course other methods, belonging to the class of Riemannian quasi-Newton methods, can also be executed with it. This can be done by the optional input arguments, which can be defined after the necessary ones. If the LBFGS method with a different memory size should be executed, \lstinline!memory_size::Int! must be set equal to the number of vectors to be stored, starting from \lstinline!0!. The standard memory size is \lstinline!20!. If the LRBFGS method is not to be used, but a method where an operator is updated by means of \cref{RiemannianInverseBFGSFormula} (following the methodology from \cref{Section5.1}), \lstinline!memory_size! must be set equal to a negative number, e.g. \lstinline!memory_size = -1!. Since the update is realized by means of coordinates, an initial operator can be specified by means of \lstinline!initial_operator::AbstractMatrix! as a matrix. The default initial operator is \lstinline!Matrix(I,manifold_dimension(M), manifold_dimension(M))!, which represents the identity in the tangent space. By default the method is carried out with the exponential map, \lstinline!ExponentialRetraction()!, and the parallel transport, \lstinline!ParallelTransport()!. If this is not the case, this can be adjusted via \lstinline!retraction_method::AbstractRetractionMethod! and \lstinline!vector_transport_method::AbstractVectorTransportMethod!. \\ 
A cautious RBFGS and a cautious LRBFGS method can also be used. This can be done by setting \lstinline!cautious_update::Bool! to \lstinline!true!, the default is \lstinline!false!. For the cautious LRBFGS method, short CLRBFGS, the decision rule
\begin{equation}\label{CautiousTrigger}
    \frac{g_{x_{k+1}}(y_k,s_k)}{\lVert s_k \rVert^{2}_{x_{k+1}}} \geq \theta(\lVert \operatorname{grad} f(x_k) \rVert_{x_k})
\end{equation}
is used in each iteration to decide whether the vectors $s_k, y_k$ will be stored or not. The function $\theta$ can be set via \lstinline!cautious_function::Function!. Care must be taken that it is a monotone increasing function satisfying $\theta(0) = 0$ and it is strictly increasing at $0$. By default \lstinline!(x)->x*10^(-4)! is used. \\
As we found out in this thesis, the stepsize plays an essential role for the methods. The method to determine a stepsize can be set via \lstinline!step_size::Stepsize!. By default \lstinline!WolfePowellLineseach()!, which is a generalization of \cite[Algorithmus~9.3]{UlbrichUlbrich:2012}, that determines a stepsize which fulfills \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.2}, with the chosen \lstinline!retraction_method! and \lstinline!vector_transport_method! is used. The constants in the Wolfe conditions are set to $c_1 = 10^{âˆ’4}$ and $c_2 = 0.999$. We point out that the resulting stepsize does not lead to the fulfillment of the curvature condition \cref{RiemannianCurvatureCondition}, unless \lstinline!ExponentialRetraction()! and \lstinline!ParallelTransport()! or a combination of retraction and vector transport, that fulfills the locking condition \cref{LockingCondition}, are used. The criteria by which the method is aborted can be adjusted by \lstinline!stopping_criterion::StoppingCriterion!. By default, the method aborts either when $1000$ iterations have been reached or when the norm of the gradient is lesser than $10^{-6}$. \\
The numerical experiments are implemented in the toolbox \lstinline!Manopt.jl!. They were run on a Lenovo ThinkPad L490, 64 bit Windows system, 1.8 Ghz Intel Core i7-8565U, 32 GB RAM, with Julia 1.5.2.


\input{Chapter4/RayleighQuotientMinimization.tex}
\input{Chapter4/BrockettCostFunction.tex}