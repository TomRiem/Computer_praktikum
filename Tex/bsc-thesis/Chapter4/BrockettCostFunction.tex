\subsection{Brockett Cost Function}
\label{Section5.2.2}

To show the performance of the cautious LRBFGS method, implemented in \lstinline!Manopt.jl!, we compare the average time per run and the average number of iterations with the results from \cite[p.~1683]{HuangGallivanAbsil:2015} with respect to the Brockett cost function minimization problem on the Stiefel manifold $St(k,n)$. For a symmetric matrix $A \in \mathbb{R}^{n \times n}$, the eigenvectors for the $k$ smallest eigenvalues $\lambda_1 \leq \cdots \leq \lambda_k$ are the columns of a global minimizer of
\begin{equation}\label{BrockettCostFunction}
    \begin{split}
        f \colon \; St(k,n) & \to \mathbb{R} \\
        X & \mapsto \operatorname{trace}(X^{\mathrm{T}} A X N) 
    \end{split}
\end{equation}
where $N = diag(\mu_1, \cdots, \mu_p)$ with $\mu_1 > \cdots > \mu_p > 0$. \cref{BrockettCostFunction} is not a retraction-convex function on the entire domain. However, for generic choices of $A$, the function is strongly retraction-convex in a sublevel set around any global minimizer \cite[p.~1678]{HuangGallivanAbsil:2015}. \cref{BrockettCostFunction} can be considered as a weighted sum $\sum_i \mu_i x^{\mathrm{T}}_i A x_i$ of Rayleigh quotients on the sphere under an orthogonality constraint $x^{\mathrm{T}}_i x_j = \delta_{ij}$. Its gradient is given by 
\begin{equation*}
    \operatorname{grad} f(X) = 2 A X N - X X^{\mathrm{T}} A X N - X N X^{\mathrm{T}}.
\end{equation*}

Unfortunately, we are unaware of any closed expression for the parallel transport on the Stiefel manifold \cite{EdelmanAriasSmith:1998}. Therefore we cannot use the default choice of exponential map, \lstinline!ExponentialRetraction()!, and parallel transport, \lstinline!ParallelTransport()!, for solving this optimization problem. A closed form expression of the exponential map is known for the Stiefel manifold, but it is computationally very expensive. \\
We decided to conduct this experiment with the QR-based retraction on the Stiefel manifold. With $Q R = x_k + \alpha_k \eta_k \in \mathbb{R}^{n \times k}$ the retraction reads
\begin{equation}\label{QRRetraction}
    \retract{x_k}(\alpha_k \eta_k) = Q D,
\end{equation}
where $D$ is a $n \times k$ matrix with $D = \operatorname{diag}({\operatorname{sgn}(R_{ii} + 0.5)}^k_{i = 1})$ and $\operatorname{sgn} \colon \; \mathbb{R} \to \{-1, 0, 1 \}$ is the well-known signum function. \\
We use for this experiment a projection-based vector transport, i.e.
\begin{equation}\label{VectorTransportProjection}
    \vectorTransport{x_k}{\retract{x_k}(\alpha_k \eta_k)}(\alpha_k \eta_k) = \proj{\retract{x_k}(\alpha_k \eta_k)} (\alpha_k \eta_k) 
\end{equation}
where $\proj_x \colon \; \mathbb{R}^{n \times k} \to \tangent{x}, \; \proj{x}(M) = M - x^{\mathrm{T}} \operatorname{Symm} (x^{\mathrm{T}}M)$ with $\operatorname{Symm}(A) = (A + A^\mathrm{T})/2$ projects a matrix $M$ onto the tangent space of $x$. We see that \cref{VectorTransportProjection} fulfills all the properties of \cref{VectorTransport}. We note that this vector transport with associated retraction (defined in \cref{QRRetraction}) is not isometric and is also not the vector transport by differentiated retraction of the chosen QR-based retraction. This implies of course that \cref{VectorTransportProjection} and \cref{QRRetraction} do not meet the locking condition, \cref{LockingCondition}. We take the liberty of this lack of precision in the experiments, as on the one hand no suitable combination of vector transport and retraction, which would fulfill the locking condition, is implemented in \lstinline!Manopt.jl! and on the other hand both \cref{VectorTransportProjection} and \cref{QRRetraction} offer implementations which are computationally cheap. \\
We set in each iteration
\begin{equation*}
    \begin{split}
        & s_k = \proj{\retract{x_k}(\alpha_k \eta_k)} (\alpha_k \eta_k) \\
        & y_k = \beta^{-1}_k \operatorname{grad} f(x_{k+1}) - \proj{\retract{x_k}(\alpha_k \eta_k)}(\operatorname{grad} f(x_k)) \\
        & \text{where} \quad \beta_k = \frac{\lVert \alpha_k \eta_k \rVert_{x_k}}{\lVert \proj{\retract{x_k}(\alpha_k \eta_k)} (\alpha_k \eta_k) \rVert_{x_{k+1}}}.
    \end{split}
\end{equation*}
We note that the operators $\mathcal{B}^{CLRBFGS}_k$ do not have to be positive definite, firstly because the vector transport, \cref{VectorTransportProjection}, is not isometric and secondly because the curvature condition, \cref{RiemannianCurvatureCondition}, is not always satisfied. This means that $\eta_k$ does not necessarily have to be a descent direction. We hope that by applying the decision rule, \cref{CautiousTrigger}, in the cautious update, we will still get a convergent method, because only vectors $s_k, y_k \in \tangent{x_{k+1}}$ which already fulfill the curvature condition, \cref{RiemannianCurvatureCondition}, will be stored. \\
To apply the CLRBFGS method with a memory size of $4$, implemented in \lstinline!Manopt.jl!, to the optimization problem defined by the cost function \cref{BrockettCostFunction} for $St(32,32)$, \cref{BrockettCode} must be executed in Julia. We point out that when the constructor to create a Stiefel manifold is executed, the dimensions are reversed, that means \lstinline!Stiefel(n,k)! creates the manifold $St(k,n)$ in Julia. The problem is defined by setting \lstinline!A_symm = A + A'!, where the elements of \lstinline!A! are drawn from the standard normal distribution using Julia’s \lstinline!randn(n,n)! with seed \lstinline!42! and \lstinline!N! is a diagonal matrix whose diagonal elements are integers from $k$ to $1$. With \lstinline!random_point(M)! a random point is created on the given manifold. The stopping criterion requires to abort the method, that the ratio of the norm of the initial gradient and the norm of the current gradient is less than $10^{-6}$. \\ 

\begin{lstlisting}[caption={The Brockett cost function experiment in Julia for $n = 32, k = 32$ and a memory size of $4$.}, label={BrockettCode}]
    using Manopt, Manifolds, LinearAlgebra, Random
    Random.seed!(42)
    n = 32
    k = 32
    M = Stiefel(n,k)
    A = randn(n,n)
    A_symm = A + A' 
    N = diagm(k : -1 : 1)
    F(X::Array{Float64,2}) = tr(X' * A_symm * X * N)
    grad_F(X::Array{Float64,2}) = 2 * A_symm * X * N - X * X' * A_symm * X * N - X * N * X' * A_symm * X
    x = random_point(M)
    
    quasi_Newton(M, 
        F, 
        grad_F, 
        x; 
        memory_size = 4, 
        cautious_update = true,
        retraction_method = QRRetraction(), 
        vector_transport_method = ProjectionTransport(), 
        stopping_criterion = StopWhenGradientNormLess(norm(M, x, grad_F(x)) * 10^(-6))) 
\end{lstlisting}


\begin{table}[H]\label{tab:BrockettResultsMemory1}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l l l l l l l}
            \toprule
            Memory size & \multicolumn{2}{c}{$1$} & \multicolumn{2}{c}{$2$} & \multicolumn{2}{c}{$4$}  \\ 
            \midrule
            Method & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA \\ 
            \midrule
            Time in seconds & $1.504$ & $0.653$ & $1.665$ & $0.662$ & $1.299$ & $0.741$ \\ 
            \midrule
            Iterations & $664$ & $760$ & $568$ & $678$ & $482$ & $609$  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of CLRBFGS from \lstinline!Manopt.jl! with LRBFGS from \cite{HuangGallivanAbsil:2015} for a memory size of $1, 2, 4$.}
\end{table}


\begin{table}[H]\label{tab:BrockettResultsMemory2}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l l l l l l l}
            \toprule
            Memory size & \multicolumn{2}{c}{$8$} & \multicolumn{2}{c}{$16$} & \multicolumn{2}{c}{$32$}  \\ 
            \midrule
            Method & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA \\ 
            \midrule
            Time in seconds & $1.470$ & $0.973$ & $1.667$ & $1.36$ & $2.676$ & $2.08$ \\ 
            \midrule
            Iterations & $464$ & $584$ & $463$ & $538$ & $414$ & $491$  \\
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of CLRBFGS from \lstinline!Manopt.jl! with LRBFGS from \cite{HuangGallivanAbsil:2015} for a memory size of $8, 16, 32$.}
\end{table}

\cref{tab:BrockettResultsMemory1} and \cref{tab:BrockettResultsMemory2} contain the results of the cautious LRBFGS method from \lstinline!Manopt.jl! and the LRBFGS method from \cite{HuangGallivanAbsil:2015} for the memory sizes \lstinline!1,2,4,8,16,32!. The CLRBFGS columns represent the results obtained by \lstinline!quasi_Newton()!. \cref{BrockettCode} was used in the experiments depending on the input argument \lstinline!memory_size!. For the time measurement the package BenchmarkTools.jl was used. The time given is the average of $10$ random runs, where the time in each run was measured with a benchmark of $1$ sample and $1$ evaluation per sample. The iterations are the average of the measured iterations from $10$ random runs. In the LRBFGS-HGA columns, the results from \cite[Table~4]{HuangGallivanAbsil:2015} have been included. \\
The first thing to notice in \cref{tab:BrockettResultsMemory1} and \cref{tab:BrockettResultsMemory2} is the time difference between the two methods. The CLRBFGS method from \lstinline!Manopt.jl! is on average slower by a factor of $\sim 1.6$ per iteration. This can have several reasons. Of course it could again be attributed to the use of the different software. The experiments in \cite{HuangGallivanAbsil:2015} were also implemented and performed in \textsc{Matlab}. But it would be surprising if this is the reason, because in general Julia is faster and this experiment would be an exception. It could also simply be the hardware. The processor of the computer with which the experiments were performed in \cite{HuangGallivanAbsil:2015} reaches 3.6 Ghz. The most likely reason, however, is the different computation of the stepsize. In \cite{HuangGallivanAbsil:2015}, a generalization of \cite[Algorithm~A6.3.1mod]{DennisSchnabel:1996} is used, which determines a stepsize that fulfills the Wolfe conditions \cref{RiemannianWolfeConditions1} and \cref{RiemannianWolfeConditions2.1} with the same constants (i.e. $c_1 = 10^{−4}$ and $c_2 = 0.999$), and the initial stepsize is determined by a generalization of the approach in \cite[p.~60]{NocedalWright:2006}. This approach seems to be very sophisticated and could produce an appropriate stepsize quickly with little effort. This suggests that a generalization of \cite[Algorithmus~9.3]{UlbrichUlbrich:2012}, as we use it in \lstinline!quasi_Newton()!, may not be the first choice. \\
The average numbers of iterations in \cref{tab:BrockettResultsMemory1} and \cref{tab:BrockettResultsMemory2} are surprisingly lower for the CLRBFGS method from \lstinline!Manopt.jl!. It can therefore be assumed that the decision rule \cref{CautiousTrigger} is fulfilled in most cases. This means on the one hand that the curvature condition \cref{RiemannianCurvatureCondition} is fulfilled in most cases, which is surprising, since \cref{VectorTransportProjection} and \cref{QRRetraction} do not meet the locking condition, \cref{LockingCondition}, and on the other hand, since new information about the curvature is added to the memory by fulfilling the decision rule (through storing the vectors $s_k$ and $y_k$), that this information is well processed by the CLRBFGS method from \lstinline!Manopt.jl!, which suggests an efficient computation for $\eta_k$. \\

\begin{table}[H]\label{tab:BrockettResultsLargeDimensions}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l l l l l l l l l}
            \toprule
            Manifold & \multicolumn{2}{c}{$St(2,1000)$} & \multicolumn{2}{c}{$St(3,1000)$} & \multicolumn{2}{c}{$St(4,1000)$} & \multicolumn{2}{c}{$St(5,1000)$}  \\ 
            \midrule
            Method & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA & CLRBFGS & LRBFGS-HGA \\ 
            \midrule
            Time in seconds & $168.038$ & $0.807$ & $211.895$ & $1.70$ & $306.802$ & $2.70$ & $ 272.467$ & $4.48$ \\ 
            \midrule
            Iterations & $283$ & $233$ & $417$ & $368$ & $505$ & $449$ & $551$ & $526$ \\ 
            \bottomrule
        \end{tabular}
    }
    \caption{Comparison of CLRBFGS from \lstinline!Manopt.jl! with LRBFGS from \cite{HuangGallivanAbsil:2015} on $St(k,1000)$ for $k = 2, 3, 4, 5$.}
\end{table}

\cref{tab:BrockettResultsLargeDimensions} contains the results of the cautious LRBFGS method from \lstinline!Manopt.jl! and the LRBFGS method from \cite{HuangGallivanAbsil:2015} on $St(k,1000)$ for $k = 2, 3, 4, 5$. The CLRBFGS columns represent the results obtained by \lstinline!quasi_Newton()!. \cref{BrockettCode} was used in the experiments depending on the parameter \lstinline!k!. For the time measurement the package BenchmarkTools.jl was used. The time given is the average of $5$ random runs, where the time in each run was measured with a benchmark of $1$ sample and $1$ evaluation per sample. The iterations are the average of the measured iterations from $5$ random runs. In the LRBFGS-HGA columns, the results from \cite[Table~4]{HuangGallivanAbsil:2015} have been included. \\
The values in \cref{tab:BrockettResultsLargeDimensions} show that the performance of CLRBFGS method from \lstinline!Manopt.jl! rather cannot compete with the performance of the LRBFGS method from \cite{HuangGallivanAbsil:2015}. For the time differences, of course, the above arguments can be applied again, but also others associated with the higher average number of iterations. Most obviously, this could be attributed to the characteristic of $\eta_k$ not necessarily to be a descent direction, as already mentioned. But there could be two other reasons. On the one hand, it could be due to the fact that the decision rule \cref{CautiousTrigger} was not fulfilled in some iterations and therefore no new vectors were added to the memory. If this happens, it leads to a “worse” approximation, because the information about the curvature is not updated. With a memory size of $4$, this can have serious effects on the computation of the search direction $\eta_k$. If the search direction is not calculated “well”, it can happen that the determination of an appropriate stepsize turns out to be very complex. This, of course also, takes time. \\
On the other hand, it can also be due to the fact that the two methods could differ in one point, which concerns numerical accuracy. \cite{HuangGallivanAbsil:2015} state that it can happen, that some “old” pairs of $\{ \widetilde{s}_i, \widetilde{y}_i\}_{i=k-m}^{k-1}$ are not in the tangent space $\tangent{x_k}$ because of the repeated use of the chosen vector transport, $\vectorTransportSymbol^S$, which can be attributed to its numerical inaccuracy, if it is significant. \cite{HuangGallivanAbsil:2015} note that in order to avoid this disadvantage, one can project all vectors, which were transported more than once, i.e. $\{ \widetilde{s}_i, \widetilde{y}_i \}_{i=k-m+1}^{k-1}$, again onto the tangent space of the next iterate $\tangent{x_{k+1}}$ \cite[p.~1676]{HuangGallivanAbsil:2015}. Whether this approach is actually used in the implementation of the method from \cite{HuangGallivanAbsil:2015}, used for the experiments, is left open. However, the implementation of the CLRBFGS method from \lstinline!Manopt.jl! also differs from the actual LRBFGS method, \cref{LRBFGSMethod} (not only by using the cautious decision rule): Before \cref{LRBFGSTwo-LoopRecursion} returns the search direction $\eta_k$, it is projected again onto the tangent space of the current iterate, i.e. \cref{LRBFGSTwo-LoopRecursion} actually returns $\eta_k =  \proj{x_k}(- \mathcal{B}^{LRBFGS}_k[\operatorname{grad} f(x_k)]) \in \tangent{x_k}$. This is to make sure, that the search direction $\eta_k$, which may have been calculated with inaccuracies in \cref{LRBFGSTwo-LoopRecursion}, actually lies in the appropriate tangent space $\tangent{x_k}$. These different approaches of the two methods can of course be reflected both in the number of iterations and in the average time per run. If this is the case, the LRBFGS method from \cite{HuangGallivanAbsil:2015} is of course again to be preferred.
